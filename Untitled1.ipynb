{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as  f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('model7.pkl', 'rb') as f:\n",
    "    model_best = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych z normalnej pracy\n",
    "#okreslenie poprawności pracy na podstawie dołączonego arkusza\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tLICZBA PRÓBEK\n",
    "# \t02/03/2017\t\t0-2\t\t\t\t397721\n",
    "# \t06/03/2017\t\t3\t\t\t\t48513\n",
    "# \t16/03/2017\t\t4\t\t\t\t518348\n",
    "# \t22/03/2017\t\t5\t\t\t\t644888\n",
    "# \t23/03/2017\t\t6\t\t\t\t408255\n",
    "# \t29/03/2017\t\t7-8\t\t\t\t517850\n",
    "# \t13/04/2017\t\t9-13\t\t\t120844\n",
    "# \t19/04/2017\t\t14-17\t\t\t636238\n",
    "# \t28/04/2017\t\t18\t\t\t\t227459\n",
    "# \t12/05/2017\t\t19-20\t\t\t332583\n",
    "# \t17/05/2017\t\t21\t\t\t\t1993321\n",
    "# \t07/06/2017\t\t22-23\t\t\t183853\n",
    "# \t21/08/2017\t\t24-31\t\t\t738162\n",
    "# \t22/08/2017\t\t32-35\t\t\t1952183\n",
    "# \t23/08/2017\t\t36-37\t\t\t287377\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"E:\\\\9sem\\\\INO\\\\Dane\\\\fine\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_fine = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_fine.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma w danych z etykietą Voltage Dips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_fine)):\n",
    "    data_fine[i] = data_fine[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja dodająca do zbiorów kolumny z przesuniętymi w czasie prękościami obrotowymi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def add_history(dataset):\n",
    "    for x in range(0, len(dataset)):\n",
    "        temp = np.concatenate(([dataset[x].values[0, 19]], dataset[x].values[0:-1, 19]))\n",
    "        temp2 = np.concatenate(([dataset[x].values[0, 20]], dataset[x].values[0:-1, 20]))\n",
    "        dataset[x].insert(42, 42, temp)\n",
    "        dataset[x].insert(43, 43, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dodanie dwóch kolumn z przesuniętymi w czasie prędkościami obrotowymi\n",
    "add_history(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja usuwająca zakresy wierszy ze zbioru danych\n",
    "import pandas as pd\n",
    "def drop_rows(dataset, beg, end):\n",
    "    dataset.drop(dataset.index[beg:end], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnych obserwacji- długie stany ustalone\n",
    "#wybór na podstawie analizy wykresów przebiegów\n",
    "#zbiory po przetworzeniu:\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tPRÓBEK\t\n",
    "# \t02/03/2017\t\t0-1\t\t\t\t194697\t\t\t\n",
    "# \t06/03/2017\t\t2\t\t\t\t48513\n",
    "# \t16/03/2017\t\t3\t\t\t\t128348\n",
    "# \t22/03/2017\t\t4\t\t\t\t299888\n",
    "# \t23/03/2017\t\t5\t\t\t\t193255\n",
    "# \t29/03/2017\t\t6\t\t\t\t157116\n",
    "# \t13/04/2017\t\t7-11\t\t\t120844\n",
    "# \t19/04/2017\t\t12-13\t\t\t89385\n",
    "# \t28/04/2017\t\t14\t\t\t\t127459\n",
    "# \t12/05/2017\t\t15-16\t\t\t107583\n",
    "# \t17/05/2017\t\t17\t\t\t\t693321\n",
    "# \t07/06/2017\t\t18-19\t\t\t123853\n",
    "# \t21/08/2017\t\t20-22\t\t\t296545\n",
    "# \t22/08/2017\t\t23-25\t\t\t293511\n",
    "# \t23/08/2017\t\t26-27\t\t\t41377\n",
    "drop_rows(data_fine[2], 100000, 300000)\n",
    "drop_rows(data_fine[4], 370000, 500000)\n",
    "drop_rows(data_fine[4], 220000, 340000)\n",
    "drop_rows(data_fine[4], 60000, 200000)\n",
    "drop_rows(data_fine[5], 520000, 620000)\n",
    "drop_rows(data_fine[5], 460000, 500000)\n",
    "drop_rows(data_fine[5], 360000, 390000)\n",
    "drop_rows(data_fine[5], 140000, 265000)\n",
    "drop_rows(data_fine[5], 50000, 100000)\n",
    "drop_rows(data_fine[6], 350000, 360000)\n",
    "drop_rows(data_fine[6], 300000, 320000)\n",
    "drop_rows(data_fine[6], 210000, 270000)\n",
    "drop_rows(data_fine[6], 140000, 190000)\n",
    "drop_rows(data_fine[6], 50000, 125000)\n",
    "drop_rows(data_fine[8], 410000, 450000)\n",
    "drop_rows(data_fine[8], 140000, 375000)\n",
    "drop_rows(data_fine[8], 0, 35000)\n",
    "drop_rows(data_fine[15], 65000, -1)\n",
    "drop_rows(data_fine[17], 20000, 100000)\n",
    "drop_rows(data_fine[18], 100000, 200000)\n",
    "drop_rows(data_fine[19], 210000, 240000)\n",
    "drop_rows(data_fine[19], 0, 150000)\n",
    "drop_rows(data_fine[20], 10000, 55000)\n",
    "drop_rows(data_fine[21], 1300000, 1750000)\n",
    "drop_rows(data_fine[21], 250000, 1100000)\n",
    "drop_rows(data_fine[23], 90000, 110000)\n",
    "drop_rows(data_fine[23], 40000, 80000)\n",
    "drop_rows(data_fine[29], 165000, 180000)\n",
    "drop_rows(data_fine[29], 100000, 140000)\n",
    "drop_rows(data_fine[29], 32000, 75000)\n",
    "drop_rows(data_fine[30], 150000, 180000)\n",
    "drop_rows(data_fine[30], 125000, 140000)\n",
    "drop_rows(data_fine[30], 66000, 92000)\n",
    "drop_rows(data_fine[30], 25000, 50000)\n",
    "drop_rows(data_fine[31], 85000, 160000)\n",
    "drop_rows(data_fine[31], 20000, 65000)\n",
    "drop_rows(data_fine[33], 400000, 500000)\n",
    "drop_rows(data_fine[33], 230000, 375000)\n",
    "drop_rows(data_fine[33], 30000, 200000)\n",
    "drop_rows(data_fine[34], 330000, 440000)\n",
    "drop_rows(data_fine[34], 200000, 310000)\n",
    "drop_rows(data_fine[34], 165000, 180000)\n",
    "drop_rows(data_fine[34], 30000, 150000)\n",
    "drop_rows(data_fine[35], 270000, 365000)\n",
    "drop_rows(data_fine[35], 160000, 256000)\n",
    "drop_rows(data_fine[35], 30000, 140000)\n",
    "drop_rows(data_fine[36], 100000, 195000)\n",
    "drop_rows(data_fine[36], 7000, 85000)\n",
    "drop_rows(data_fine[37], 7000, 80000)\n",
    "indices = 0, 7, 14, 16, 24, 25, 26, 27, 28, 32\n",
    "for i in sorted(indices, reverse=True):\n",
    "    del data_fine[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbicie wszystki pomiarów do jednego dataframe\n",
    "# \tDATA\t\t\tPOCZATEK\t\tKONIEC\n",
    "# \t02/03/2017\t\t0\t\t\t\t194696\t\t\t\n",
    "# \t06/03/2017\t\t194697\t\t\t243209\n",
    "# \t16/03/2017\t\t243210\t\t\t371557\n",
    "# \t22/03/2017\t\t371558\t\t\t671445\n",
    "# \t23/03/2017\t\t671446\t\t\t864700\n",
    "# \t29/03/2017\t\t864701\t\t\t1021816\n",
    "# \t13/04/2017\t\t1021817\t\t\t1142660\n",
    "# \t19/04/2017\t\t1142661 \t\t1243538\n",
    "# \t28/04/2017\t\t1243539\t\t\t1370997\n",
    "# \t17/05/2017\t\t1370998\t\t\t693321\n",
    "# \t12/05/2017\t\t1370998 \t\t1478580\t\t\n",
    "# \t07/06/2017\t\t1478579\t\t\t2197611\n",
    "# \t21/08/2017\t\t2197611\t\t\t2497184\n",
    "# \t22/08/2017\t\t2497185\t\t\t2812172\n",
    "# \t23/08/2017\t\t2812173\t\t\t2927187\n",
    "import pandas as pd\n",
    "fine_all = pd.concat(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnej już listy dataframe'ow\n",
    "del data_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie outlierów\n",
    "#stwierdzone na podstawie analizy wykresów przebiegów\n",
    "indices = [42931, 42932, 42933, 42934, 42935, 42936, 42937, 42938, 42939]\n",
    "for i in sorted(indices, reverse=True):\n",
    "    fine_all = fine_all.drop(i)\n",
    "drop_rows(fine_all, 42900, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapicklowanie przetworzonego zbioru danych pochodzących z normalnej pracy\n",
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:  \n",
    "    dill.dump(fine_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Voltage Dips\n",
    "#Dane pochodzą z jednego dnia 14/03/2016\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"E:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\vdips\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_vdips = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_vdips.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumny NaN-ów\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_vdips)):\n",
    "    data_vdips[i] = data_vdips[i].drop(42, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóżnionymi o jedną próbkę wartościami prędkości obrotowych\n",
    "add_history(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie danych do jednego dataframe\n",
    "import pandas as pd\n",
    "vdips_all = pd.concat(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie zbioru\n",
    "import dill\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzenej już listy dataframe'ów\n",
    "del data_vdips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Surge do listy\n",
    "#Stan danych po usunięci dwóch wadliwych dataframe:\n",
    "# \tDATA\t\t\tNR ZBIORU\t\tPRÓBEK\n",
    "# \t17/01/2017\t\t\t0-5\t\t\t411448\n",
    "# \t18/01/2017\t\t\t6-23\t\t1370255\n",
    "# \t19/01/2017\t\t\t24-30\t\t271524\n",
    "# \t20/01/2017\t\t\t31-35\t\t916380\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"E:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\surge\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_surge = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_surge.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie uszkodzonych dataFrame\n",
    "#wykonać x2\n",
    "del data_surge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma vdips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_surge)):\n",
    "    data_surge[i] = data_surge[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóźnionymi o jedną próbkę predkościami obrotowymi\n",
    "add_history(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie wszystkich danych w jeden dataframe\n",
    "#Rozkład danych po zbiciu:\n",
    "# \tDATA\t\t\tPOCZĄTEK\t\tKONIEC\n",
    "# \t17/01/2017\t\t0\t\t\t\t411447\n",
    "# \t18/01/2017\t\t411448\t\t\t1781702\n",
    "# \t19/01/2017\t\t1781703\t\t\t2053226\n",
    "# \t20/01/2017\t\t2053227\t\t\t2969606\n",
    "import pandas as pd\n",
    "surge_all = pd.concat(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie serii danych\n",
    "import dill\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzebnej już listy zbiorów\n",
    "del data_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "def column_max(fine, vdips, surge, col_num):\n",
    "    max_val = fine.values[0, col_num]\n",
    "    max_f = np.nanmax(fine.values[:, col_num])\n",
    "    if max_f > max_val:\n",
    "        max_val = max_f\n",
    "    max_v = np.nanmax(vdips.values[:, col_num])\n",
    "    if max_v > max_val:\n",
    "        max_val = max_v\n",
    "    max_s = np.nanmax(surge.values[:, col_num])\n",
    "    if max_s > max_val:\n",
    "        max_val = max_s\n",
    "    return max_val\n",
    "def column_min(fine, vdips, surge, col_num):\n",
    "    min_val = fine.values[0, col_num]\n",
    "    min_f = np.nanmin(fine.values[:, col_num])\n",
    "    if min_f < min_val:\n",
    "        min_val = min_f\n",
    "    min_v = np.nanmin(vdips.values[:, col_num])\n",
    "    if min_v < min_val:\n",
    "        min_val = min_v\n",
    "    min_s = np.nanmin(surge.values[:, col_num])\n",
    "    if min_s < min_val:\n",
    "        min_val = min_s\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.1525530e+03  0.0000000e+00]\n",
      " [ 9.8535160e+01  0.0000000e+00]\n",
      " [ 1.1469907e+02 -3.5445600e-01]\n",
      " [ 2.1851852e+02 -1.5104167e+01]\n",
      " [ 1.0000362e+02 -1.5776910e+01]\n",
      " [ 4.5782695e+03 -1.0000000e+04]\n",
      " [ 9.5789920e+03 -1.0257523e+04]\n",
      " [ 5.1963977e+04 -1.0000000e+04]\n",
      " [ 1.0552517e+03  0.0000000e+00]\n",
      " [ 9.2881946e+02 -5.4976850e+00]\n",
      " [ 1.1708125e+03 -1.2113159e+01]\n",
      " [ 1.5322266e+05  5.0000000e+04]\n",
      " [-1.8359336e+03 -5.0000000e+04]\n",
      " [ 6.1300000e+01  0.0000000e+00]\n",
      " [ 6.6400000e+01  0.0000000e+00]\n",
      " [ 7.9500000e+01  0.0000000e+00]\n",
      " [ 7.2500000e+01  0.0000000e+00]\n",
      " [ 5.6800000e+01  0.0000000e+00]\n",
      " [ 2.9000000e+01  0.0000000e+00]\n",
      " [ 6.0048003e+03 -6.3900000e+01]\n",
      " [ 6.0396000e+03 -1.5900001e+01]\n",
      " [ 1.9988000e+02 -9.7480000e+01]\n",
      " [ 1.3696000e+02 -4.1820000e+01]\n",
      " [ 3.2767000e+03  0.0000000e+00]\n",
      " [ 3.0000000e+04  0.0000000e+00]\n",
      " [ 3.0000000e+04  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0026765e+02  0.0000000e+00]\n",
      " [ 5.5644000e+02  0.0000000e+00]\n",
      " [ 4.1943040e+05  0.0000000e+00]\n",
      " [ 5.4050000e+02  0.0000000e+00]\n",
      " [ 4.1943040e+05 -4.9000000e-01]\n",
      " [ 1.1100000e+02  0.0000000e+00]\n",
      " [ 4.9940000e+01  0.0000000e+00]\n",
      " [ 7.0289000e+02  0.0000000e+00]\n",
      " [ 5.1900000e+02 -2.0000000e+00]\n",
      " [ 2.9670000e+01 -8.6000000e-01]\n",
      " [ 1.0900000e+02  0.0000000e+00]\n",
      " [ 6.0048003e+03 -6.3900000e+01]\n",
      " [ 6.0396000e+03 -1.5900001e+01]]\n"
     ]
    }
   ],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max(fine_all, vdips_all, surge_all, col)\n",
    "    col_ranges[col, 1] = column_min(fine_all, vdips_all, surge_all, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja realizująca normalizacją min-max danych\n",
    "def normalise(dataset, columns_ranges):\n",
    "        for col in range(0, 44):\n",
    "            for row in range(0, len(dataset)):\n",
    "                value = dataset.values[row, col]\n",
    "                min_val = columns_ranges[col, 1]\n",
    "                max_val = columns_ranges[col, 0]\n",
    "                dataset.values[row, col] = (value - min_val) / (max_val - min_val)\n",
    "            print(f'Znormalizowano kolumnę {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n"
     ]
    }
   ],
   "source": [
    "#normalizacja danych pochodzących z prawidłowej pracy\n",
    "normalise(fine_all, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisanie znormalizowanych danych\n",
    "with open('fine_all.pkl', 'wb') as f:\n",
    "    dill.dump(fine_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n"
     ]
    }
   ],
   "source": [
    "#normalizacja danych z etyietą Voltage Dips\n",
    "normalise(vdips_all, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisanie znormalizowanych danych\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n"
     ]
    }
   ],
   "source": [
    "#normalizacja danych z etykietę Surge\n",
    "normalise(surge_all, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisanie znormalizowanych danych\n",
    "import dill\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DOBÓR ZMIENNYCH WEJŚCIOWYCH\n",
    "#obliczenie współczynników korelacji liniowej między atrybutami\n",
    "#przekierowanie korelacji do arkusza w celu łatwiejszego przeglądania\n",
    "Pearsons = fine_all.corr()\n",
    "path = \"E:\\\\9sem\\\\INO\\\\fine_corrs.xls\"\n",
    "Pearsons.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x20644a79eb8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KONTYNUACJA DOBORU ATRYBUTÓW - OBSERWACJA PRZEBIEGÓW\n",
    "#wykreślenie przebiegu wyjścia i potencjalnych wejść\n",
    "#wstepny dobór wejśc na podstawie analizy korelacji Pearsona (impl:pandas.df.corr) i przebiegów\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plotter.plot(fine_all.values[:, 19], label='target')\n",
    "plotter.plot(fine_all.values[:, 6], label='input6')\n",
    "plotter.plot(fine_all.values[:, 40], label='input40')\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wybór zbioru uczącego\n",
    "#NIEUDANA próba stworzenia modelu na podstawie atrybutów\n",
    "#wybranych analizą korelacyjną i przeglądaniem przebiegów\n",
    "    #problem z prawidłową predykcją w stanie 0\n",
    "    #problem z długimi symulacjami - rozjazd\n",
    "#Uzasadnienie podziału danych na uczące i testowe w pliku graficznym zbior_uczacy.png\n",
    "import numpy as np\n",
    "#train_set = fine_all.values[5000:40000, [6, 40, 42, 19]]\n",
    "train_set = fine_all.values[305000:315000, [6, 40, 42, 19]]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, [6, 40, 42, 19]]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, [6, 40, 42, 19]]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "#train_set = train_set[::10, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapisanie zbioru uczącego na podstawie samodzielnie wybranych kolumn\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[65000:160000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[395000:459000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[470770:546400, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[867400:958000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[979500:998000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1070000:1115190, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1212500:1230000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1270000:1310000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1346000:1367500, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1403420:1423060, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1436000:1444870, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1457230:1462000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1650000:1665000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[1759000:1840000, [6, 40, 42, 19]])\n",
    "test_set.append(fine_all.values[2162000:2169620, [6, 40, 42, 19]])\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symulacja serii danych podając poprzednie wyjście na wejście następnego sample'a\n",
    "def sim(model, test_set, set_num, isrec):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0:\n",
    "            sample = test_set[set_num][0:1, 0:col-1]\n",
    "            if isrec == 1:\n",
    "                sample = np.reshape(sample, (-1, 1, col-1))\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = test_set[set_num][i:i+1, 0:col-1]\n",
    "            sample[0, col-2] = y\n",
    "            if isrec == 1:\n",
    "                sample = np.reshape(sample, (-1, 1, col-1))\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    return y_serie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Próba stworzenia modelu neuronowego\n",
    "#Funkcja do reinicjalizacji wag początkowych reset_weights\n",
    "#funckja do wytrenowania sieci i jej testowania simulation\n",
    "from keras import backend as bck\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, SimpleRNN, Embedding, Dropout\n",
    "from keras import optimizers, regularizers, losses\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def reset_weights(model):\n",
    "    session = bck.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'):\n",
    "            layer.kernel.initializer.run(session=session)\n",
    "def simulation(test_single, model, RNN, test_set, train_set):\n",
    "    col = len(test_set[0][0])\n",
    "    for i in range(0, test_single):\n",
    "        reset_weights(model)\n",
    "        prop = optimizers.rmsprop(lr=0.01)\n",
    "        sgd = optimizers.sgd(lr=0.01, nesterov=True, momentum=0.005)\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        if RNN == 0:\n",
    "            model.fit(x=train_set[:, 0:col-1], y=train_set[:, col-1], verbose=0)\n",
    "        if RNN == 1:\n",
    "            X = train_set[:, 0:col-1].reshape(-1, 1, col-1)\n",
    "            Y = train_set[:, col-1].reshape(-1, col-1)\n",
    "            model.fit(x=X, y=Y, validation_split=0.2, epochs=10,\n",
    "                      batch_size = 50, verbose=0)\n",
    "        y_serie = sim2(model, test_set, 0, RNN)\n",
    "        curr_score = mean_squared_error(test_set[0][:, col-1], np.reshape(y_serie, -1))\n",
    "        if i == 0:\n",
    "            best_score = curr_score\n",
    "            best_model = model\n",
    "        else:\n",
    "            if curr_score < best_score:\n",
    "                best_score = curr_score\n",
    "                best_model = model\n",
    "        print(f'\\tWytrenowano {i+1} z {test_single} sieci danej struktury')\n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja automatyzująca testy różnych sieci neuronowych o jednej warstwie ukrytej\n",
    "#trenuje zadaną liczbę sieci, testuje na pierwszym zbiorze testowym i zapamiętuje najlepszą\n",
    "    #edit: dodano obsługę sieci rekurencyjnych\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def sweep_one_layer(min_num, max_num, test_single, RNN, test_set, train_set):\n",
    "    col = len(test_set[0][0])\n",
    "    for n in range(min_num, max_num+1):\n",
    "        print(f'Trenuję sieć o strukturze {n}-1')\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=n, input_dim = col-1, activation = 'tanh'))\n",
    "        model.add(Dropout(0.5))\n",
    "        model.add(Dense(1, activation='linear'))\n",
    "        curr_model = simulation(test_single, model, RNN, test_set, train_set)\n",
    "        y_serie = sim(curr_model, test_set, 0, RNN)\n",
    "        score = mean_squared_error(np.reshape(y_serie, -1), test_set[0][:, col-1])\n",
    "        if n == min_num:\n",
    "            best_model = curr_model\n",
    "            best_score = score\n",
    "        else:\n",
    "            if score < best_score:\n",
    "                best_model = curr_model\n",
    "                best_score = score\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analogiczna funkcja automatyzująca dobór sieci o dwóch warstwach ukrytych\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "def sweep_two_layers(min_num1, max_num1, min_num2, max_num2, test_single,\n",
    "                    RNN, test_set, train_set):\n",
    "    col = len(test_set[0][0])\n",
    "    for n in range(min_num1, max_num1+1):\n",
    "        for m in range(min_num2, max_num2+1):\n",
    "            print(f'Trenuję sieć o strukturze {n}-{m}-1')\n",
    "            model = Sequential()\n",
    "            model.add(Dense(units=n, input_dim = col-1, activation = 'tanh'))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(units=m, activation='tanh'))\n",
    "            model.add(Dropout(0.5))\n",
    "            model.add(Dense(1, activation='linear'))\n",
    "            curr_model = simulation(test_single, model, RNN, test_set, train_set)\n",
    "            y_serie = sim(curr_model, test_set, 0, RNN)\n",
    "            score = mean_squared_error(np.reshape(y_serie, -1), test_set[0][:, col-1])\n",
    "            if m == min_num2 and n == min_num1:\n",
    "                best_model = curr_model\n",
    "                best_score = score\n",
    "            else:\n",
    "                if score < best_score:\n",
    "                    best_model = curr_model\n",
    "                    best_score = score\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trenuję sieć o strukturze 7-1\n",
      "\tWytrenowano 1 z 5 sieci danej struktury\n",
      "\tWytrenowano 2 z 5 sieci danej struktury\n",
      "\tWytrenowano 3 z 5 sieci danej struktury\n",
      "\tWytrenowano 4 z 5 sieci danej struktury\n",
      "\tWytrenowano 5 z 5 sieci danej struktury\n"
     ]
    }
   ],
   "source": [
    "model_best = sweep_one_layer(7, 7, 5, 0, test_set, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trenuję sieć o strukturze 12-8-1\n",
      "\tWytrenowano 1 z 1 sieci danej struktury\n"
     ]
    }
   ],
   "source": [
    "model_best = sweep_two_layers(12, 12, 8, 8, 1, 0, test_set, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005601180353552608\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e18662d940>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dill\n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plotter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "set_num = 0\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "col = len(test_set[0][0])\n",
    "y_serie = model_best.predict(test_set[set_num][:, :col-1])\n",
    "print(mean_squared_error(np.reshape(y_serie, -1), test_set[set_num][:, col-1]))\n",
    "plotter.plot(np.reshape(y_serie, -1), label='Predykcja', lw=1)\n",
    "plotter.plot(test_set[set_num][:, col-1], label='Rzeczywistosc', lw=1)\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0.037394571355098785\n"
     ]
    }
   ],
   "source": [
    "#symulacja wyuczonego modelu i porównanie z targetem\n",
    "def comparison(model, test_set, set_num):\n",
    "    col = len(test_set[0][0])\n",
    "    y_serie = sim(model, test_set, set_num, 0)\n",
    "    print(col)\n",
    "    print(mean_squared_error(np.reshape(y_serie, -1), test_set[set_num][:, col-1]))\n",
    "    plotter.plot(np.reshape(y_serie, -1), label='Predykcja', lw=1)\n",
    "    plotter.plot(test_set[set_num][:, col-1], label='Rzeczywistosc', lw=1)\n",
    "    plotter.legend()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plotter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "comparison(model_best, test_set, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0067733304538036086\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1f111c71710>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Symulacja uzyskanego modelu i porównanie z targetem. Dodatkowo MSE.\n",
    "#problem ze stanem 0\n",
    "#problem z rozjazdem\n",
    "\n",
    "#Ze względu na klęskę modeli z apriorycznie wybieranymi wejsciami stosujemy backward regression\n",
    "col = len(test_set[0][0])\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plotter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#y_serie = model_best.predict(test_set[0][:, 0:col-1])\n",
    "y_serie = sim(model_best, test_set, 0, 0)\n",
    "print(mean_squared_error(np.reshape(y_serie, -1), test_set[0][:, col-1]))\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plotter.plot(np.reshape(y_serie, -1), label='Predykcja', lw=1)\n",
    "plotter.plot(test_set[0][:, col-1], label='Rzeczywistosc', lw=1)\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for i in sorted(indices, reverse=True):\n",
    "    train_set = np.delete(train_set, i, axis=1)\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbioru testowego początkowo zawierającego wszystkie atrybuty\n",
    "#zbiór testowy pochodzi z prawidłowej pracy\n",
    "#uzasadnienie doboru w grafice zbior_uczacy.png\n",
    "import numpy as np\n",
    "test_set=[]\n",
    "#02/03/2017\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "#22/03/2017\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "#29/03/2017\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "#13/04/2017\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "#19/04/2017\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "#28/04/2017\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "#12/05/2017\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "#17/05/2017\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "for i in range(0, len(test_set)):\n",
    "    test_set[i] = test_set[i][~np.isnan(test_set[i]).any(axis=1)]\n",
    "    col = len(test_set[i][0])\n",
    "    test_set[i][:, [19, col-1]] = test_set[i][:, [col-1, 19]]\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set[j] = np.delete(test_set[j], i, axis=1)\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbior testowy z danych z etykietą Voltage Dips\n",
    "#Dane z 14/03/2017\n",
    "import numpy as np\n",
    "test_set_vdips=[]\n",
    "test_set_vdips.append(vdips_all.values[250000:270000, :])\n",
    "test_set_vdips.append(vdips_all.values[380000:400000, :])\n",
    "test_set_vdips.append(vdips_all.values[500000:520000, :])\n",
    "test_set_vdips.append(vdips_all.values[820000:840000, :])\n",
    "test_set_vdips.append(vdips_all.values[980000:1000000, :])\n",
    "test_set_vdips.append(vdips_all.values[1100000:1120000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_vdips[i] = test_set_vdips[i][~np.isnan(test_set_vdips[i]).any(axis=1)]\n",
    "    col = len(test_set_vdips[i][0])\n",
    "    test_set_vdips[i][:, [19, col-1]] = test_set_vdips[i][:, [col-1, 19]]\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_vdips)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_vdips[j] = np.delete(test_set_vdips[j], i, axis=1)\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbiorów testowych z etykietą Surge\n",
    "import numpy as np\n",
    "test_set_surge=[]\n",
    "#17/01/2017\n",
    "test_set_surge.append(surge_all.values[150000:400000, :])\n",
    "#18/01/2017\n",
    "test_set_surge.append(surge_all.values[570000:800000, :])\n",
    "test_set_surge.append(surge_all.values[900000:1030000, :])\n",
    "test_set_surge.append(surge_all.values[1113000:1150000, :])\n",
    "test_set_surge.append(surge_all.values[1280000:1305000, :])\n",
    "test_set_surge.append(surge_all.values[1350000:1380000, :])\n",
    "test_set_surge.append(surge_all.values[1580000:1605000, :])\n",
    "#20/01/2017\n",
    "test_set_surge.append(surge_all.values[2700000:2900000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_surge[i] = test_set_surge[i][~np.isnan(test_set_surge[i]).any(axis=1)]\n",
    "    col = len(test_set_surge[i][0])\n",
    "    test_set_surge[i][:, [19, col-1]] = test_set_surge[i][:, [col-1, 19]]\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_surge)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_surge[j] = np.delete(test_set_surge[j], i, axis=1)\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testuję dodanie kolumny numer 0\n",
      "\tMSE na zbiorze normalnym 3.5917756376831626e-05\n",
      "Testuję dodanie kolumny numer 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-c55e0f4e17cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[0mused_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m27\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m29\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m11\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m26\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0my_serie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mscore_normal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_serie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mused_test\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtr_len\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0merr_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mscore_normal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cbef1060a58d>\u001b[0m in \u001b[0;36msim2\u001b[1;34m(model, test_set, set_num, isrec)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misrec\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0my_serie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_serie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             cache_size=self.cache_size)\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_sparse_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#tym razem forward regression, będziemy stopniowo dodawać kolumny\n",
    "#7\n",
    "import numpy as np\n",
    "import dill\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "with open('train_set.pkl', 'rb') as f:\n",
    "    train_set = dill.load(f)\n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "with open('surge_test.pkl', 'rb') as f:\n",
    "    test_set_surge = dill.load(f)   \n",
    "used_train = []\n",
    "col = len(train_set[0]) - 1\n",
    "act_list = [27, 29, 20, 5, 11, 26, 2]\n",
    "for i in range(0, col-1):\n",
    "    if i not in act_list:\n",
    "        err_sum = 0\n",
    "        used_train = train_set[:, [col-1, 27, 29, 20, 5, 11, 26, 2, i, col]]\n",
    "        used_test = []\n",
    "        tr_len = len(used_train[0]) - 1\n",
    "        model = SVR(kernel='rbf', epsilon=0.01, C=0.75, gamma='auto')\n",
    "        model.fit(used_train[:, :tr_len], used_train[:, tr_len])\n",
    "        print(f'Testuję dodanie kolumny numer {i}')\n",
    "#wariant testowania pojedynczego case'a - żeby poprawić jakiś konkretny\n",
    "        for j in range(0, len(test_set)):    \n",
    "            used_test.append(test_set[j][:, [col-1, 27, 29, 20, 5, 11, 26, 2, i, col]])\n",
    "        y_serie = sim2(model, used_test, 8, 0)\n",
    "        score_normal = mean_squared_error(np.reshape(y_serie, -1), used_test[8][:, tr_len])       \n",
    "        err_sum += score_normal\n",
    "#wariant testowania średniej ze wszystkich test-case'ow\n",
    "#         for j in range(0, len(test_set)):    \n",
    "#             used_test.append(test_set[j][:, [col-1, 27, 29, 21, i, col]])\n",
    "#             y_serie = sim2(model, used_test, j, 0)   \n",
    "#             score_normal = mean_squared_error(np.reshape(y_serie, -1), used_test[j][:, tr_len])\n",
    "#             err_sum += score_normal\n",
    "        err_sum = err_sum / len(test_set)\n",
    "        print(f'\\tMSE na zbiorze normalnym {err_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie okrojonych zbiorów uczących i testowych dla nowego zestawy kolumn\n",
    "#nie wiem dlaczego funkcja do usuwania zestawu kolumn zacina kernela\n",
    "#edit:ostateczny opis pozostałych sygnałów w sygnaly.xlsx arkusz2\n",
    "import dill\n",
    "import numpy as np\n",
    "with open('train_set.pkl', 'rb') as f:\n",
    "    train_set = dill.load(f)\n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "with open('surge_test.pkl', 'rb') as f:\n",
    "    test_set_surge = dill.load(f)\n",
    "with open('vdips_test.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "col = len(train_set[0]) - 1\n",
    "train_set = train_set[:, [col-1, 27, 29, 20, 5, 11, 26, 2, col]]\n",
    "with open('train_set_new.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "for i in range(0, len(test_set)):\n",
    "    test_set[i] = test_set[i][:, [col-1, 27, 29, 20, 5, 11, 26, 2, col]]\n",
    "with open('test_set_new.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)\n",
    "for i in range(0, len(test_set_surge)):\n",
    "    test_set_surge[i] = test_set_surge[i][:, [col-1, 27, 29, 20, 5, 11, 26, 2, col]]\n",
    "with open('test_set_surge_new.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_vdips[i] = test_set_vdips[i][:, [col-1, 27, 29, 20, 5, 11, 26, 2, col]]\n",
    "with open('test_set_vdips_new.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001738447219300133\n"
     ]
    }
   ],
   "source": [
    "#symulacja na nowych zbiorach\n",
    "def compare(model, test_set, set_num):\n",
    "    col = len(test_set[0][0])\n",
    "    y_serie = sim2(model, test_set, set_num, 0)\n",
    "    print(mean_squared_error(np.reshape(y_serie, -1), test_set[set_num][:, col-1]))\n",
    "    res = np.subtract(np.reshape(y_serie, -1), test_set[set_num][:, col-1])\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.subplot(2, 1, 1)\n",
    "    plotter.title('Test predykcji')\n",
    "    plotter.plot(np.reshape(y_serie, -1), label='Predykcja', lw=1)\n",
    "    plotter.plot(test_set[set_num][:, col-1], label='Rzeczywistosc', lw=1)\n",
    "#     plotter.plot(test_set[set_num][:, 1], label='Input1', lw=1)\n",
    "#     plotter.plot(test_set[set_num][:, 2], label='Input2', lw=1)\n",
    "#     plotter.plot(test_set[set_num][:, 3], label='Input3', lw=1)\n",
    "#     plotter.plot(test_set[set_num][:, 4], label='Input4', lw=1)\n",
    "#     plotter.plot(test_set[set_num][:, 5], label='Input5', lw=1)\n",
    "    plotter.legend()\n",
    "    plotter.subplot(2, 1, 2)\n",
    "    plotter.stackplot(range(0, len(res)), res, labels=['Residuum'], baseline='zero', lw=1)\n",
    "    plotter.legend()\n",
    "    \n",
    "import dill\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plotter\n",
    "import numpy as np\n",
    "with open('train_set_new.pkl', 'rb') as f:\n",
    "    train_set = dill.load(f)\n",
    "with open('test_set_new.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "with open('test_set_surge_new.pkl', 'rb') as f:\n",
    "    test_set_surge = dill.load(f)\n",
    "with open('test_set_vdips_new.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "col = len(test_set[0][0])\n",
    "# model = SVR(kernel='rbf', epsilon=0.0075, C=1.05, gamma='auto')\n",
    "# model.fit(train_set[:, 0:col-1], train_set[:, col-1])\n",
    "# print('Wytrenowałem')\n",
    "compare(model, test_set_surge, 1)\n",
    "#do poprawy: 9, 11, 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obecnie wygrywa model o epsilon = 0.0075 i C = 0.75\n",
      "Obecnie wygrywa model o epsilon = 0.0075 i C = 0.8\n",
      "Obecnie wygrywa model o epsilon = 0.0075 i C = 1.0500000000000003\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-15589faaa86e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m             \u001b[0my_serie\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msim2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_serie\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_set\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mscore_best\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cbef1060a58d>\u001b[0m in \u001b[0;36msim2\u001b[1;34m(model, test_set, set_num, isrec)\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misrec\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m                 \u001b[0msample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0my_serie\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my_serie\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    325\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 327\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 350\u001b[1;33m             cache_size=self.cache_size)\n\u001b[0m\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_sparse_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#wybór najlepszych hyperparametrów modelu\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import SVR\n",
    "import dill\n",
    "with open('train_set_new.pkl', 'rb') as f:\n",
    "    train_set = dill.load(f)\n",
    "with open('test_set_new.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "with open('test_set_surge_new.pkl', 'rb') as f:\n",
    "    test_set_surge = dill.load(f)\n",
    "with open('test_set_vdips_new.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "col = col = len(test_set[0][0])\n",
    "eps = 0.0075\n",
    "pen = 0.75\n",
    "score_best = 10\n",
    "while eps <= 2:\n",
    "    pen = 0.75\n",
    "    while pen <= 2:\n",
    "        model = SVR(kernel='rbf', epsilon=eps, C=pen, gamma='auto')\n",
    "        model.fit(train_set[:, 0:col-1], train_set[:, col-1])\n",
    "        score = 0\n",
    "        for i in range(0, len(test_set)):\n",
    "            y_serie = sim2(model, test_set, i, 0)\n",
    "            score += mean_squared_error(np.reshape(y_serie, -1), test_set[i][:,col-1])\n",
    "        if score < score_best:\n",
    "            score_best = score\n",
    "            eps_best = eps\n",
    "            pen_best = pen\n",
    "            print(f'Obecnie wygrywa model o epsilon = {eps_best} i C = {pen_best}')\n",
    "        pen +=0.05\n",
    "    eps += 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim2(model, test_set, set_num, isrec):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0:\n",
    "            sample = test_set[set_num][0:1, 0:col-1]\n",
    "            if isrec == 1:\n",
    "                sample = np.reshape(sample, (-1, 1, col-1))\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = test_set[set_num][i:i+1, 0:col-1]\n",
    "            sample[0, 0] = y\n",
    "            if isrec == 1:\n",
    "                sample = np.reshape(sample, (-1, 1, col-1))\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    return y_serie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
