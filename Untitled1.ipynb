{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#init cell\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data.values[:, col_num], linewidth=0.5)\n",
    "\t\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column2(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data[:, col_num], linewidth=0.5)\n",
    "    \n",
    "import dill\n",
    "with open('train_set.pkl', 'rb') as  f:\n",
    "    train_set = dill.load(f)\n",
    "    \n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "    \n",
    "with open('train_set.pkl', 'rb') as  f:\n",
    "    train_set = dill.load(f)\n",
    "    \n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "    \n",
    "with open('surge_test.pkl', 'rb') as  f:\n",
    "    test_set_surge = dill.load(f)\n",
    "    \n",
    "with open('vdips_test.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "\n",
    "import numpy as np\n",
    "def sim(model, test_set, set_num):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0:\n",
    "            sample = np.copy(test_set[set_num][0:1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "def sim_with_refreshes(model, test_set\n",
    "                       ,set_num, refr_period):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0 or i%refr_period == 0:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "def eval_prediction(y_pred, y_target, do_plot):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    if do_plot == 1:\n",
    "        \n",
    "        import matplotlib.pyplot as plotter\n",
    "        %matplotlib qt\n",
    "\n",
    "        figManager = plotter.get_current_fig_manager()\n",
    "        figManager.window.showMaximized()\n",
    "\n",
    "        plotter.subplot(2, 1, 1)\n",
    "        plotter.plot(y_pred[:, 0], label='Prediction', lw=1)\n",
    "        plotter.plot(y_target, label='Target', lw=1)\n",
    "        plotter.legend()\n",
    "        plotter.grid(b=True, which='both', axis='y')\n",
    "\n",
    "        residuals = np.copy(y_pred[:, 0])\n",
    "        for i in range(0, len(residuals)):\n",
    "            residuals[i] = residuals[i] - y_target[i]\n",
    "        residuals = residuals.flatten()\n",
    "\n",
    "        plotter.subplot(2, 1, 2)\n",
    "        plotter.fill_between(range(0, len(residuals))\n",
    "                             ,residuals, label='Residuals', lw=1)\n",
    "        plotter.legend()\n",
    "        plotter.grid(b=True, which='both', axis='y')\n",
    "    \n",
    "    score = mse(y_pred[:, 0], y_target)\n",
    "    print(score)\n",
    "    return score\n",
    "    \n",
    "def multi_train_NLP(structs_list, patience, batch_size, \n",
    "                    epochs, train_data, test_data, set_num\n",
    "                   ,criterion, single_times, dropout_2list):\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "    sys.stdout = open('log.txt', 'w')\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score=1\n",
    "    \n",
    "    struct_num = 0\n",
    "    for structure in structs_list:\n",
    "        print(f'Trenuje siec o struturze {structure}')\n",
    "        curr_model = train_MLP_Ntimes(N=single_times\n",
    "                                     ,structure=structure\n",
    "                                     ,patience=patience\n",
    "                                     ,batch_size=batch_size\n",
    "                                     ,epochs=epochs\n",
    "                                     ,set_num=set_num\n",
    "                                     ,train_data=train_data\n",
    "                                     ,test_data=test_data\n",
    "                                     ,debug_log='no'\n",
    "                                     ,criterion=criterion\n",
    "                                     ,dropout_list=dropout_2list[struct_num])\n",
    "    \n",
    "        y = sim(curr_model, test_data, set_num)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            curr_score = mse(y[:, 0], test_data[set_num][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for i in range(0, len(residuals)):\n",
    "                residuals[i] = residuals[i] - test_data[set_num][i, -1]\n",
    "            curr_score = np.max(residuals)      \n",
    "            \n",
    "        print(f'Wynik biezącej sieci: {curr_score}')\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_model = curr_model\n",
    "            best_score = curr_score\n",
    "            \n",
    "        struct_num = struct_num + 1\n",
    "    sys.stdout = bck_stdout\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    return(best_model)\n",
    "\n",
    "def train_MLP_Ntimes(N, structure, patience, batch_size, epochs\n",
    "                    ,train_data, test_data, set_num\n",
    "                    ,debug_log, criterion, dropout_list):\n",
    "    best_score = 1\n",
    "    nets_trained = 0\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        import sys\n",
    "        bck_stdout = sys.stdout\n",
    "        sys.stdout = open('log.txt', 'w')\n",
    "    \n",
    "    while nets_trained < N:\n",
    "        model = train_NLP(structure=structure\n",
    "                         ,patience=patience\n",
    "                         ,batch_size=batch_size\n",
    "                         ,epochs=epochs\n",
    "                         ,train_data=train_data\n",
    "                         ,dropout_list=dropout_list)\n",
    "        y = sim(model, test_data, set_num)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = eval_prediction(y, test_set[set_num][:, -1], 0)\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_data[set_num][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "        print(f\"Wynik sieci nr {nets_trained+1}: {score}\")\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "        del model\n",
    "        nets_trained = nets_trained + 1\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'sound.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        sys.stdout = bck_stdout\n",
    "    return best_model\n",
    "\n",
    "def train_NLP(structure, patience,batch_size\n",
    "              ,epochs, train_data, dropout_list):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras import regularizers\n",
    "    \n",
    "    inputs_count = len(train_data[0]) - 1\n",
    "    layer_num = 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer_size in structure:\n",
    "        if layer_num == 1:\n",
    "            model.add(Dense(layer_size, input_dim=inputs_count, \n",
    "                            activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "        else:\n",
    "            model.add(Dense(layer_size, activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "        layer_num = layer_num + 1\n",
    "        \n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                   verbose=1, patience=patience)\n",
    "\n",
    "    model.fit(train_data[:, :-1],\n",
    "              train_data[:, -1], batch_size=batch_size,\n",
    "              epochs=epochs, validation_split = 0.35,\n",
    "              callbacks=[es], workers=8, use_multiprocessing=1)\n",
    "    return model\n",
    "\n",
    "def train_SVR(C, epsilon, train_data):\n",
    "    from sklearn.svm import SVR as svr\n",
    "    model = svr(kernel='rbf', gamma='scale', C=C, epsilon=epsilon)\n",
    "    model.fit(train_data[:, :-1], train_data[:, -1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def multi_train_SVR(C_eps_pairs\n",
    "                   ,train_data\n",
    "                   ,test_data\n",
    "                   ,set_num\n",
    "                   ,criterion):\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "   # sys.stdout = open('log.txt', 'w')\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse \n",
    "    best_score = 1\n",
    "\n",
    "    for pair in C_eps_pairs:\n",
    "        print(f'Trenuje SVR o C={pair[0]} i epsilon={pair[1]}')\n",
    "        curr_model = train_SVR(C=pair[0]\n",
    "                               ,epsilon=pair[1]\n",
    "                               ,train_data=train_data)\n",
    "    \n",
    "        y = sim(curr_model, test_data, set_num)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            curr_score = mse(y[:, 0], test_data[set_num][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for i in range(0, len(residuals)):\n",
    "                residuals[i] = residuals[i] - test_data[set_num][i, -1]\n",
    "            curr_score = np.max(residuals)\n",
    "        \n",
    "    \n",
    "        print(f'Wynik biezacego SVRa: {curr_score}')\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "\n",
    "    sys.stdout = bck_stdout  \n",
    "    return best_model\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    #based on tutorial:\n",
    "    #https://towardsdatascience.com/\n",
    "    #time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python-6ceee9c6c651\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "with open('ts_trim_test_in.pkl', 'rb') as  f:\n",
    "    TXs = dill.load(f)\n",
    "    \n",
    "with open('ts_trim_test_out.pkl', 'rb') as f:\n",
    "    TYs = dill.load(f)\n",
    "    \n",
    "with open('ts_trim_train_in.pkl', 'rb') as  f:\n",
    "    Xs = dill.load(f)\n",
    "    \n",
    "with open('ts_trim_train_out.pkl', 'rb') as f:\n",
    "    Ys = dill.load(f)\n",
    "    \n",
    "def prepare_timeseries(train_set, test_set_list, prepare):\n",
    "    pd_X = pd.DataFrame(data=train_set[:, :-1])\n",
    "    pd_Y = pd.DataFrame(data=train_set[:, -1])\n",
    "\n",
    "    pd_TX = []\n",
    "    pd_TY = []\n",
    "\n",
    "    for test in test_set_list:\n",
    "        pd_TX.append(pd.DataFrame(test[:, :-1]))\n",
    "        pd_TY.append(pd.DataFrame(test[:, -1]))\n",
    "\n",
    "    print(f'Sekwencjonuje zbior uczacy')\n",
    "    Xs, Ys = create_dataset(pd_X, pd_Y, prepare)\n",
    "\n",
    "    with open('ts_trim_train_in.pkl', 'wb') as f:\n",
    "        dill.dump(Xs,f)\n",
    "\n",
    "    with open('ts_trim_train_out.pkl', 'wb') as f:\n",
    "        dill.dump(Ys,f)\n",
    "\n",
    "    TXs = []\n",
    "    TYs = []\n",
    "    \n",
    "    for i in range(0, len(pd_TX)):\n",
    "        print(f'Sekswencjonuje zbior testowy nr: {i}')\n",
    "        txs, tys = create_dataset(pd_TX[i], pd_TY[i], prepare)\n",
    "        TXs.append(txs)\n",
    "        TYs.append(tys)\n",
    "        \n",
    "    with open('ts_trim_test_in.pkl', 'wb') as f:\n",
    "        dill.dump(TXs,f)\n",
    "\n",
    "    with open('ts_trim_test_out.pkl', 'wb') as f:\n",
    "        dill.dump(TYs,f)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs\n",
    "\n",
    "def train_LSTM(structure, patience\n",
    "              ,epochs, batch_size\n",
    "              ,train_X, train_Y):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import Activation\n",
    "    from keras.layers import LSTM\n",
    "    from keras.callbacks import EarlyStopping    \n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(0, len(structure)):\n",
    "        if i == 0:\n",
    "            model.add(LSTM(units=structure[i]\n",
    "                          ,input_shape=(Xs.shape[1], Xs.shape[2])\n",
    "                          ,activation='tanh'))\n",
    "        else:\n",
    "            model.add(Dense(units=structure[i]\n",
    "                           ,activation='tanh'))\n",
    "        \n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(loss='mean_squared_error'\n",
    "                  ,optimizer='adam')\n",
    "\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min'\n",
    "                      ,verbose=1, patience=patience)\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.35,\n",
    "              verbose=1,\n",
    "              shuffle=False,\n",
    "              callbacks=[es])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def multi_train_LSTM(structure_list, patience\n",
    "                    ,epochs, batch_size\n",
    "                    ,train_X, train_Y\n",
    "                    ,test_X, test_Y, set_num):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "    sys.stdout = open('log.txt', 'w')\n",
    "    best_score = 1\n",
    "    \n",
    "    for struct in structure_list:\n",
    "        print(f'Trenuje siec o strukturze: {struct}')\n",
    "        curr_model = train_LSTM(struct, patience=patience\n",
    "                               ,epochs=epochs, batch_size=batch_size\n",
    "                               ,train_X=train_X, train_Y=train_Y)\n",
    "        \n",
    "        pred = curr_model.predict(test_X[set_num])\n",
    "        curr_score = mse(pred[:, 0], test_Y[set_num])\n",
    "        print(f'Wynik biezacej sieci: {curr_score}')\n",
    "        \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "            \n",
    "    sys.stdout = bck_stdout\n",
    "    return best_model\n",
    "\n",
    "def generate_SVR_params(C_min, C_max, C_step\n",
    "                       ,eps_min, eps_max, eps_step):\n",
    "    C_max = C_max + C_step\n",
    "    C_vals = np.arange(C_min, C_max, C_step)\n",
    "    \n",
    "    eps_max = eps_max + eps_step\n",
    "    eps_vals = np.arange(eps_min, eps_max, eps_step)\n",
    "    \n",
    "    params_list = []\n",
    "    for C in C_vals:\n",
    "        for eps in eps_vals:\n",
    "            params_list.append([C, eps])\n",
    "    \n",
    "    return params_list\n",
    "\n",
    "def try_add_column(cols_taken, test_col, criterion, train_set\n",
    "                  ,test_set, model_type):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    col = len(train_set[0])\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, col):\n",
    "        if i not in cols_taken:\n",
    "            cols_test = [i]\n",
    "            for _col in cols_taken:\n",
    "                cols_test.append(_col)\n",
    "\n",
    "            train_set_trim = train_set[:, cols_test]\n",
    "            test_set_trim = []\n",
    "            for test in test_set:\n",
    "                test_set_trim.append(test[:, cols_test])\n",
    "            \n",
    "            if model_type == 'SVR':\n",
    "                model = train_SVR(C=0.4, epsilon=0.014\n",
    "                                  ,train_data=train_set_trim)\n",
    "            \n",
    "            pred = sim(model, test_set_trim, test_col)\n",
    "            \n",
    "            if criterion == 'mse':\n",
    "                score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "            if criterion == 'max':\n",
    "                residuals = np.copy(pred[:, 0])\n",
    "                for j in range(0, len(residuals)):\n",
    "                    residuals[j] = residuals[j] - \\\n",
    "                    test_set_trim[test_col][j, -1]\n",
    "                score = np.max(np.abs(residuals))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_model = model\n",
    "                best_score = score\n",
    "                best_col = i\n",
    "\n",
    "            print(cols_test)\n",
    "            print(f'Test kolumny {i}: wynik: {score}')\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "    \n",
    "def try_drop_column(start_cols, test_col, criterion\n",
    "                   ,train_set ,test_set, model_type):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, len(base_cols)):\n",
    "        cols_test = np.copy(base_cols)\n",
    "        cols_test = np.delete(base_cols, i)\n",
    "\n",
    "        train_set_trim = train_set[:, cols_test]\n",
    "        test_set_trim = []\n",
    "        for test in test_set:\n",
    "            test_set_trim.append(test[:, cols_test])\n",
    "        \n",
    "        if model_type == 'SVR':\n",
    "            model = train_SVR(C=0.4, epsilon=0.014\n",
    "                              ,train_data=train_set_trim)\n",
    "        \n",
    "        pred = sim(model, test_set_trim, test_col)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(pred[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_set_trim[test_col][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "\n",
    "        if score < best_score:\n",
    "            best_model = model\n",
    "            best_score = score\n",
    "            best_col = i\n",
    "\n",
    "        print(cols_test)\n",
    "        print(f'Test kolumny {base_cols[i]}: wynik: {score}')\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "\n",
    "def dump_timeseries(Xs, Ys, TXs, TYs, seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Xs,f)\n",
    "\n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Ys,f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TXs,f)\n",
    "\n",
    "    with open(f'ts_trim_test_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TYs,f)\n",
    "        \n",
    "def load_timeseries(seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'rb') as f:\n",
    "        Xs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'rb') as f:\n",
    "        Ys = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'rb') as f:\n",
    "        TXs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'rb') as f:\n",
    "        TXs = dill.load(f)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as  f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('model7.pkl', 'rb') as f:\n",
    "    model_best = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych z normalnej pracy\n",
    "#okreslenie poprawności pracy na podstawie dołączonego arkusza\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tLICZBA PRÓBEK\n",
    "# \t02/03/2017\t\t0-2\t\t\t\t397721\n",
    "# \t06/03/2017\t\t3\t\t\t\t48513\n",
    "# \t16/03/2017\t\t4\t\t\t\t518348\n",
    "# \t22/03/2017\t\t5\t\t\t\t644888\n",
    "# \t23/03/2017\t\t6\t\t\t\t408255\n",
    "# \t29/03/2017\t\t7-8\t\t\t\t517850\n",
    "# \t13/04/2017\t\t9-13\t\t\t120844\n",
    "# \t19/04/2017\t\t14-17\t\t\t636238\n",
    "# \t28/04/2017\t\t18\t\t\t\t227459\n",
    "# \t12/05/2017\t\t19-20\t\t\t332583\n",
    "# \t17/05/2017\t\t21\t\t\t\t1993321\n",
    "# \t07/06/2017\t\t22-23\t\t\t183853\n",
    "# \t21/08/2017\t\t24-31\t\t\t738162\n",
    "# \t22/08/2017\t\t32-35\t\t\t1952183\n",
    "# \t23/08/2017\t\t36-37\t\t\t287377\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\fine\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_fine = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_fine.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma w danych z etykietą Voltage Dips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_fine)):\n",
    "    data_fine[i] = data_fine[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja dodająca do zbiorów kolumny z przesuniętymi w czasie prękościami obrotowymi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def add_history(dataset):\n",
    "    for x in range(0, len(dataset)):\n",
    "        temp = np.concatenate(([dataset[x].values[0, 19]], dataset[x].values[0:-1, 19]))\n",
    "        temp2 = np.concatenate(([dataset[x].values[0, 20]], dataset[x].values[0:-1, 20]))\n",
    "        dataset[x].insert(42, 42, temp)\n",
    "        dataset[x].insert(43, 43, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dodanie dwóch kolumn z przesuniętymi w czasie prędkościami obrotowymi\n",
    "add_history(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja usuwająca zakresy wierszy ze zbioru danych\n",
    "import pandas as pd\n",
    "def drop_rows(dataset, beg, end):\n",
    "    dataset.drop(dataset.index[beg:end], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnych obserwacji- długie stany ustalone\n",
    "#wybór na podstawie analizy wykresów przebiegów\n",
    "#zbiory po przetworzeniu:\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tPRÓBEK\t\n",
    "# \t02/03/2017\t\t0-1\t\t\t\t194697\t\t\t\n",
    "# \t06/03/2017\t\t2\t\t\t\t48513\n",
    "# \t16/03/2017\t\t3\t\t\t\t128348\n",
    "# \t22/03/2017\t\t4\t\t\t\t299888\n",
    "# \t23/03/2017\t\t5\t\t\t\t193255\n",
    "# \t29/03/2017\t\t6\t\t\t\t157116\n",
    "# \t13/04/2017\t\t7-11\t\t\t120844\n",
    "# \t19/04/2017\t\t12-13\t\t\t89385\n",
    "# \t28/04/2017\t\t14\t\t\t\t127459\n",
    "# \t12/05/2017\t\t15-16\t\t\t107583\n",
    "# \t17/05/2017\t\t17\t\t\t\t693321\n",
    "# \t07/06/2017\t\t18-19\t\t\t123853\n",
    "# \t21/08/2017\t\t20-22\t\t\t296545\n",
    "# \t22/08/2017\t\t23-25\t\t\t293511\n",
    "# \t23/08/2017\t\t26-27\t\t\t41377\n",
    "drop_rows(data_fine[2], 100000, 300000)\n",
    "drop_rows(data_fine[4], 370000, 500000)\n",
    "drop_rows(data_fine[4], 220000, 340000)\n",
    "drop_rows(data_fine[4], 60000, 200000)\n",
    "drop_rows(data_fine[5], 520000, 620000)\n",
    "drop_rows(data_fine[5], 460000, 500000)\n",
    "drop_rows(data_fine[5], 360000, 390000)\n",
    "drop_rows(data_fine[5], 140000, 265000)\n",
    "drop_rows(data_fine[5], 50000, 100000)\n",
    "drop_rows(data_fine[6], 350000, 360000)\n",
    "drop_rows(data_fine[6], 300000, 320000)\n",
    "drop_rows(data_fine[6], 210000, 270000)\n",
    "drop_rows(data_fine[6], 140000, 190000)\n",
    "drop_rows(data_fine[6], 50000, 125000)\n",
    "drop_rows(data_fine[8], 410000, 450000)\n",
    "drop_rows(data_fine[8], 140000, 375000)\n",
    "drop_rows(data_fine[8], 0, 35000)\n",
    "drop_rows(data_fine[15], 65000, -1)\n",
    "drop_rows(data_fine[17], 20000, 100000)\n",
    "drop_rows(data_fine[18], 100000, 200000)\n",
    "drop_rows(data_fine[19], 210000, 240000)\n",
    "drop_rows(data_fine[19], 0, 150000)\n",
    "drop_rows(data_fine[20], 10000, 55000)\n",
    "drop_rows(data_fine[21], 1300000, 1750000)\n",
    "drop_rows(data_fine[21], 250000, 1100000)\n",
    "drop_rows(data_fine[23], 90000, 110000)\n",
    "drop_rows(data_fine[23], 40000, 80000)\n",
    "drop_rows(data_fine[29], 165000, 180000)\n",
    "drop_rows(data_fine[29], 100000, 140000)\n",
    "drop_rows(data_fine[29], 32000, 75000)\n",
    "drop_rows(data_fine[30], 150000, 180000)\n",
    "drop_rows(data_fine[30], 125000, 140000)\n",
    "drop_rows(data_fine[30], 66000, 92000)\n",
    "drop_rows(data_fine[30], 25000, 50000)\n",
    "drop_rows(data_fine[31], 85000, 160000)\n",
    "drop_rows(data_fine[31], 20000, 65000)\n",
    "drop_rows(data_fine[33], 400000, 500000)\n",
    "drop_rows(data_fine[33], 230000, 375000)\n",
    "drop_rows(data_fine[33], 30000, 200000)\n",
    "drop_rows(data_fine[34], 330000, 440000)\n",
    "drop_rows(data_fine[34], 200000, 310000)\n",
    "drop_rows(data_fine[34], 165000, 180000)\n",
    "drop_rows(data_fine[34], 30000, 150000)\n",
    "drop_rows(data_fine[35], 270000, 365000)\n",
    "drop_rows(data_fine[35], 160000, 256000)\n",
    "drop_rows(data_fine[35], 30000, 140000)\n",
    "drop_rows(data_fine[36], 100000, 195000)\n",
    "drop_rows(data_fine[36], 7000, 85000)\n",
    "drop_rows(data_fine[37], 7000, 80000)\n",
    "indices = 0, 7, 14, 16, 24, 25, 26, 27, 28, 32\n",
    "for i in sorted(indices, reverse=True):\n",
    "    del data_fine[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbicie wszystki pomiarów do jednego dataframe\n",
    "# \tDATA\t\t\tPOCZATEK\t\tKONIEC\n",
    "# \t02/03/2017\t\t0\t\t\t\t194696\t\t\t\n",
    "# \t06/03/2017\t\t194697\t\t\t243209\n",
    "# \t16/03/2017\t\t243210\t\t\t371557\n",
    "# \t22/03/2017\t\t371558\t\t\t671445\n",
    "# \t23/03/2017\t\t671446\t\t\t864700\n",
    "# \t29/03/2017\t\t864701\t\t\t1021816\n",
    "# \t13/04/2017\t\t1021817\t\t\t1142660\n",
    "# \t19/04/2017\t\t1142661 \t\t1243538\n",
    "# \t28/04/2017\t\t1243539\t\t\t1370997\n",
    "# \t17/05/2017\t\t1370998\t\t\t693321\n",
    "# \t12/05/2017\t\t1370998 \t\t1478580\t\t\n",
    "# \t07/06/2017\t\t1478579\t\t\t2197611\n",
    "# \t21/08/2017\t\t2197611\t\t\t2497184\n",
    "# \t22/08/2017\t\t2497185\t\t\t2812172\n",
    "# \t23/08/2017\t\t2812173\t\t\t2927187\n",
    "import pandas as pd\n",
    "fine_all = pd.concat(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnej już listy dataframe'ow\n",
    "del data_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(fine_all, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie outlierów\n",
    "#stwierdzone na podstawie analizy wykresów przebiegów\n",
    "indices = [42931, 42932, 42933, 42934, 42935, 42936, 42937, 42938, 42939]\n",
    "for i in sorted(indices, reverse=True):\n",
    "    fine_all = fine_all.drop(i)\n",
    "drop_rows(fine_all, 42900, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapicklowanie przetworzonego zbioru danych pochodzących z normalnej pracy\n",
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:  \n",
    "    dill.dump(fine_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Voltage Dips\n",
    "#Dane pochodzą z jednego dnia 14/03/2016\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\vdips\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_vdips = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_vdips.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumny NaN-ów\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_vdips)):\n",
    "    data_vdips[i] = data_vdips[i].drop(42, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóżnionymi o jedną próbkę wartościami prędkości obrotowych\n",
    "add_history(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie danych do jednego dataframe\n",
    "import pandas as pd\n",
    "vdips_all = pd.concat(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie zbioru\n",
    "import dill\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzenej już listy dataframe'ów\n",
    "del data_vdips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Surge do listy\n",
    "#Stan danych po usunięci dwóch wadliwych dataframe:\n",
    "# \tDATA\t\t\tNR ZBIORU\t\tPRÓBEK\n",
    "# \t17/01/2017\t\t\t0-5\t\t\t411448\n",
    "# \t18/01/2017\t\t\t6-23\t\t1370255\n",
    "# \t19/01/2017\t\t\t24-30\t\t271524\n",
    "# \t20/01/2017\t\t\t31-35\t\t916380\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\surge\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_surge = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_surge.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie uszkodzonych dataFrame\n",
    "#wykonać x2\n",
    "del data_surge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma vdips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_surge)):\n",
    "    data_surge[i] = data_surge[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóźnionymi o jedną próbkę predkościami obrotowymi\n",
    "add_history(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie wszystkich danych w jeden dataframe\n",
    "#Rozkład danych po zbiciu:\n",
    "# \tDATA\t\t\tPOCZĄTEK\t\tKONIEC\n",
    "# \t17/01/2017\t\t0\t\t\t\t411447\n",
    "# \t18/01/2017\t\t411448\t\t\t1781702\n",
    "# \t19/01/2017\t\t1781703\t\t\t2053226\n",
    "# \t20/01/2017\t\t2053227\t\t\t2969606\n",
    "import pandas as pd\n",
    "surge_all = pd.concat(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie serii danych\n",
    "import dill\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzebnej już listy zbiorów\n",
    "del data_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max_df(fine, col_num):\n",
    "    max_val = fine[col_num].max()\n",
    "    return max_val\n",
    "def column_min_df(fine, col_num):\n",
    "    min_val = fine[col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja realizująca normalizacją min-max danych\n",
    "def normalise(dataset, columns_ranges):\n",
    "        for col in range(0, 44):\n",
    "            min_val = columns_ranges[col, 1]\n",
    "            max_val = columns_ranges[col, 0]\n",
    "            dataset[col] = dataset[col].subtract(min_val)\n",
    "            dataset[col] = dataset[col].divide(max_val - min_val)\n",
    "                #dataset.values[row, col] = (value - min_val) / (max_val - min_val)\n",
    "            print(f'Znormalizowano kolumnę {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 9.1525530e+03  1.9722790e+01]\n",
      " [ 9.8520690e+01  1.3057002e+00]\n",
      " [ 1.0054615e+02 -3.5445600e-01]\n",
      " [ 9.9826385e+01 -1.4756944e+00]\n",
      " [ 4.3153214e+01 -3.9785880e-01]\n",
      " [ 4.5782695e+03 -8.4801800e+03]\n",
      " [ 9.5789920e+03 -1.0257523e+04]\n",
      " [ 4.0034723e+04 -7.8993066e+02]\n",
      " [ 1.0552517e+03  1.8359375e+01]\n",
      " [ 9.2881946e+02 -1.1574074e+00]\n",
      " [ 1.1708125e+03 -2.2712173e+00]\n",
      " [ 1.4644820e+05  9.8893234e+04]\n",
      " [-1.9422734e+03 -1.4051648e+04]\n",
      " [ 6.1300000e+01  2.0100000e+01]\n",
      " [ 4.6400000e+01  2.0000000e+01]\n",
      " [ 6.3000000e+01  2.0800000e+01]\n",
      " [ 4.1900000e+01  2.0000000e+01]\n",
      " [ 5.6800000e+01  2.0100000e+01]\n",
      " [ 2.6800000e+01  2.1000000e+01]\n",
      " [ 5.8689000e+03 -6.1500000e+01]\n",
      " [ 5.9616000e+03 -1.5900001e+01]\n",
      " [ 1.9988000e+02 -4.7440000e+01]\n",
      " [ 1.3696000e+02 -2.5320000e+01]\n",
      " [ 6.2600000e+01  1.9200000e+01]\n",
      " [ 5.7630000e+03  0.0000000e+00]\n",
      " [ 6.0000000e+03  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0026765e+02  7.9571760e-02]\n",
      " [ 4.3400000e+01  0.0000000e+00]\n",
      " [ 7.0181000e+02  0.0000000e+00]\n",
      " [ 5.4050000e+02  0.0000000e+00]\n",
      " [ 2.5840000e+01 -4.9000000e-01]\n",
      " [ 1.1100000e+02  0.0000000e+00]\n",
      " [ 4.9870000e+01  0.0000000e+00]\n",
      " [ 7.0214000e+02  2.9174000e+02]\n",
      " [ 5.1900000e+02 -2.0000000e+00]\n",
      " [ 2.7570000e+01 -5.2000000e-01]\n",
      " [ 1.0700000e+02  0.0000000e+00]\n",
      " [ 5.8689000e+03 -6.1500000e+01]\n",
      " [ 5.9616000e+03 -1.5900001e+01]]\n"
     ]
    }
   ],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max_df(fine_all, col)\n",
    "    col_ranges[col, 1] = column_min_df(fine_all, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'vdips_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-e0aace0bd282>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfine_all_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_ranges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mvdips_all_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvdips_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mnormalise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvdips_all_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol_ranges\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vdips_all' is not defined"
     ]
    }
   ],
   "source": [
    "#wstepna normalizacja ułatwiająca porównywanie przebiegów\n",
    "#na wspólnym wykresie.\n",
    "fine_all_norm = fine_all.copy()\n",
    "normalise(fine_all_norm, col_ranges)\n",
    "\n",
    "vdips_all_norm = vdips_all.copy()\n",
    "normalise(vdips_all_norm, col_ranges)\n",
    "\n",
    "surge_all_norm = surge_all.copy()\n",
    "normalise(surge_all_norm, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DOBÓR ZMIENNYCH WEJŚCIOWYCH\n",
    "#obliczenie współczynników korelacji liniowej między atrybutami\n",
    "#przekierowanie korelacji do arkusza w celu łatwiejszego przeglądania\n",
    "Pearsons = fine_all_norm.corr()\n",
    "path = \"E:\\\\9sem\\\\INO\\\\fine_corrs.xls\"\n",
    "Pearsons.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x24b410ef1d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#KONTYNUACJA DOBORU ATRYBUTÓW - OBSERWACJA PRZEBIEGÓW\n",
    "#wykreślenie przebiegu wyjścia i potencjalnych wejść\n",
    "#wstepny dobór wejśc na podstawie analizy korelacji Pearsona (impl:pandas.df.corr) i przebiegów\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plotter.plot(fine_all_norm.values[:, 19], label='target')\n",
    "plotter.plot(fine_all_norm.values[:, 6], label='input6')\n",
    "plotter.plot(fine_all_norm.values[:, 40], label='input40')\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vdips_all_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-cd86ce612105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#usunięcie wstępnych znormalizowanych danych\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0mfine_all_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mvdips_all_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mdel\u001b[0m \u001b[0msurge_all_norm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vdips_all_norm' is not defined"
     ]
    }
   ],
   "source": [
    "#usunięcie wstępnych znormalizowanych danych\n",
    "del fine_all_norm\n",
    "del vdips_all_norm\n",
    "del surge_all_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(fine_all, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353078, 44)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max(np_array, col_num):\n",
    "    max_val = np_array[:, col_num].max()\n",
    "    return max_val\n",
    "def column_min(np_array, col_num):\n",
    "    min_val = np_array[:, col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.2072646e+03  9.7779890e+01]\n",
      " [ 9.8350690e+01  6.8373840e+01]\n",
      " [ 1.0053892e+02  1.2235967e+01]\n",
      " [ 6.4525460e+00 -1.3599538e+00]\n",
      " [ 4.7381365e-01 -3.8339120e-01]\n",
      " [ 3.1481484e+03 -6.0619214e+03]\n",
      " [ 2.0109961e+02 -8.8657400e+03]\n",
      " [ 3.3513453e+04 -7.8125000e+02]\n",
      " [ 1.0544271e+03  1.8489584e+01]\n",
      " [ 9.2650464e+02 -1.1574074e+00]\n",
      " [ 1.1276594e+03 -2.2712173e+00]\n",
      " [ 1.3948930e+05  9.9222370e+04]\n",
      " [-2.3893242e+03 -1.4051648e+04]\n",
      " [ 3.2000000e+01  2.0200000e+01]\n",
      " [ 3.3700000e+01  2.0000000e+01]\n",
      " [ 5.9300000e+01  2.1100000e+01]\n",
      " [ 3.6500000e+01  2.0000000e+01]\n",
      " [ 5.1700000e+01  2.0100000e+01]\n",
      " [ 2.6300000e+01  2.1200000e+01]\n",
      " [ 5.8851000e+03 -1.5000000e+00]\n",
      " [ 5.8851000e+03 -1.5000000e+00]\n",
      " [ 1.4081000e+02 -3.6530000e+01]\n",
      " [ 8.9980000e+01 -2.4690000e+01]\n",
      " [ 5.4300000e+01  1.9800000e+01]\n",
      " [ 5.7630000e+03  0.0000000e+00]\n",
      " [ 5.8650000e+03  0.0000000e+00]\n",
      " [ 1.0000000e+02  9.9900000e+01]\n",
      " [ 1.0000000e+02  1.3000000e+01]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 6.0000000e+00  0.0000000e+00]\n",
      " [ 0.0000000e+00  0.0000000e+00]\n",
      " [ 1.0026765e+02  9.0422450e-02]\n",
      " [ 3.1390000e+01  0.0000000e+00]\n",
      " [ 6.9576000e+02  5.5046000e+02]\n",
      " [ 4.3770000e+02  0.0000000e+00]\n",
      " [ 1.9000000e+01 -3.4000000e-01]\n",
      " [ 1.1000000e+02  0.0000000e+00]\n",
      " [ 4.8610000e+01  0.0000000e+00]\n",
      " [ 6.9999000e+02  4.5582000e+02]\n",
      " [ 4.2800000e+02  0.0000000e+00]\n",
      " [ 2.6710000e+01 -4.0000000e-01]\n",
      " [ 1.0700000e+02  0.0000000e+00]\n",
      " [ 5.7546000e+03 -9.6000000e+00]\n",
      " [ 5.7546000e+03 -9.6000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max(train_set, col)\n",
    "    col_ranges[col, 1] = column_min(train_set, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.2072646e+03  9.7779890e+01]\n",
      " [ 9.8350690e+01  6.8373840e+01]\n",
      " [ 1.0053892e+02  1.2235967e+01]\n",
      " [ 6.4525460e+00 -1.3599538e+00]\n",
      " [ 4.7381365e-01 -3.8339120e-01]\n",
      " [ 3.1481484e+03 -6.0619214e+03]\n",
      " [ 2.0109961e+02 -8.8657400e+03]\n",
      " [ 3.3513453e+04 -7.8125000e+02]\n",
      " [ 1.0544271e+03  1.8489584e+01]\n",
      " [ 9.2650464e+02 -1.1574074e+00]\n",
      " [ 1.1276594e+03 -2.2712173e+00]\n",
      " [ 1.3948930e+05  9.9222370e+04]\n",
      " [-2.3893242e+03 -1.4051648e+04]\n",
      " [ 3.2000000e+01  2.0200000e+01]\n",
      " [ 3.3700000e+01  2.0000000e+01]\n",
      " [ 5.9300000e+01  2.1100000e+01]\n",
      " [ 3.6500000e+01  2.0000000e+01]\n",
      " [ 5.1700000e+01  2.0100000e+01]\n",
      " [ 2.6300000e+01  2.1200000e+01]\n",
      " [ 5.8851000e+03 -1.5000000e+00]\n",
      " [ 5.8851000e+03 -1.5000000e+00]\n",
      " [ 1.4081000e+02 -3.6530000e+01]\n",
      " [ 8.9980000e+01 -2.4690000e+01]\n",
      " [ 5.4300000e+01  1.9800000e+01]\n",
      " [ 5.7630000e+03  0.0000000e+00]\n",
      " [ 5.8650000e+03  0.0000000e+00]\n",
      " [ 1.0000000e+02  9.9900000e+01]\n",
      " [ 1.0000000e+02  1.3000000e+01]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 6.0000000e+00  0.0000000e+00]\n",
      " [ 1.0000000e+02  0.0000000e+00]\n",
      " [ 1.0026765e+02  9.0422450e-02]\n",
      " [ 3.1390000e+01  0.0000000e+00]\n",
      " [ 6.9576000e+02  5.5046000e+02]\n",
      " [ 4.3770000e+02  0.0000000e+00]\n",
      " [ 1.9000000e+01 -3.4000000e-01]\n",
      " [ 1.1000000e+02  0.0000000e+00]\n",
      " [ 4.8610000e+01  0.0000000e+00]\n",
      " [ 6.9999000e+02  4.5582000e+02]\n",
      " [ 4.2800000e+02  0.0000000e+00]\n",
      " [ 2.6710000e+01 -4.0000000e-01]\n",
      " [ 1.0700000e+02  0.0000000e+00]\n",
      " [ 5.7546000e+03 -9.6000000e+00]\n",
      " [ 5.7546000e+03 -9.6000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "col_ranges[30, 0] = 100\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n",
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n",
      "Znormalizowano kolumnę 0\n",
      "Znormalizowano kolumnę 1\n",
      "Znormalizowano kolumnę 2\n",
      "Znormalizowano kolumnę 3\n",
      "Znormalizowano kolumnę 4\n",
      "Znormalizowano kolumnę 5\n",
      "Znormalizowano kolumnę 6\n",
      "Znormalizowano kolumnę 7\n",
      "Znormalizowano kolumnę 8\n",
      "Znormalizowano kolumnę 9\n",
      "Znormalizowano kolumnę 10\n",
      "Znormalizowano kolumnę 11\n",
      "Znormalizowano kolumnę 12\n",
      "Znormalizowano kolumnę 13\n",
      "Znormalizowano kolumnę 14\n",
      "Znormalizowano kolumnę 15\n",
      "Znormalizowano kolumnę 16\n",
      "Znormalizowano kolumnę 17\n",
      "Znormalizowano kolumnę 18\n",
      "Znormalizowano kolumnę 19\n",
      "Znormalizowano kolumnę 20\n",
      "Znormalizowano kolumnę 21\n",
      "Znormalizowano kolumnę 22\n",
      "Znormalizowano kolumnę 23\n",
      "Znormalizowano kolumnę 24\n",
      "Znormalizowano kolumnę 25\n",
      "Znormalizowano kolumnę 26\n",
      "Znormalizowano kolumnę 27\n",
      "Znormalizowano kolumnę 28\n",
      "Znormalizowano kolumnę 29\n",
      "Znormalizowano kolumnę 30\n",
      "Znormalizowano kolumnę 31\n",
      "Znormalizowano kolumnę 32\n",
      "Znormalizowano kolumnę 33\n",
      "Znormalizowano kolumnę 34\n",
      "Znormalizowano kolumnę 35\n",
      "Znormalizowano kolumnę 36\n",
      "Znormalizowano kolumnę 37\n",
      "Znormalizowano kolumnę 38\n",
      "Znormalizowano kolumnę 39\n",
      "Znormalizowano kolumnę 40\n",
      "Znormalizowano kolumnę 41\n",
      "Znormalizowano kolumnę 42\n",
      "Znormalizowano kolumnę 43\n"
     ]
    }
   ],
   "source": [
    "import dill\n",
    "\n",
    "normalise(fine_all, col_ranges)\n",
    "with open('fine_all.pkl', 'wb') as f:\n",
    "    dill.dump(fine_all, f)\n",
    "    \n",
    "normalise(vdips_all, col_ranges)\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)\n",
    "    \n",
    "normalise(surge_all, col_ranges)\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(fine_all, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353078, 44)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, 42, 19]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, 42, 19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "structs_list = [[6, 4]]\n",
    "model = multi_train_NLP(structs_list=structs_list, \n",
    "                        patience=3,\n",
    "                        batch_size=800,\n",
    "                        epochs=10,\n",
    "                        train_data=train_set_trim,\n",
    "                        test_set=test_set_trim,\n",
    "                        set_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07889205479278084\n"
     ]
    }
   ],
   "source": [
    "y = sim(model, test_set_trim, 1)\n",
    "eval_prediction(y, test_set_trim[1][:, -1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trenuje SVR o C=0.5 i epsilon=0.1\n",
      "Wynik biezacego SVRa: 0.14862536846870933\n"
     ]
    }
   ],
   "source": [
    "params = [[0.5, 0.1]]\n",
    "model = multi_train_SVR(C_eps_pairs=params\n",
    "                       ,train_data=train_set_trim\n",
    "                       ,test_data=test_set_trim\n",
    "                       ,set_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14862536846870933\n"
     ]
    }
   ],
   "source": [
    "y = sim(model, test_set_trim, 0)\n",
    "y = y.reshape(-1, 1)\n",
    "eval_prediction(y, test_set_trim[0][:, -1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, 19]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, 19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekwencjonuje zbior uczacy\n",
      "Sekswencjonuje zbior testowy nr: 0\n",
      "Sekswencjonuje zbior testowy nr: 1\n",
      "Sekswencjonuje zbior testowy nr: 2\n",
      "Sekswencjonuje zbior testowy nr: 3\n",
      "Sekswencjonuje zbior testowy nr: 4\n",
      "Sekswencjonuje zbior testowy nr: 5\n",
      "Sekswencjonuje zbior testowy nr: 6\n",
      "Sekswencjonuje zbior testowy nr: 7\n",
      "Sekswencjonuje zbior testowy nr: 8\n",
      "Sekswencjonuje zbior testowy nr: 9\n",
      "Sekswencjonuje zbior testowy nr: 10\n",
      "Sekswencjonuje zbior testowy nr: 11\n",
      "Sekswencjonuje zbior testowy nr: 12\n",
      "Sekswencjonuje zbior testowy nr: 13\n",
      "Sekswencjonuje zbior testowy nr: 14\n",
      "Sekswencjonuje zbior testowy nr: 15\n"
     ]
    }
   ],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_train_LSTM(structure_list=[[4]]\n",
    "                        ,patience=3, epochs=25\n",
    "                        ,batch_size=1000, train_X=Xs\n",
    "                        ,train_Y=Ys, test_X=TXs\n",
    "                        ,test_Y=TYs, set_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014214596038177048\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(TXs[0])\n",
    "eval_prediction(pred, TYs[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353078, 44)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for i in sorted(indices, reverse=True):\n",
    "    train_set = np.delete(train_set, i, axis=1)\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbioru testowego początkowo zawierającego wszystkie atrybuty\n",
    "#zbiór testowy pochodzi z prawidłowej pracy\n",
    "#uzasadnienie doboru w grafice zbior_uczacy.png\n",
    "import numpy as np\n",
    "test_set=[]\n",
    "#02/03/2017\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "#22/03/2017\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "#29/03/2017\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "#13/04/2017\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "#19/04/2017\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "#28/04/2017\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "#12/05/2017\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "#17/05/2017\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "for i in range(0, len(test_set)):\n",
    "    test_set[i] = test_set[i][~np.isnan(test_set[i]).any(axis=1)]\n",
    "    col = len(test_set[i][0])\n",
    "    test_set[i][:, [19, col-1]] = test_set[i][:, [col-1, 19]]\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set[j] = np.delete(test_set[j], i, axis=1)\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(vdips_all, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbior testowy z danych z etykietą Voltage Dips\n",
    "#Dane z 14/03/2017\n",
    "import numpy as np\n",
    "test_set_vdips=[]\n",
    "test_set_vdips.append(vdips_all.values[250000:270000, :])\n",
    "test_set_vdips.append(vdips_all.values[380000:400000, :])\n",
    "test_set_vdips.append(vdips_all.values[500000:520000, :])\n",
    "test_set_vdips.append(vdips_all.values[820000:840000, :])\n",
    "test_set_vdips.append(vdips_all.values[980000:1000000, :])\n",
    "test_set_vdips.append(vdips_all.values[1100000:1120000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_vdips[i] = test_set_vdips[i][~np.isnan(test_set_vdips[i]).any(axis=1)]\n",
    "    col = len(test_set_vdips[i][0])\n",
    "    test_set_vdips[i][:, [19, col-1]] = test_set_vdips[i][:, [col-1, 19]]\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_vdips)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_vdips[j] = np.delete(test_set_vdips[j], i, axis=1)\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbiorów testowych z etykietą Surge\n",
    "import numpy as np\n",
    "test_set_surge=[]\n",
    "#17/01/2017\n",
    "test_set_surge.append(surge_all.values[150000:400000, :])\n",
    "#18/01/2017\n",
    "test_set_surge.append(surge_all.values[570000:800000, :])\n",
    "test_set_surge.append(surge_all.values[900000:1030000, :])\n",
    "test_set_surge.append(surge_all.values[1113000:1150000, :])\n",
    "test_set_surge.append(surge_all.values[1280000:1305000, :])\n",
    "test_set_surge.append(surge_all.values[1350000:1380000, :])\n",
    "test_set_surge.append(surge_all.values[1580000:1605000, :])\n",
    "#20/01/2017\n",
    "test_set_surge.append(surge_all.values[2700000:2900000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_surge[i] = test_set_surge[i][~np.isnan(test_set_surge[i]).any(axis=1)]\n",
    "    col = len(test_set_surge[i][0])\n",
    "    test_set_surge[i][:, [19, col-1]] = test_set_surge[i][:, [col-1, 19]]\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_surge)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_surge[j] = np.delete(test_set_surge[j], i, axis=1)\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 0: wynik: 0.0009269801814762924\n",
      "[1, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 1: wynik: 0.000836919040293115\n",
      "[2, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 2: wynik: 0.0008282951087256454\n",
      "[3, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 3: wynik: 0.0008103128682728062\n",
      "[4, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 4: wynik: 0.0008013165219207473\n",
      "[5, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 5: wynik: 0.0009297672976002782\n",
      "[6, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 6: wynik: 0.0010291615976589757\n",
      "[7, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 7: wynik: 0.0008329087269126301\n",
      "[8, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 8: wynik: 0.0007793782341894118\n",
      "[10, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 10: wynik: 0.0006394581886785025\n",
      "[11, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 11: wynik: 0.0008044054675807169\n",
      "[12, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 12: wynik: 0.0005835577219171741\n",
      "[13, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 13: wynik: 0.000767337885124733\n",
      "[14, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 14: wynik: 0.0007655612284517934\n",
      "[15, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 15: wynik: 0.0011607645033077096\n",
      "[16, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 16: wynik: 0.0008167351130668426\n",
      "[18, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 18: wynik: 0.0007802602007137425\n",
      "[19, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 19: wynik: 0.0009034234466810226\n",
      "[20, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 20: wynik: 0.0009609019733970676\n",
      "[21, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 21: wynik: 0.00091314353515753\n",
      "[22, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 22: wynik: 0.0007909219939352065\n",
      "[23, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 23: wynik: 0.0008485763977345248\n",
      "[24, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 24: wynik: 0.0009093062566512958\n",
      "[26, 9, 17, 25, 27, 28, 29, 30, 31]\n",
      "Test kolumny 26: wynik: 0.0015523689835690753\n",
      "Najlepsza kolumna to: 12 z wynikiem: 0.0005835577219171741\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0])\n",
    "cols_taken = [9, 17, 25, 27, 28, 29, col-2, col-1]\n",
    "try_add_column(cols_taken=cols_taken\n",
    "              ,test_col=15\n",
    "              ,criterion='mse'\n",
    "              ,train_set=train_set\n",
    "              ,test_set=test_set\n",
    "              ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column2(test_set[0], 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 17, 25, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0]) - 1\n",
    "#cols_taken = [17, 29, 5, 27, col-1, col]\n",
    "cols_taken = [9, 17, 25, 27, 28, 29, col-1, col]\n",
    "print(cols_taken)\n",
    "\n",
    "train_set_trim = train_set[:, cols_taken]\n",
    "test_set_trim = []\n",
    "for test in test_set:\n",
    "    test_set_trim.append(test[:, cols_taken])\n",
    "    \n",
    "vdips_trim = []\n",
    "for test in test_set_vdips:\n",
    "    vdips_trim.append(test[:, cols_taken])\n",
    "    \n",
    "surge_trim = []\n",
    "for test in test_set_surge:\n",
    "    surge_trim.append(test[:, cols_taken])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_SVR(C=0.4, epsilon=0.008, train_data=train_set_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_eps = generate_SVR_params(0.4, 1.6, 0.2\n",
    "                           ,0.008, 0.016, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trenuje SVR o C=0.4 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.0001233655026914608\n",
      "Trenuje SVR o C=0.4 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.00013377936489935172\n",
      "Trenuje SVR o C=0.4 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00012771822279869773\n",
      "Trenuje SVR o C=0.4 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.0001196931297672362\n",
      "Trenuje SVR o C=0.4 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00011665295204284496\n",
      "Trenuje SVR o C=0.4 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.00011729761960794782\n",
      "Trenuje SVR o C=0.4 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.0001231864117090603\n",
      "Trenuje SVR o C=0.4 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.00013871238253185852\n",
      "Trenuje SVR o C=0.4 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.00015747899477076436\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.00013670288774761342\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.00013632383479423367\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00013786700651097648\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.0001292992073172009\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00012840216824492885\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.00012334733492320893\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.00013788001413175021\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.00015401128210447564\n",
      "Trenuje SVR o C=0.6000000000000001 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.00017831101018122547\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.0001546501597776593\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.00015007329926994072\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00014477912812980336\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.00014272867409782417\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00013693229534995249\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.0001429438059324376\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.0001607858110536234\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.00017912500141098703\n",
      "Trenuje SVR o C=0.8000000000000002 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.00020255291206745698\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.0001643087148304881\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.00017239167378442207\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00016959211422080942\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.00016405455392573336\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00015428595196027582\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.00016064535879184138\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.0001852012543467656\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.00021697088924276518\n",
      "Trenuje SVR o C=1.0000000000000002 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.0002209058713414632\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.00018855553362400108\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.0001909527985853734\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00020668540061828864\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.00019281603503525083\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00017706459077375207\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.0001957287371830692\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.00021812840199564976\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.0002479533544527586\n",
      "Trenuje SVR o C=1.2000000000000002 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.0002564864535829848\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.00020693699625328921\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.00022555698618080381\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.0002262998901032767\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.00020799492379384432\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00022008233131941315\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.0002328358325692763\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.0002618361406841174\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.00025637941618547074\n",
      "Trenuje SVR o C=1.4000000000000004 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.00029466308250938997\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.008\n",
      "Wynik biezacego SVRa: 0.00025974770091743895\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.009000000000000001\n",
      "Wynik biezacego SVRa: 0.0002574594565218963\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.010000000000000002\n",
      "Wynik biezacego SVRa: 0.00027369705230371875\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.011000000000000003\n",
      "Wynik biezacego SVRa: 0.0002537485693732252\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.012000000000000004\n",
      "Wynik biezacego SVRa: 0.00026150510939524616\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.013000000000000005\n",
      "Wynik biezacego SVRa: 0.0002846833471663593\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.014000000000000005\n",
      "Wynik biezacego SVRa: 0.0003115489511150752\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.015000000000000006\n",
      "Wynik biezacego SVRa: 0.0002942249252356714\n",
      "Trenuje SVR o C=1.6000000000000005 i epsilon=0.016000000000000007\n",
      "Wynik biezacego SVRa: 0.00034351105813013776\n"
     ]
    }
   ],
   "source": [
    "model = multi_train_SVR(C_eps, train_set_trim, test_set_trim\n",
    "                        ,7 , 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.4, cache_size=200, coef0=0.0, degree=3, epsilon=0.012000000000000004,\n",
       "    gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001220653128970991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0001220653128970991"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 0\n",
    "y = sim(model, test_set_trim, col)\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0018313055273731834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0018313055273731834"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:106: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  app.exec_()\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "y = sim(model, surge_trim, num)\n",
    "eval_prediction(y, surge_trim[num][:, -1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32784320492623953\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.32784320492623953"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 5\n",
    "y = sim(model, vdips_trim, num)\n",
    "eval_prediction(y, vdips_trim[num][:, -1], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 25 27 28 29 30 31]\n",
      "Test kolumny 9: wynik: 0.000909040814146435\n",
      "[ 9 25 27 28 29 30 31]\n",
      "Test kolumny 17: wynik: 0.0012970143203150837\n",
      "[ 9 17 27 28 29 30 31]\n",
      "Test kolumny 25: wynik: 0.000959292502203773\n",
      "[ 9 17 25 28 29 30 31]\n",
      "Test kolumny 27: wynik: 0.08519323549842511\n",
      "[ 9 17 25 27 29 30 31]\n",
      "Test kolumny 28: wynik: 0.0009548138964801653\n",
      "[ 9 17 25 27 28 30 31]\n",
      "Test kolumny 29: wynik: 0.0015774273253795197\n",
      "[ 9 17 25 27 28 29 31]\n",
      "Test kolumny 30: wynik: 0.003423166449700847\n",
      "[ 9 17 25 27 28 29 30]\n",
      "Test kolumny 31: wynik: 0.003126141302610814\n",
      "Najlepsza kolumna to: 0 z wynikiem: 0.000909040814146435\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0])\n",
    "base_cols = [9, 17, 25, 27, 28, 29, col-2, col-1]\n",
    "try_drop_column(start_cols=base_cols\n",
    "               ,test_col=11\n",
    "               ,criterion='mse'\n",
    "               ,train_set=train_set\n",
    "               ,test_set=test_set\n",
    "               ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_9_17_27_29.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_list = [[5,6]]\n",
    "dropouts=[[0,0]]\n",
    "model = multi_train_NLP(structs_list=struct_list, patience=3\n",
    "                       ,batch_size=1000, epochs=100\n",
    "                       ,train_data=train_set_trim\n",
    "                       ,test_data=test_set_trim\n",
    "                       ,set_num = 9, criterion='mse'\n",
    "                       ,single_times = 5\n",
    "                       ,dropout_2list=dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.21221133,  0.11389147, -0.32329348, -0.10989959, -0.03034207],\n",
       "        [-0.27682832, -0.09624609,  0.323266  ,  0.6242851 ,  0.03375495],\n",
       "        [-0.07437903,  0.12139084, -0.44150937, -0.299035  , -0.41793123],\n",
       "        [ 0.29596663,  0.06523001,  0.23245783,  0.3492079 ,  0.10944318],\n",
       "        [ 0.4045692 , -0.22286865,  0.10578701,  0.08379979, -0.19417873],\n",
       "        [-0.11277731, -0.08811434, -0.43158787, -0.42363122, -0.34777752],\n",
       "        [-0.50484276,  0.5585321 ,  0.5177559 ,  0.23991705, -0.08423644]],\n",
       "       dtype=float32),\n",
       " array([ 0.03788985, -0.08355072, -0.1044693 , -0.23909606,  0.02150264],\n",
       "       dtype=float32),\n",
       " array([[-0.13316812,  0.25908887, -0.45555565, -0.30624083,  0.03597004,\n",
       "         -0.2190762 , -0.291932  , -0.09540743, -0.02515903,  0.04952152,\n",
       "         -0.1664677 , -0.13343777,  0.11712614,  0.29215434,  0.04354215,\n",
       "         -0.06086809,  0.0828923 , -0.19191323,  0.03250973,  0.4057726 ,\n",
       "          0.01707353, -0.02227308,  0.26567507, -0.09380013, -0.13412783,\n",
       "          0.20832545,  0.15948874,  0.13126394, -0.31305954,  0.12943567,\n",
       "          0.05792366,  0.0710634 ],\n",
       "        [-0.03913514, -0.25805935,  0.11355254, -0.40554628, -0.19975734,\n",
       "         -0.45637617, -0.30890974, -0.27250984, -0.02386328,  0.0255487 ,\n",
       "          0.0397146 , -0.1316707 , -0.22566672, -0.2853629 , -0.46788648,\n",
       "          0.10318078,  0.11349647, -0.1502303 , -0.2707815 , -0.11889847,\n",
       "          0.1028447 ,  0.0768091 , -0.16635409, -0.29351068, -0.22871478,\n",
       "         -0.38488045,  0.03075277,  0.48629904,  0.22842877,  0.15371197,\n",
       "         -0.22478853,  0.14591007],\n",
       "        [ 0.3498974 ,  0.2715188 ,  0.10753783,  0.17208736, -0.07323813,\n",
       "          0.2347168 , -0.1244213 , -0.06969446,  0.30497375,  0.32957292,\n",
       "          0.30880344,  0.0986895 , -0.09695894, -0.39062715, -0.1370654 ,\n",
       "          0.22628924,  0.04216775,  0.4014436 ,  0.2006485 ,  0.03087518,\n",
       "         -0.16775177, -0.1469785 , -0.15843152,  0.23633593,  0.36846527,\n",
       "         -0.24393691, -0.15415904,  0.05022241, -0.16993144, -0.03937503,\n",
       "         -0.1945342 , -0.02727197],\n",
       "        [-0.2011698 , -0.14958635,  0.00139539, -0.12669522,  0.30257457,\n",
       "         -0.14501972,  0.0718189 ,  0.36971474, -0.10295757, -0.10663702,\n",
       "         -0.11552233,  0.43002695,  0.08663967,  0.21082558,  0.23275517,\n",
       "         -0.45282152,  0.16688313, -0.1260256 ,  0.04805401, -0.14363885,\n",
       "         -0.21837892,  0.26103932,  0.16191845, -0.35121498, -0.00534964,\n",
       "          0.32418787,  0.01476916,  0.20082009,  0.26956892,  0.18068285,\n",
       "          0.08679671, -0.16854462],\n",
       "        [-0.15488356, -0.18535061, -0.03369154,  0.06598993, -0.21225044,\n",
       "          0.00361288,  0.19102836, -0.2498974 , -0.24060524, -0.29403272,\n",
       "         -0.20871978, -0.2484307 ,  0.02390283,  0.22273225, -0.0371069 ,\n",
       "          0.21126848, -0.25601658, -0.28299275, -0.2529191 ,  0.03787431,\n",
       "          0.41984734, -0.10317886, -0.03349463,  0.17043184, -0.3573988 ,\n",
       "         -0.05689606,  0.12181716, -0.40953234, -0.05220514, -0.20332116,\n",
       "          0.16234508,  0.17452532]], dtype=float32),\n",
       " array([-0.03401129,  0.00404693, -0.04204702, -0.00510379, -0.00159498,\n",
       "        -0.0036806 ,  0.01092113, -0.01004758, -0.02750698, -0.03035346,\n",
       "        -0.04068659, -0.00013074,  0.022186  ,  0.06098836,  0.0250994 ,\n",
       "        -0.00117004, -0.01734415, -0.04361895, -0.01384571,  0.03358065,\n",
       "         0.0333813 , -0.00520172,  0.02811259,  0.00917874, -0.03882389,\n",
       "         0.03490562,  0.02381144, -0.04914679, -0.02635315, -0.01012186,\n",
       "         0.03076977,  0.01289172], dtype=float32),\n",
       " array([[ 0.25036398],\n",
       "        [-0.2503958 ],\n",
       "        [ 0.10263896],\n",
       "        [-0.2751617 ],\n",
       "        [ 0.15193248],\n",
       "        [-0.24280718],\n",
       "        [-0.2514184 ],\n",
       "        [ 0.10474627],\n",
       "        [ 0.17499192],\n",
       "        [ 0.15057799],\n",
       "        [ 0.12628798],\n",
       "        [ 0.06716399],\n",
       "        [-0.22716731],\n",
       "        [-0.08487474],\n",
       "        [-0.3329788 ],\n",
       "        [-0.10792292],\n",
       "        [ 0.10713458],\n",
       "        [ 0.12938288],\n",
       "        [ 0.25371787],\n",
       "        [-0.09739812],\n",
       "        [-0.07205246],\n",
       "        [ 0.1346136 ],\n",
       "        [-0.28103215],\n",
       "        [-0.08950949],\n",
       "        [ 0.10557541],\n",
       "        [-0.3289258 ],\n",
       "        [-0.18533467],\n",
       "        [ 0.05764258],\n",
       "        [ 0.07932875],\n",
       "        [ 0.131911  ],\n",
       "        [-0.1377484 ],\n",
       "        [-0.15827663]], dtype=float32),\n",
       " array([0.2525072], dtype=float32)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0010903673033827626"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 0\n",
    "y = sim(model, test_set_trim, col)\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0053225548991264054"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:106: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  app.exec_()\n"
     ]
    }
   ],
   "source": [
    "num = 2\n",
    "y = sim(model, surge_trim, num)\n",
    "eval_prediction(y, surge_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3517088497375891"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num = 5\n",
    "y = sim(model, vdips_trim, num)\n",
    "eval_prediction(y, vdips_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_MLP_5_2_2020-4-3.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekwencjonuje zbior uczacy\n",
      "Sekswencjonuje zbior testowy nr: 0\n",
      "Sekswencjonuje zbior testowy nr: 1\n",
      "Sekswencjonuje zbior testowy nr: 2\n",
      "Sekswencjonuje zbior testowy nr: 3\n",
      "Sekswencjonuje zbior testowy nr: 4\n",
      "Sekswencjonuje zbior testowy nr: 5\n",
      "Sekswencjonuje zbior testowy nr: 6\n",
      "Sekswencjonuje zbior testowy nr: 7\n",
      "Sekswencjonuje zbior testowy nr: 8\n",
      "Sekswencjonuje zbior testowy nr: 9\n",
      "Sekswencjonuje zbior testowy nr: 10\n",
      "Sekswencjonuje zbior testowy nr: 11\n",
      "Sekswencjonuje zbior testowy nr: 12\n",
      "Sekswencjonuje zbior testowy nr: 13\n",
      "Sekswencjonuje zbior testowy nr: 14\n",
      "Sekswencjonuje zbior testowy nr: 15\n"
     ]
    }
   ],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_timeseries(Xs, Ys, TXs, TYs, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = load_timeseries(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229468 samples, validate on 123560 samples\n",
      "Epoch 1/25\n",
      "229468/229468 [==============================] - 21s 90us/step - loss: 0.0858 - val_loss: 0.0145\n",
      "Epoch 2/25\n",
      "229468/229468 [==============================] - 19s 82us/step - loss: 0.0057 - val_loss: 0.0040\n",
      "Epoch 3/25\n",
      "229468/229468 [==============================] - 18s 78us/step - loss: 9.7513e-04 - val_loss: 0.0027\n",
      "Epoch 4/25\n",
      "229468/229468 [==============================] - 17s 73us/step - loss: 7.5461e-04 - val_loss: 0.0022\n",
      "Epoch 5/25\n",
      "229468/229468 [==============================] - 17s 73us/step - loss: 6.1426e-04 - val_loss: 0.0020\n",
      "Epoch 6/25\n",
      "229468/229468 [==============================] - 17s 74us/step - loss: 4.9005e-04 - val_loss: 0.0018\n",
      "Epoch 7/25\n",
      "229468/229468 [==============================] - 17s 73us/step - loss: 3.8394e-04 - val_loss: 0.0016\n",
      "Epoch 8/25\n",
      "229468/229468 [==============================] - 17s 72us/step - loss: 2.9675e-04 - val_loss: 0.0014\n",
      "Epoch 9/25\n",
      "229468/229468 [==============================] - 17s 73us/step - loss: 2.5010e-04 - val_loss: 0.0014\n",
      "Epoch 10/25\n",
      "229468/229468 [==============================] - 17s 74us/step - loss: 2.8081e-04 - val_loss: 0.0015-\n",
      "Epoch 11/25\n",
      "229468/229468 [==============================] - 17s 74us/step - loss: 4.2828e-04 - val_loss: 0.0022\n",
      "Epoch 12/25\n",
      "229468/229468 [==============================] - 17s 75us/step - loss: 7.3497e-04 - val_loss: 0.0036\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "structure = [8,4]\n",
    "model = train_LSTM(structure=structure, patience=3\n",
    "              ,epochs=25, batch_size=1000\n",
    "              ,train_X=Xs, train_Y=Ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00415108535984568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00415108535984568"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(TXs[0])\n",
    "eval_prediction(pred, TYs[0], 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
