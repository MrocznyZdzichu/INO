{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib Qt\n",
    "manager = plt.get_current_fig_manager()\n",
    "manager.window.showMaximized()\n",
    "\n",
    "#init cell\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data.values[:, col_num], linewidth=0.5)\n",
    "\t\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column2(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data[:, col_num], linewidth=0.5)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "def sim(model, test_set, set_num):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0:\n",
    "            sample = np.copy(test_set[set_num][0:1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "\n",
    "def sim_with_refreshes(model, test_set\n",
    "                       ,set_num, refr_period):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0 or i%refr_period == 0:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "\n",
    "def eval_prediction(y_pred, y_target, do_plot):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    if do_plot == 1:\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(y_pred[:, 0], label='Predykcja', lw=1)\n",
    "        plt.plot(y_target, label='Wartośći rzeczywiste', lw=1)\n",
    "        plt.legend()\n",
    "        plt.ylabel('Znormalizowana wartośc sygnału')\n",
    "        plt.grid(b=True, which='both', axis='y')\n",
    "\n",
    "        residuals = np.copy(y_pred[:, 0])\n",
    "        for i in range(0, len(residuals)):\n",
    "            residuals[i] = residuals[i] - y_target[i]\n",
    "        residuals = residuals.flatten()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.fill_between(range(0, len(residuals))\n",
    "                             ,residuals, label='Residuals', lw=1)\n",
    "        plt.grid(b=True, which='both', axis='y')\n",
    "        plt.xlabel('Numer obserwacji')\n",
    "        plt.ylabel('Błąd predykcji')\n",
    "        plt.show()\n",
    "    \n",
    "    score = mse(y_pred[:, 0], y_target)\n",
    "    print2(score)\n",
    "    return score\n",
    "    \n",
    "def multi_train_NLP(structs_list, patience, batch_size, \n",
    "                    epochs, train_data, test_data, set_num\n",
    "                   ,criterion, single_times, dropout_2list, log_name=None):\n",
    "\n",
    "    import time\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score=1\n",
    "    best_structure = structs_list[0]\n",
    "    \n",
    "    struct_num = 0\n",
    "    for structure in structs_list:\n",
    "        print2(f'Training network with structure: {structure}', log_name)\n",
    "        [curr_model, curr_score] = train_MLP_Ntimes(N  =single_times\n",
    "                                     ,structure   =structure\n",
    "                                     ,patience    =patience\n",
    "                                     ,batch_size  =batch_size\n",
    "                                     ,epochs      =epochs\n",
    "                                     ,set_num     =set_num\n",
    "                                     ,train_data  =train_data\n",
    "                                     ,test_data   =test_data\n",
    "                                     ,debug_log   ='no'\n",
    "                                     ,criterion   =criterion\n",
    "                                     ,dropout_list=dropout_2list[struct_num]\n",
    "                                     ,log_name    =log_name)\n",
    "            \n",
    "        print2(f'Current score: {curr_score}',log_name)\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_model     = curr_model\n",
    "            best_score     = curr_score\n",
    "            best_structure = structure\n",
    "            \n",
    "        struct_num = struct_num + 1\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    print2(f'Best score: {best_score} with structure: {best_structure}', log_name)\n",
    "    return(best_model)\n",
    "\n",
    "def train_MLP_Ntimes(N, structure, patience, batch_size, epochs\n",
    "                    ,train_data, test_data, set_num\n",
    "                    ,debug_log, criterion, dropout_list, log_name=None):\n",
    "    import time\n",
    "    \n",
    "    best_score = 1\n",
    "    nets_trained = 0\n",
    "    \n",
    "    while nets_trained < N:\n",
    "        print2(f\"Train no. {nets_trained+1}/{N}\", log_name)\n",
    "        \n",
    "        model = train_NLP(structure=structure\n",
    "                         ,patience=patience\n",
    "                         ,batch_size=batch_size\n",
    "                         ,epochs=epochs\n",
    "                         ,train_data=train_data\n",
    "                         ,dropout_list=dropout_list\n",
    "                         ,log_name=log_name)\n",
    "        \n",
    "        t_start = time.time()\n",
    "        y = sim(model, test_data, set_num)\n",
    "        t_stop = time.time()\n",
    "        \n",
    "        print2(f\"Simulation time: {t_stop-t_start} s\",log_name)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = eval_prediction(y, test_set[set_num][:, -1], 0)\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_data[set_num][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "            \n",
    "        print2(f\"Score: {score}\",log_name)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "        del model\n",
    "        nets_trained = nets_trained + 1\n",
    "    \n",
    "    print2(f\"Best score: {best_score}\",log_name)\n",
    "    import winsound\n",
    "    filename = 'sound.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "\n",
    "    return best_model,best_score\n",
    "\n",
    "def train_NLP(structure, patience,batch_size\n",
    "              ,epochs, train_data, dropout_list\n",
    "             ,log_name=None):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras import regularizers\n",
    "    from keras.optimizers import SGD\n",
    "    import time\n",
    "    \n",
    "    inputs_count = len(train_data[0]) - 1\n",
    "    layer_num = 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer_size in structure:\n",
    "        if layer_num == 1:\n",
    "            model.add(Dense(layer_size, input_dim=inputs_count, \n",
    "                            activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "            \n",
    "        else:\n",
    "            model.add(Dense(layer_size, activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "            \n",
    "        layer_num = layer_num + 1\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                   verbose=1, patience=patience)\n",
    "\n",
    "    t_start = time.time()\n",
    "    hist = model.fit(train_data[:, :-1],\n",
    "              train_data[:, -1], batch_size=batch_size,\n",
    "              epochs=epochs, validation_split = 0.35,\n",
    "              callbacks=[es], workers=8, use_multiprocessing=1)\n",
    "    t_stop = time.time()\n",
    "    \n",
    "\n",
    "    print2(f\"Model's training time: {t_stop-t_start} s\", log_name)\n",
    "    print2(f\"Validation error: {hist.history['val_loss'][1]}\", log_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_SVR(C, epsilon, train_data):\n",
    "    import time\n",
    "    from sklearn.svm import SVR as svr\n",
    "    \n",
    "    model = svr(kernel='rbf', gamma='scale', C=C, epsilon=epsilon)\n",
    "    \n",
    "    start = time.time()\n",
    "    model.fit(train_data[:, :-1], train_data[:, -1])\n",
    "    stop = time.time()\n",
    "    \n",
    "    print(f'Czas treningu modelu: {stop-start}s')\n",
    "    return model\n",
    "\n",
    "def multi_train_SVR(C_eps_pairs\n",
    "                   ,train_data\n",
    "                   ,test_data\n",
    "                   ,set_num\n",
    "                   ,criterion):\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "   # sys.stdout = open('log.txt', 'w')\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse \n",
    "    best_score = 1\n",
    "\n",
    "    for pair in C_eps_pairs:\n",
    "        print(f'Trenuje SVR o C={pair[0]} i epsilon={pair[1]}')\n",
    "        curr_model = train_SVR(C=pair[0]\n",
    "                               ,epsilon=pair[1]\n",
    "                               ,train_data=train_data)\n",
    "    \n",
    "        y = sim(curr_model, test_data, set_num)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            curr_score = mse(y[:, 0], test_data[set_num][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for i in range(0, len(residuals)):\n",
    "                residuals[i] = residuals[i] - test_data[set_num][i, -1]\n",
    "            curr_score = np.max(residuals)\n",
    "        \n",
    "    \n",
    "        print(f'Wynik biezacego SVRa: {curr_score}')\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "\n",
    "    sys.stdout = bck_stdout  \n",
    "    return best_model\n",
    "           \n",
    "\n",
    "def train_tree(alpha ,train_data, log_name):\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    import time\n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    \n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=log_name,level=logging.INFO)\n",
    "    \n",
    "    model = DecisionTreeRegressor(\n",
    "        ccp_alpha=alpha\n",
    "        ,max_features=1\n",
    "    )\n",
    "    \n",
    "    t_start = time.time()\n",
    "    model.fit(train_data[:,:-1], train_data[:,-1])\n",
    "    t_stop = time.time()\n",
    "    \n",
    "    logging.info(f'Czas treningu modelu: {t_stop-t_start}s')\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_tree_nTimes(alpha, n,train_data\n",
    "                      ,test_data, test_num, log_name):\n",
    "    best_score = 1\n",
    "    for i in range(0, n):\n",
    "        model = train_tree(alpha\n",
    "                          ,train_data\n",
    "                          ,log_name)\n",
    "        \n",
    "        if n == 1:\n",
    "            return model\n",
    "        \n",
    "        y = sim(model, test_data, test_num)\n",
    "        score = eval_prediction(y, test_data[test_num][:, -1], 0)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "    return best_model,best_score\n",
    "\n",
    "\n",
    "def train_multi_trees(alpha ,n, train_data\n",
    "                      ,test_data, test_num, log_mode):\n",
    "    \n",
    "    import logging\n",
    "    from datetime import datetime\n",
    "    log_name = datetime.now().strftime('trees\\\\trees_GS_%Y_%m_%d_%H_%M.txt')\n",
    "    \n",
    "    for handler in logging.root.handlers[:]:\n",
    "        logging.root.removeHandler(handler)\n",
    "    logging.basicConfig(filename=log_name,level=logging.INFO)\n",
    "    \n",
    "    best_score = 1\n",
    "    for cp in alpha:\n",
    "        logging.info(f'Trenuje drzewo o CP: {cp}')\n",
    "        \n",
    "        [model,score] = train_tree_nTimes(cp ,n,train_data\n",
    "                                  ,test_data, test_num, log_name)\n",
    "        \n",
    "        logging.info(f'Wynik biezacego drzewa: {score}')\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_cp = cp\n",
    "            \n",
    "    logging.info(f'Wynik najlepszego drzewa: {best_score}. CP = {best_cp}')\n",
    "        \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def train_adaBoost(trees, lr, depth\n",
    "                ,train_data, log_name=None):\n",
    "    from sklearn.ensemble import AdaBoostRegressor\n",
    "    from sklearn.tree import DecisionTreeRegressor\n",
    "    import time\n",
    "    estimator = DecisionTreeRegressor(\n",
    "        max_depth=depth\n",
    "        ,max_features=1\n",
    "    )\n",
    "    \n",
    "    model = AdaBoostRegressor(\n",
    "        base_estimator=estimator\n",
    "        ,n_estimators=trees\n",
    "        ,learning_rate=lr\n",
    "        ,loss='linear'\n",
    "    )\n",
    "    \n",
    "    t_start = time.time()\n",
    "    model.fit(train_data[:,:-1], train_data[:,-1])\n",
    "    t_stop = time.time()\n",
    "    \n",
    "    print2(f'Czas trwania treningu: {t_stop - t_start}',log_name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_adaBoost_nTimes(trees, lr, depth, n\n",
    "                       ,train_data, test_data\n",
    "                       ,test_num, log_name=None):\n",
    "    import time\n",
    "    \n",
    "    best_score = 1\n",
    "    for i in range(0, n):\n",
    "        model = train_adaBoost(trees, lr, depth\n",
    "                            ,train_data, log_name)\n",
    "        \n",
    "        if n == 1:\n",
    "            return model, best_score\n",
    "        \n",
    "        t_start = time.time()\n",
    "        y = sim(model, test_data, test_num)\n",
    "        t_stop = time.time()\n",
    "        \n",
    "        print2(f'Czas trwania obliczania predykcji modelu: {t_stop - t_start}', log_name)\n",
    "        score = eval_prediction(y, test_data[test_num][:, -1], 0)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "    return best_model, best_score\n",
    "\n",
    "\n",
    "def train_multi_adaBoost(trees, lr, depth\n",
    "                      ,n, train_data, test_data\n",
    "                      ,test_num, log_name):\n",
    "    params=[]\n",
    "    params.append(trees)\n",
    "    params.append(lr)\n",
    "    params.append(depth)\n",
    "    \n",
    "    best_score = 1\n",
    "    best_par = []\n",
    "    import itertools\n",
    "    for par_el in itertools.product(*params):\n",
    "        print2(f'AdaBoost training parameters:', log_name)\n",
    "        print2(f'Number of estimators: {par_el[0]}', log_name)\n",
    "        print2(f'Learning rate: {par_el[1]}', log_name)\n",
    "        print2(f'Max depth: {par_el[2]}', log_name)\n",
    "        \n",
    "        [model, score] = train_adaBoost_nTimes(par_el[0], par_el[1], par_el[2]\n",
    "                                   ,n,train_data, test_data, test_num, log_name)\n",
    "        \n",
    "        print2(f'Wynik biezacego modelu: {score}',log_name)\n",
    "        \n",
    "        if score <= best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "            best_par = par_el\n",
    "    print2(f'Najlepszy model: liczb_drzew: {best_par[0]}, stala uczenia: {best_par[1]}, liczba poziomow: {best_par[2]}, wynik: {best_score}',log_name)\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    #based on tutorial:\n",
    "    #https://towardsdatascience.com/\n",
    "    #time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python-6ceee9c6c651\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "    \n",
    "def prepare_timeseries(train_set, test_set_list, prepare):\n",
    "    pd_X = pd.DataFrame(data=train_set[:, :-1])\n",
    "    pd_Y = pd.DataFrame(data=train_set[:, -1])\n",
    "\n",
    "    pd_TX = []\n",
    "    pd_TY = []\n",
    "\n",
    "    for test in test_set_list:\n",
    "        pd_TX.append(pd.DataFrame(test[:, :-1]))\n",
    "        pd_TY.append(pd.DataFrame(test[:, -1]))\n",
    "\n",
    "    print(f'Sekwencjonuje zbior uczacy')\n",
    "    Xs, Ys = create_dataset(pd_X, pd_Y, prepare)\n",
    "\n",
    "    TXs = []\n",
    "    TYs = []\n",
    "    \n",
    "    for i in range(0, len(pd_TX)):\n",
    "        print(f'Sekswencjonuje zbior testowy nr: {i}')\n",
    "        txs, tys = create_dataset(pd_TX[i], pd_TY[i], prepare)\n",
    "        TXs.append(txs)\n",
    "        TYs.append(tys)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs\n",
    "\n",
    "\n",
    "def prepare_surge(test_set_list, prepare):\n",
    "    pd_TX = []\n",
    "    pd_TY = []\n",
    "\n",
    "    for test in test_set_list:\n",
    "        pd_TX.append(pd.DataFrame(test[:, :-1]))\n",
    "        pd_TY.append(pd.DataFrame(test[:, -1]))\n",
    "\n",
    "    TXs = []\n",
    "    TYs = []\n",
    "    \n",
    "    for i in range(0, len(pd_TX)):\n",
    "        print(f'Sekswencjonuje zbior testowy nr: {i}')\n",
    "        txs, tys = create_dataset(pd_TX[i], pd_TY[i], prepare)\n",
    "        TXs.append(txs)\n",
    "        TYs.append(tys)\n",
    "        \n",
    "    return TXs, TYs\n",
    "\n",
    "\n",
    "def train_LSTM_ntimes(structure\n",
    "                      ,patience\n",
    "                      ,epochs\n",
    "                      ,batch_size\n",
    "                      ,dropouts\n",
    "                      ,rec_dropouts\n",
    "                      ,N \n",
    "                      ,train_X\n",
    "                      ,train_Y\n",
    "                      ,set_num\n",
    "                      ,test_X\n",
    "                      ,test_Y\n",
    "                      ,criterion\n",
    "                      ,log_name=None):\n",
    "    best_score = 1\n",
    "    nets_trained = 0\n",
    "    \n",
    "    while nets_trained < N:\n",
    "        model = train_LSTM(structure=structure, patience=3\n",
    "                          ,epochs=epochs, batch_size=batch_size\n",
    "                          ,dropouts=dropouts, rec_dropouts=rec_dropouts\n",
    "                          ,train_X=Xs, train_Y=Ys, log_name=log_name)\n",
    "        \n",
    "        y = model.predict(test_X[set_num])\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = eval_prediction(y, test_Y[set_num][:, -1], 0)\n",
    "            \n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_data[set_num][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "            \n",
    "        print2(f\"Wynik sieci nr {nets_trained+1}: {score}\", log_name)\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "        del model\n",
    "        nets_trained = nets_trained + 1\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'sound.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "\n",
    "    return best_model, best_score\n",
    "\n",
    "\n",
    "def train_LSTM(structure, patience\n",
    "              ,epochs, batch_size\n",
    "              ,dropouts, rec_dropouts\n",
    "              ,train_X, train_Y, log_name=None):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "    from keras.layers import Activation\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Masking\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    import time\n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(0, len(structure)):\n",
    "        if i == 0:\n",
    "            model.add(Masking(mask_value=-10\n",
    "                             ,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "            \n",
    "            model.add(LSTM(units=structure[i]\n",
    "                          ,activation='tanh'\n",
    "                          ,recurrent_activation='sigmoid'\n",
    "                          ,dropout=dropouts[i]\n",
    "                          ,recurrent_dropout=rec_dropouts[i]\n",
    "                          ,return_sequences=False))\n",
    "        else:\n",
    "            model.add(Dense(units=structure[i]\n",
    "                           ,activation='tanh'))\n",
    "            model.add(Dropout(dropouts[i]))\n",
    "                      \n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error'\n",
    "                  ,optimizer='adam')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min'\n",
    "                      ,verbose=1, patience=patience)\n",
    "    \n",
    "    t_start = time.time()\n",
    "    model.fit(train_X, train_Y, epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.35,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              callbacks=[es])\n",
    "    t_stop = time.time()\n",
    "           \n",
    "    print2(f'Training time: {t_stop-t_start} s', log_name)\n",
    "    return model\n",
    "\n",
    "def multi_train_LSTM(structure_list\n",
    "                     ,patience\n",
    "                     ,dropouts\n",
    "                     ,rec_dropouts\n",
    "                     ,epochs\n",
    "                     ,batch_size\n",
    "                     ,train_single\n",
    "                     ,train_X\n",
    "                     ,train_Y\n",
    "                     ,test_X\n",
    "                     ,test_Y\n",
    "                     ,set_num\n",
    "                     ,criterion\n",
    "                     ,log_name=None):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    \n",
    "    best_score = 1\n",
    "    best_structure = structure_list[1]\n",
    "           \n",
    "    for i in range(0, len(structure_list)):\n",
    "        print2(f'Trenuje siec o strukturze: {structure_list[i]}', log_name)\n",
    "        [curr_model,curr_score] = train_LSTM_ntimes(\n",
    "            structure=structure_list[i], patience=patience\n",
    "            ,epochs=epochs, batch_size=batch_size\n",
    "            ,rec_dropouts=rec_dropouts[i]\n",
    "            ,dropouts=dropouts[i], N=train_single\n",
    "            ,train_X=train_X, train_Y=train_Y, set_num=set_num\n",
    "            ,test_X=test_X, test_Y=test_Y, criterion='mse', log_name=log_name)\n",
    "        \n",
    "        print2(f'Wynik biezacej sieci: {curr_score}', log_name)\n",
    "        \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "            best_structure = structure_list[i]\n",
    "            \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    print2(f'Najlepszy model - struktura {best_structure}, wynik: {best_score}', log_name)\n",
    "    return best_model\n",
    "\n",
    "def generate_SVR_params(C_min, C_max, C_step\n",
    "                       ,eps_min, eps_max, eps_step):\n",
    "    C_max = C_max + C_step\n",
    "    C_vals = np.arange(C_min, C_max, C_step)\n",
    "    \n",
    "    eps_max = eps_max + eps_step\n",
    "    eps_vals = np.arange(eps_min, eps_max, eps_step)\n",
    "    \n",
    "    params_list = []\n",
    "    for C in C_vals:\n",
    "        for eps in eps_vals:\n",
    "            params_list.append([C, eps])\n",
    "    \n",
    "    return params_list\n",
    "\n",
    "# def try_add_column(cols_taken, test_col, criterion, train_set\n",
    "#                   ,test_set, model_type):\n",
    "#     from sklearn.metrics import mean_squared_error as mse\n",
    "#     col = len(train_set[0])\n",
    "#     best_score = 16\n",
    "#     best_score2 = 1\n",
    "#     best_col = 0\n",
    "\n",
    "#     for i in range(0, col):\n",
    "#         if i not in cols_taken:\n",
    "#             cols_test = [i]\n",
    "#             for _col in cols_taken:\n",
    "#                 cols_test.append(_col)\n",
    "\n",
    "#             train_set_trim = train_set[:, cols_test]\n",
    "#             test_set_trim = []\n",
    "#             for test in test_set:\n",
    "#                 test_set_trim.append(test[:, cols_test])\n",
    "            \n",
    "#             if model_type == 'SVR':\n",
    "#                 model = train_SVR(C=0.4, epsilon=0.02\n",
    "#                                   ,train_data=train_set_trim)\n",
    "            \n",
    "#             score_sum = 0\n",
    "#             for j in range(0, len(test_set)):\n",
    "#                 pred = sim(model, test_set_trim, j)\n",
    "#                 score = mse(pred[:, 0], test_set_trim[j][:, -1])\n",
    "#                 #print(f'MSE na scenariuszu: {j}: {score}')\n",
    "#                 score_sum += score\n",
    "               \n",
    "#                 if j == test_col:\n",
    "#                     residuals = np.copy(pred[:, 0])\n",
    "#                     for j in range(0, len(residuals)):\n",
    "#                         residuals[j] = residuals[j] - \\\n",
    "#                         test_set_trim[test_col][j, -1]\n",
    "#                     score2 = np.max(np.abs(residuals))\n",
    "#                     #print(f'Najwiekszy blad na scenariuszu {test_col}: {score2}')\n",
    "\n",
    "#             if score_sum < best_score:\n",
    "#                 best_model = model\n",
    "#                 best_score = score_sum\n",
    "#                 best_col = i\n",
    "#             if score2 < best_score2:\n",
    "#                 best_score2 = score2\n",
    "\n",
    "#             print(cols_test)\n",
    "#             print(f'Test kolumny {i}: mse: {score_sum}, max_error: {score2}')\n",
    "\n",
    "#     print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "\n",
    "def try_add_column(cols_taken, test_col, criterion, train_set\n",
    "                  ,test_set, model_type):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    col = len(train_set[0])\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, col):\n",
    "        if i not in cols_taken:\n",
    "            cols_test = [i]\n",
    "            for _col in cols_taken:\n",
    "                cols_test.append(_col)\n",
    "\n",
    "            train_set_trim = train_set[:, cols_test]\n",
    "            test_set_trim = []\n",
    "            for test in test_set:\n",
    "                test_set_trim.append(test[:, cols_test])\n",
    "            \n",
    "            if model_type == 'SVR':\n",
    "                model = train_SVR(C=0.4, epsilon=0.020\n",
    "                                  ,train_data=train_set_trim)\n",
    "            \n",
    "            pred = sim(model, test_set_trim, test_col)\n",
    "            \n",
    "            score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "            residuals = np.copy(pred[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_set_trim[test_col][j, -1]\n",
    "            score2 = np.max(np.abs(residuals))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_model = model\n",
    "                best_score = score\n",
    "                best_col = i\n",
    "\n",
    "            print(cols_test)\n",
    "            print(f'Test kolumny {i}: mse: {score}, max_error: {score2}')\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "           \n",
    "# def try_drop_column(start_cols, test_col, criterion\n",
    "#                    ,train_set ,test_set, model_type):\n",
    "    \n",
    "#     from sklearn.metrics import mean_squared_error as mse\n",
    "#     best_score = 16\n",
    "#     best_score2 = 1\n",
    "#     best_col = 0\n",
    "\n",
    "#     for i in range(0, len(base_cols)-2):\n",
    "#         cols_test = np.copy(base_cols)\n",
    "#         deleted = base_cols[i]\n",
    "#         cols_test = np.delete(base_cols, i)\n",
    "\n",
    "#         train_set_trim = train_set[:, cols_test]\n",
    "#         test_set_trim = []\n",
    "#         for test in test_set:\n",
    "#             test_set_trim.append(test[:, cols_test])\n",
    "        \n",
    "#         if model_type == 'SVR':\n",
    "#             model = train_SVR(C=0.4, epsilon=0.020\n",
    "#                               ,train_data=train_set_trim)\n",
    "        \n",
    "#             score_sum = 0\n",
    "#             for j in range(0, len(test_set)):\n",
    "#                 pred = sim(model, test_set_trim, j)\n",
    "#                 score = mse(pred[:, 0], test_set_trim[j][:, -1])\n",
    "#                 #print(f'MSE na scenariuszu: {j}: {score}')\n",
    "#                 score_sum += score\n",
    "               \n",
    "#                 if j == test_col:\n",
    "#                     residuals = np.copy(pred[:, 0])\n",
    "#                     for j in range(0, len(residuals)):\n",
    "#                         residuals[j] = residuals[j] - \\\n",
    "#                         test_set_trim[test_col][j, -1]\n",
    "#                     score2 = np.max(np.abs(residuals))\n",
    "#                     #print(f'Najwiekszy blad na scenariuszu {test_col}: {score2}')\n",
    "\n",
    "#             if score_sum < best_score:\n",
    "#                 best_model = model\n",
    "#                 best_score = score_sum\n",
    "#                 best_col = i\n",
    "#             if score2 < best_score2:\n",
    "#                 best_score2 = score2\n",
    "\n",
    "#             print(cols_test)\n",
    "#             print(f'Drop kolumny {deleted}: mse: {score_sum}, max_error: {score2}')\n",
    "\n",
    "#     print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "\n",
    "def try_drop_column(start_cols, test_col, criterion\n",
    "                   ,train_set ,test_set, model_type):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, len(base_cols)):\n",
    "        cols_test = np.copy(base_cols)\n",
    "        cols_test = np.delete(base_cols, i)\n",
    "\n",
    "        train_set_trim = train_set[:, cols_test]\n",
    "        test_set_trim = []\n",
    "        for test in test_set:\n",
    "            test_set_trim.append(test[:, cols_test])\n",
    "        \n",
    "        if model_type == 'SVR':\n",
    "            model = train_SVR(C=0.4, epsilon=0.014\n",
    "                              ,train_data=train_set_trim)\n",
    "        \n",
    "        pred = sim(model, test_set_trim, test_col)\n",
    "        \n",
    "        score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "        residuals = np.copy(pred[:, 0])\n",
    "        for j in range(0, len(residuals)):\n",
    "            residuals[j] = residuals[j] - \\\n",
    "            test_set_trim[test_col][j, -1]\n",
    "        score2 = np.max(np.abs(residuals))\n",
    "\n",
    "        if score < best_score:\n",
    "            best_model = model\n",
    "            best_score = score\n",
    "            best_col = i\n",
    "\n",
    "        print(cols_test)\n",
    "        print(f'Test kolumny {base_cols[i]}: mse: {score}, error_max: {score2}' )\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "           \n",
    "def dump_timeseries(Xs, Ys, TXs, TYs, seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Xs,f)\n",
    "\n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Ys,f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TXs,f)\n",
    "\n",
    "    with open(f'ts_trim_test_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TYs,f)\n",
    "        \n",
    "        \n",
    "def dump_surges(SXs, SYs, seq_len):\n",
    "    with open(f'ts_surge_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(SXs,f)\n",
    "\n",
    "    with open(f'ts_surge_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(SYs,f)\n",
    "        \n",
    "        \n",
    "def load_timeseries(seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'rb') as f:\n",
    "        Xs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'rb') as f:\n",
    "        Ys = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'rb') as f:\n",
    "        TXs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_out_{seq_len}.pkl', 'rb') as f:\n",
    "        TYs = dill.load(f)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs\n",
    "\n",
    "\n",
    "def load_surges(seq_len):\n",
    "    with open(f'ts_surge_in_{seq_len}.pkl', 'rb') as f:\n",
    "        SXs = dill.load(f)\n",
    "\n",
    "    with open(f'ts_surge_out_{seq_len}.pkl', 'rb') as f:\n",
    "        SYs = dill.load(f)\n",
    "    \n",
    "    return SXs, SYs\n",
    "\n",
    "\n",
    "import sys\n",
    "bckStdout = sys.stdout\n",
    "\n",
    "def print2(msg, log_name=None):\n",
    "    if log_name == None:\n",
    "        print(msg)\n",
    "        \n",
    "    else:\n",
    "        import logging\n",
    "        for handler in logging.root.handlers[:]:\n",
    "            logging.root.removeHandler(handler)\n",
    "        logging.basicConfig(filename=log_name,level=logging.INFO)\n",
    "        logging.info(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "    \n",
    "with open('train_set.pkl', 'rb') as  f:\n",
    "    train_set = dill.load(f)\n",
    "    \n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "    \n",
    "with open('surge_test.pkl', 'rb') as  f:\n",
    "    test_set_surge = dill.load(f)\n",
    "    \n",
    "with open('vdips_test.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('surge_test.pkl', 'rb') as  f:\n",
    "    test_set_surge = dill.load(f)\n",
    "\n",
    "test_set_surge.pop(6)\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.94486012, 0.30990414, 0.19629631, ..., 0.71962617, 0.69412928,\n",
       "         0.69527428],\n",
       "        [0.94498071, 0.31002704, 0.19629631, ..., 0.71962617, 0.69527428,\n",
       "         0.69673155],\n",
       "        [0.94413623, 0.310068  , 0.21851853, ..., 0.71962617, 0.69673155,\n",
       "         0.69746022],\n",
       "        ...,\n",
       "        [0.94196455, 0.31039566, 0.1851852 , ..., 0.71962617, 0.69589886,\n",
       "         0.69730405],\n",
       "        [0.94196455, 0.31023181, 0.16666668, ..., 0.71962617, 0.69730405,\n",
       "         0.69777246],\n",
       "        [0.94160227, 0.31010895, 0.21481483, ..., 0.71962617, 0.69777246,\n",
       "         0.69662748]]),\n",
       " array([[0.9923988 , 0.99250421, 0.20370372, ..., 0.70093458, 0.69506611,\n",
       "         0.69631519],\n",
       "        [0.99131296, 0.99254528, 0.19629631, ..., 0.70093458, 0.69631519,\n",
       "         0.69709587],\n",
       "        [0.99179547, 0.99270908, 0.19259261, ..., 0.70093458, 0.69709587,\n",
       "         0.69657541],\n",
       "        ...,\n",
       "        [0.99095102, 0.32014416, 0.20370372, ..., 0.71962617, 0.69574269,\n",
       "         0.69704382],\n",
       "        [0.99034755, 0.32034897, 0.21481483, ..., 0.71962617, 0.69704382,\n",
       "         0.69756428],\n",
       "        [0.99010637, 0.32088145, 0.21111113, ..., 0.71962617, 0.69756428,\n",
       "         0.69667954]]),\n",
       " array([[0.99070983, 0.34115669, 0.19629631, ..., 0.54205607, 0.86936609,\n",
       "         0.87056313],\n",
       "        [0.99215728, 0.34119765, 0.1814815 , ..., 0.54205607, 0.87056313,\n",
       "         0.870355  ],\n",
       "        [0.99288151, 0.34152531, 0.18888891, ..., 0.54205607, 0.870355  ,\n",
       "         0.86915791],\n",
       "        ...,\n",
       "        [0.9930021 , 0.34328663, 0.1851852 , ..., 0.54205607, 0.87040699,\n",
       "         0.86947018],\n",
       "        [0.9940881 , 0.3435324 , 0.17407409, ..., 0.54205607, 0.86947018,\n",
       "         0.86853341],\n",
       "        [0.9940881 , 0.34349145, 0.19259261, ..., 0.54205607, 0.86853341,\n",
       "         0.86837722]]),\n",
       " array([[0.98986501, 0.35360857, 0.21851853, ..., 0.45794393, 0.95570943,\n",
       "         0.95628188],\n",
       "        [0.99058874, 0.35352666, 0.21481483, ..., 0.45794393, 0.95628188,\n",
       "         0.95643801],\n",
       "        [0.99312269, 0.35393624, 0.19629631, ..., 0.45794393, 0.95643801,\n",
       "         0.95591756],\n",
       "        ...,\n",
       "        [0.98889977, 0.35397721, 0.19259261, ..., 0.45794393, 0.95513688,\n",
       "         0.95518898],\n",
       "        [0.99131296, 0.35418201, 0.19629631, ..., 0.45794393, 0.95518898,\n",
       "         0.95586551],\n",
       "        [0.9923988 , 0.35369047, 0.20740742, ..., 0.45794393, 0.95586551,\n",
       "         0.9560737 ]]),\n",
       " array([[0.67917476, 0.30793806, 0.20370372, ..., 0.87850467, 0.57926514,\n",
       "         0.5794733 ],\n",
       "        [0.67977823, 0.30769229, 0.19259261, ..., 0.87850467, 0.5794733 ,\n",
       "         0.57853652],\n",
       "        [0.68086407, 0.30810192, 0.10370372, ..., 0.87850467, 0.57853652,\n",
       "         0.57702717],\n",
       "        ...,\n",
       "        [0.6819499 , 0.30797901, 0.18888891, ..., 0.87850467, 0.57759967,\n",
       "         0.57864057],\n",
       "        [0.68158796, 0.30806092, 0.20740742, ..., 0.87850467, 0.57864057,\n",
       "         0.57942126],\n",
       "        [0.68122601, 0.30785615, 0.05925928, ..., 0.87850467, 0.57942126,\n",
       "         0.5794733 ]]),\n",
       " array([[0.67893358, 0.30765135, 0.19629631, ..., 0.71962617, 0.69792861,\n",
       "         0.69917768],\n",
       "        [0.68207066, 0.3078971 , 0.1814815 , ..., 0.71962617, 0.69917768,\n",
       "         0.69954204],\n",
       "        [0.68243261, 0.30818384, 0.19629631, ..., 0.71962617, 0.69954204,\n",
       "         0.69844906],\n",
       "        ...,\n",
       "        [0.6826738 , 0.30711887, 0.1851852 , ..., 0.71962617, 0.69943791,\n",
       "         0.69772045],\n",
       "        [0.68412158, 0.30761038, 0.19629631, ..., 0.71962617, 0.69772045,\n",
       "         0.6967836 ],\n",
       "        [0.6849664 , 0.30818384, 0.16666668, ..., 0.71962617, 0.6967836 ,\n",
       "         0.69709587]]),\n",
       " array([[0.99625978, 0.70680754, 0.18888891, ..., 0.93457944, 0.52024565,\n",
       "         0.50519826],\n",
       "        [0.99638054, 0.70717616, 0.17777779, ..., 0.93457944, 0.5202977 ,\n",
       "         0.50514728],\n",
       "        [0.99601859, 0.70688947, 0.19259261, ..., 0.93457944, 0.51915272,\n",
       "         0.50535114],\n",
       "        ...,\n",
       "        [0.99577727, 0.70647985, 0.1851852 , ..., 0.93457944, 0.47454981,\n",
       "         0.27703598],\n",
       "        [0.99529437, 0.70676661, 0.1851852 , ..., 0.93457944, 0.47559072,\n",
       "         0.27708696],\n",
       "        [0.99553576, 0.70688947, 0.17777779, ..., 0.93457944, 0.47652753,\n",
       "         0.27764754]])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fine_all.pkl','rb') as f:\n",
    "    fine_all = dill.load(f)\n",
    "    \n",
    "with open('surge_all.pkl','rb') as f:\n",
    "    surge_all = dill.load(f)\n",
    "    \n",
    "with open('vdips_all.pkl','rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych z normalnej pracy\n",
    "#okreslenie poprawności pracy na podstawie dołączonego arkusza\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tLICZBA PRÓBEK\n",
    "# \t02/03/2017\t\t0-2\t\t\t\t397721\n",
    "# \t06/03/2017\t\t3\t\t\t\t48513\n",
    "# \t16/03/2017\t\t4\t\t\t\t518348\n",
    "# \t22/03/2017\t\t5\t\t\t\t644888\n",
    "# \t23/03/2017\t\t6\t\t\t\t408255\n",
    "# \t29/03/2017\t\t7-8\t\t\t\t517850\n",
    "# \t13/04/2017\t\t9-13\t\t\t120844\n",
    "# \t19/04/2017\t\t14-17\t\t\t636238\n",
    "# \t28/04/2017\t\t18\t\t\t\t227459\n",
    "# \t12/05/2017\t\t19-20\t\t\t332583\n",
    "# \t17/05/2017\t\t21\t\t\t\t1993321\n",
    "# \t07/06/2017\t\t22-23\t\t\t183853\n",
    "# \t21/08/2017\t\t24-31\t\t\t738162\n",
    "# \t22/08/2017\t\t32-35\t\t\t1952183\n",
    "# \t23/08/2017\t\t36-37\t\t\t287377\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\fine\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_fine = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_fine.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma w danych z etykietą Voltage Dips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_fine)):\n",
    "    data_fine[i] = data_fine[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja dodająca do zbiorów kolumny z przesuniętymi w czasie prękościami obrotowymi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def add_history(dataset):\n",
    "    for x in range(0, len(dataset)):\n",
    "        temp = np.concatenate(([dataset[x].values[0, 19]], dataset[x].values[0:-1, 19]))\n",
    "        temp2 = np.concatenate(([dataset[x].values[0, 20]], dataset[x].values[0:-1, 20]))\n",
    "        dataset[x].insert(42, 42, temp)\n",
    "        dataset[x].insert(43, 43, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dodanie dwóch kolumn z przesuniętymi w czasie prędkościami obrotowymi\n",
    "add_history(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_fine[1].values[:, 19])\n",
    "plt.plot(data_fine[1].values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja usuwająca zakresy wierszy ze zbioru danych\n",
    "import pandas as pd\n",
    "def drop_rows(dataset, beg, end):\n",
    "    dataset.drop(dataset.index[beg:end], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lobs = 0\n",
    "for dset in data_fine:\n",
    "    lobs += len(dset[0])\n",
    "    \n",
    "for dset in data_surge:\n",
    "    lobs += len(dset[0])\n",
    "    \n",
    "for dset in data_vdips:\n",
    "    lobs += len(dset[0])\n",
    "    \n",
    "print(lobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnych obserwacji- długie stany ustalone\n",
    "#wybór na podstawie analizy wykresów przebiegów\n",
    "#zbiory po przetworzeniu:\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tPRÓBEK\t\n",
    "# \t02/03/2017\t\t0-1\t\t\t\t194697\t\t\t\n",
    "# \t06/03/2017\t\t2\t\t\t\t48513\n",
    "# \t16/03/2017\t\t3\t\t\t\t128348\n",
    "# \t22/03/2017\t\t4\t\t\t\t299888\n",
    "# \t23/03/2017\t\t5\t\t\t\t193255\n",
    "# \t29/03/2017\t\t6\t\t\t\t157116\n",
    "# \t13/04/2017\t\t7-11\t\t\t120844\n",
    "# \t19/04/2017\t\t12-13\t\t\t89385\n",
    "# \t28/04/2017\t\t14\t\t\t\t127459\n",
    "# \t12/05/2017\t\t15-16\t\t\t107583\n",
    "# \t17/05/2017\t\t17\t\t\t\t693321\n",
    "# \t07/06/2017\t\t18-19\t\t\t123853\n",
    "# \t21/08/2017\t\t20-22\t\t\t296545\n",
    "# \t22/08/2017\t\t23-25\t\t\t293511\n",
    "# \t23/08/2017\t\t26-27\t\t\t41377\n",
    "drop_rows(data_fine[2], 100000, 300000)\n",
    "drop_rows(data_fine[4], 370000, 500000)\n",
    "drop_rows(data_fine[4], 220000, 340000)\n",
    "drop_rows(data_fine[4], 60000, 200000)\n",
    "drop_rows(data_fine[5], 520000, 620000)\n",
    "drop_rows(data_fine[5], 460000, 500000)\n",
    "drop_rows(data_fine[5], 360000, 390000)\n",
    "drop_rows(data_fine[5], 140000, 265000)\n",
    "drop_rows(data_fine[5], 50000, 100000)\n",
    "drop_rows(data_fine[6], 350000, 360000)\n",
    "drop_rows(data_fine[6], 300000, 320000)\n",
    "drop_rows(data_fine[6], 210000, 270000)\n",
    "drop_rows(data_fine[6], 140000, 190000)\n",
    "drop_rows(data_fine[6], 50000, 125000)\n",
    "drop_rows(data_fine[8], 410000, 450000)\n",
    "drop_rows(data_fine[8], 140000, 375000)\n",
    "drop_rows(data_fine[8], 0, 35000)\n",
    "drop_rows(data_fine[15], 65000, -1)\n",
    "drop_rows(data_fine[17], 20000, 100000)\n",
    "drop_rows(data_fine[18], 100000, 200000)\n",
    "drop_rows(data_fine[19], 210000, 240000)\n",
    "drop_rows(data_fine[19], 0, 150000)\n",
    "drop_rows(data_fine[20], 10000, 55000)\n",
    "drop_rows(data_fine[21], 1300000, 1750000)\n",
    "drop_rows(data_fine[21], 250000, 1100000)\n",
    "drop_rows(data_fine[23], 90000, 110000)\n",
    "drop_rows(data_fine[23], 40000, 80000)\n",
    "drop_rows(data_fine[29], 165000, 180000)\n",
    "drop_rows(data_fine[29], 100000, 140000)\n",
    "drop_rows(data_fine[29], 32000, 75000)\n",
    "drop_rows(data_fine[30], 150000, 180000)\n",
    "drop_rows(data_fine[30], 125000, 140000)\n",
    "drop_rows(data_fine[30], 66000, 92000)\n",
    "drop_rows(data_fine[30], 25000, 50000)\n",
    "drop_rows(data_fine[31], 85000, 160000)\n",
    "drop_rows(data_fine[31], 20000, 65000)\n",
    "drop_rows(data_fine[33], 400000, 500000)\n",
    "drop_rows(data_fine[33], 230000, 375000)\n",
    "drop_rows(data_fine[33], 30000, 200000)\n",
    "drop_rows(data_fine[34], 330000, 440000)\n",
    "drop_rows(data_fine[34], 200000, 310000)\n",
    "drop_rows(data_fine[34], 165000, 180000)\n",
    "drop_rows(data_fine[34], 30000, 150000)\n",
    "drop_rows(data_fine[35], 270000, 365000)\n",
    "drop_rows(data_fine[35], 160000, 256000)\n",
    "drop_rows(data_fine[35], 30000, 140000)\n",
    "drop_rows(data_fine[36], 100000, 195000)\n",
    "drop_rows(data_fine[36], 7000, 85000)\n",
    "drop_rows(data_fine[37], 7000, 80000)\n",
    "indices = 0, 7, 14, 16, 24, 25, 26, 27, 28, 32\n",
    "for i in sorted(indices, reverse=True):\n",
    "    del data_fine[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbicie wszystki pomiarów do jednego dataframe\n",
    "# \tDATA\t\t\tPOCZATEK\t\tKONIEC\n",
    "# \t02/03/2017\t\t0\t\t\t\t194696\t\t\t\n",
    "# \t06/03/2017\t\t194697\t\t\t243209\n",
    "# \t16/03/2017\t\t243210\t\t\t371557\n",
    "# \t22/03/2017\t\t371558\t\t\t671445\n",
    "# \t23/03/2017\t\t671446\t\t\t864700\n",
    "# \t29/03/2017\t\t864701\t\t\t1021816\n",
    "# \t13/04/2017\t\t1021817\t\t\t1142660\n",
    "# \t19/04/2017\t\t1142661 \t\t1243538\n",
    "# \t28/04/2017\t\t1243539\t\t\t1370997\n",
    "# \t17/05/2017\t\t1370998\t\t\t693321\n",
    "# \t12/05/2017\t\t1370998 \t\t1478580\t\t\n",
    "# \t07/06/2017\t\t1478579\t\t\t2197611\n",
    "# \t21/08/2017\t\t2197611\t\t\t2497184\n",
    "# \t22/08/2017\t\t2497185\t\t\t2812172\n",
    "# \t23/08/2017\t\t2812173\t\t\t2927187\n",
    "import pandas as pd\n",
    "fine_all = pd.concat(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(fine_all.values[:, 5], label='ActPressureDiffOrifice1')\n",
    "# plt.plot(fine_all.values[:, 8], label='ActFlowInlet')\n",
    "plt.plot(fine_all.values[1080000:1110000, 19], label='ActSpeedCompressorTop')\n",
    "plt.xlabel('Numer obserwacji')\n",
    "plt.ylabel('Nieznormalizowana wartość sygnału')\n",
    "plt.legend(loc='upper left')\n",
    "#plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnej już listy dataframe'ow\n",
    "del data_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[42920:42920, 5], label='ActPressureDiffOrifice1')\n",
    "plt.plot(fine_all.values[42920:42920, 8], label='ActFlowInlet')\n",
    "plt.plot(fine_all.values[42920:42950, 19], label='ActSpeedCompressorTop')\n",
    "plt.xlabel('Numer obserwacji')\n",
    "plt.ylabel('Nieznormalizowana wartość sygnału')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie outlierów\n",
    "#stwierdzone na podstawie analizy wykresów przebiegów\n",
    "indices = [42931, 42932, 42933, 42934, 42935, 42936, 42937, 42938, 42939]\n",
    "for i in sorted(indices, reverse=True):\n",
    "    fine_all = fine_all.drop(i)\n",
    "drop_rows(fine_all, 42900, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapicklowanie przetworzonego zbioru danych pochodzących z normalnej pracy\n",
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:  \n",
    "    dill.dump(fine_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Voltage Dips\n",
    "#Dane pochodzą z jednego dnia 14/03/2016\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\vdips\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_vdips = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_vdips.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumny NaN-ów\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_vdips)):\n",
    "    data_vdips[i] = data_vdips[i].drop(42, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóżnionymi o jedną próbkę wartościami prędkości obrotowych\n",
    "add_history(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_vdips[1].values[:, 19])\n",
    "plt.plot(data_vdips[1].values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie danych do jednego dataframe\n",
    "import pandas as pd\n",
    "vdips_all = pd.concat(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie zbioru\n",
    "import dill\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzenej już listy dataframe'ów\n",
    "del data_vdips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Surge do listy\n",
    "#Stan danych po usunięci dwóch wadliwych dataframe:\n",
    "# \tDATA\t\t\tNR ZBIORU\t\tPRÓBEK\n",
    "# \t17/01/2017\t\t\t0-5\t\t\t411448\n",
    "# \t18/01/2017\t\t\t6-23\t\t1370255\n",
    "# \t19/01/2017\t\t\t24-30\t\t271524\n",
    "# \t20/01/2017\t\t\t31-35\t\t916380\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\surge\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_surge = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_surge.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie uszkodzonych dataFrame\n",
    "#wykonać x2\n",
    "del data_surge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma vdips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_surge)):\n",
    "    data_surge[i] = data_surge[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóźnionymi o jedną próbkę predkościami obrotowymi\n",
    "add_history(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie wszystkich danych w jeden dataframe\n",
    "#Rozkład danych po zbiciu:\n",
    "# \tDATA\t\t\tPOCZĄTEK\t\tKONIEC\n",
    "# \t17/01/2017\t\t0\t\t\t\t411447\n",
    "# \t18/01/2017\t\t411448\t\t\t1781702\n",
    "# \t19/01/2017\t\t1781703\t\t\t2053226\n",
    "# \t20/01/2017\t\t2053227\t\t\t2969606\n",
    "import pandas as pd\n",
    "surge_all = pd.concat(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie serii danych\n",
    "import dill\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzebnej już listy zbiorów\n",
    "del data_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max_df(fine, col_num):\n",
    "    max_val = fine[col_num].max()\n",
    "    return max_val\n",
    "def column_min_df(fine, col_num):\n",
    "    min_val = fine[col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja realizująca normalizacją min-max danych\n",
    "def normalise(dataset, columns_ranges):\n",
    "        for col in range(0, 44):\n",
    "            min_val = columns_ranges[col, 1]\n",
    "            max_val = columns_ranges[col, 0]\n",
    "            dataset[col] = dataset[col].subtract(min_val)\n",
    "            dataset[col] = dataset[col].divide(max_val - min_val)\n",
    "                #dataset.values[row, col] = (value - min_val) / (max_val - min_val)\n",
    "            print(f'Znormalizowano kolumnę {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max_df(fine_all, col)\n",
    "    col_ranges[col, 1] = column_min_df(fine_all, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wstepna normalizacja ułatwiająca porównywanie przebiegów\n",
    "#na wspólnym wykresie.\n",
    "fine_all_norm = fine_all.copy()\n",
    "normalise(fine_all_norm, col_ranges)\n",
    "\n",
    "# vdips_all_norm = vdips_all.copy()\n",
    "# normalise(vdips_all_norm, col_ranges)\n",
    "\n",
    "# surge_all_norm = surge_all.copy()\n",
    "# normalise(surge_all_norm, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all_norm.values[:, 19])\n",
    "plt.plot(fine_all_norm.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = pd.read_excel('fine_corrs.xls').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'Numer zmiennej')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sbrn\n",
    "sbrn.heatmap(abs(corrs[:,1:]), vmin=0,vmax=1,cmap='plasma')\n",
    "plt.title('Graficzne przedstawienie macierzy modułu korelacji')\n",
    "plt.xlabel('Numer zmiennej')\n",
    "plt.ylabel('Numer zmiennej')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pearsons' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fe76d767ca6a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mconn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlite3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corrDB.db'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mPearsons\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corr'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_sql_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"select rowid,* from corr\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pearsons' is not defined"
     ]
    }
   ],
   "source": [
    "#DOBÓR ZMIENNYCH WEJŚCIOWYCH\n",
    "#obliczenie współczynników korelacji liniowej między atrybutami\n",
    "#przekierowanie korelacji do arkusza w celu łatwiejszego przeglądania\n",
    "# Pearsons = fine_all_norm.corr()\n",
    "# # path = \"fine_corrs.xls\"\n",
    "# # Pearsons.to_excel(path)\n",
    "\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('corrDB.db')\n",
    "Pearsons.to_sql('corr',conn)\n",
    "pd.read_sql_query(\"select rowid,* from corr\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crsr = conn.cursor\n",
    "query = \"\"\"\n",
    "select \n",
    "    round(\"10\",3)\n",
    "    ,round(\"11\",3)\n",
    "    ,round(\"14\",3)\n",
    "    ,round(\"19\",3)\n",
    "    ,round(\"40\",3)\n",
    "\n",
    "from corr\n",
    "where rowid-1 in (10,11,14,40,19)\n",
    "\"\"\"\n",
    "pd.read_sql_query(query, conn)\n",
    "res = conn.execute(query)\n",
    "for row in res:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KONTYNUACJA DOBORU ATRYBUTÓW - OBSERWACJA PRZEBIEGÓW\n",
    "#wykreślenie przebiegu wyjścia i potencjalnych wejść\n",
    "#wstepny dobór wejśc na podstawie analizy korelacji Pearsona (impl:pandas.df.corr) i przebiegów\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plotter.plot(fine_all_norm.values[827459:864021, 19], label='Prędkość')\n",
    "plotter.plot(fine_all_norm.values[827459:864021, 10], label='Przepływ')\n",
    "plotter.plot(fine_all_norm.values[827459:864021, 11], label='Ciśnienie')\n",
    "plotter.plot(fine_all_norm.values[827459:864021, 14], label='Temperatura')\n",
    "plotter.plot(fine_all_norm.values[827459:864021, 40], label='Moc')\n",
    "plotter.xlabel(\"Numer obserwacji\")\n",
    "plotter.ylabel(\"Znormalizowana wartość sygnału\")\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie wstępnych znormalizowanych danych\n",
    "del fine_all_norm\n",
    "del vdips_all_norm\n",
    "del surge_all_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)\n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)\n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, 19])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[1][:, 19])\n",
    "plt.plot(test_set[1][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max(np_array, col_num):\n",
    "    max_val = np_array[:, col_num].max()\n",
    "    return max_val\n",
    "def column_min(np_array, col_num):\n",
    "    min_val = np_array[:, col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max(train_set, col)\n",
    "    col_ranges[col, 1] = column_min(train_set, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ranges[30, 0] = 100\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise(fine_all, col_ranges)\n",
    "normalise(vdips_all, col_ranges)\n",
    "normalise(surge_all, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surge_all.values[:, 19])\n",
    "plt.plot(surge_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vdips_all.values[:, 19])\n",
    "plt.plot(vdips_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:\n",
    "    dill.dump(fine_all, f)\n",
    "\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)\n",
    "\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, -1])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "\n",
    "col = len(test_set[0][0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "for x in test_set:\n",
    "    x[:, [19, col-1]] = x[:, [col-1, 19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumt = 0\n",
    "for t in test_set:\n",
    "    print(len(t))\n",
    "    sumt += len(t)\n",
    "print(sumt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sumall = 639820+353078\n",
    "train = 353078/sumall\n",
    "print(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[0][:, -1])\n",
    "plt.plot(test_set[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nalezy wykonywac gdyz pythonowe referencje przycinaja nam zbior\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, -2, -1]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, -2, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set_trim[:, -2])\n",
    "plt.plot(train_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set_trim[:, -2])\n",
    "plt.plot(train_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_trim[:, -2])\n",
    "plt.plot(test_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structs_list = [[6, 4]]\n",
    "model = multi_train_NLP(structs_list=structs_list, \n",
    "                        patience=3,\n",
    "                        batch_size=800,\n",
    "                        epochs=10,\n",
    "                        train_data=train_set_trim,\n",
    "                        test_data=test_set_trim,\n",
    "                        set_num=0\n",
    "                       ,criterion='mse'\n",
    "                       ,single_times=1\n",
    "                       ,dropout_2list=[[0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sim(model, test_set_trim, 1)\n",
    "eval_prediction(y, test_set_trim[1][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[0.5, 0.1]]\n",
    "model = multi_train_SVR(C_eps_pairs=params\n",
    "                       ,train_data=train_set_trim\n",
    "                       ,test_data=test_set_trim\n",
    "                       ,set_num=0,\n",
    "                       criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sim(model, test_set_trim, 0)\n",
    "y = y.reshape(-1, 1)\n",
    "eval_prediction(y, test_set_trim[0][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, 19]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, 19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_train_LSTM(structure_list=[[4]]\n",
    "                        ,patience=3, epochs=25\n",
    "                        ,batch_size=1000, train_X=Xs\n",
    "                        ,train_Y=Ys, test_X=TXs\n",
    "                        ,test_Y=TYs, set_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(TXs[0])\n",
    "eval_prediction(pred, TYs[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for i in sorted(indices, reverse=True):\n",
    "    train_set = np.delete(train_set, i, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, -1])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbioru testowego początkowo zawierającego wszystkie atrybuty\n",
    "#zbiór testowy pochodzi z prawidłowej pracy\n",
    "#uzasadnienie doboru w grafice zbior_uczacy.png\n",
    "import numpy as np\n",
    "test_set=[]\n",
    "#02/03/2017\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "#22/03/2017\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "#29/03/2017\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "#13/04/2017\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "#19/04/2017\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "#28/04/2017\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "#12/05/2017\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "#17/05/2017\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "for i in range(0, len(test_set)):\n",
    "    test_set[i] = test_set[i][~np.isnan(test_set[i]).any(axis=1)]\n",
    "    col = len(test_set[i][0])\n",
    "    test_set[i][:, [19, col-1]] = test_set[i][:, [col-1, 19]]\n",
    "    \n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[0][:, -1])\n",
    "plt.plot(test_set[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set[j] = np.delete(test_set[j], i, axis=1)\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(vdips_all, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbior testowy z danych z etykietą Voltage Dips\n",
    "#Dane z 14/03/2017\n",
    "import numpy as np\n",
    "test_set_vdips=[]\n",
    "test_set_vdips.append(vdips_all.values[250000:270000, :])\n",
    "test_set_vdips.append(vdips_all.values[380000:400000, :])\n",
    "test_set_vdips.append(vdips_all.values[500000:520000, :])\n",
    "test_set_vdips.append(vdips_all.values[820000:840000, :])\n",
    "test_set_vdips.append(vdips_all.values[980000:1000000, :])\n",
    "test_set_vdips.append(vdips_all.values[1100000:1120000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_vdips[i] = test_set_vdips[i][~np.isnan(test_set_vdips[i]).any(axis=1)]\n",
    "    col = len(test_set_vdips[i][0])\n",
    "    test_set_vdips[i][:, [19, col-1]] = test_set_vdips[i][:, [col-1, 19]]\n",
    "    \n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_vdips)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_vdips[j] = np.delete(test_set_vdips[j], i, axis=1)\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)\n",
    "    \n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_vdips[0][:, -1])\n",
    "plt.plot(test_set_vdips[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbiorów testowych z etykietą Surge\n",
    "import numpy as np\n",
    "test_set_surge=[]\n",
    "#17/01/2017\n",
    "test_set_surge.append(surge_all.values[150000:400000, :])\n",
    "#18/01/2017\n",
    "test_set_surge.append(surge_all.values[570000:800000, :])\n",
    "test_set_surge.append(surge_all.values[900000:1030000, :])\n",
    "test_set_surge.append(surge_all.values[1113000:1150000, :])\n",
    "test_set_surge.append(surge_all.values[1280000:1305000, :])\n",
    "test_set_surge.append(surge_all.values[1350000:1380000, :])\n",
    "test_set_surge.append(surge_all.values[1580000:1605000, :])\n",
    "#20/01/2017\n",
    "test_set_surge.append(surge_all.values[2700000:2900000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_surge[i] = test_set_surge[i][~np.isnan(test_set_surge[i]).any(axis=1)]\n",
    "    col = len(test_set_surge[i][0])\n",
    "    test_set_surge[i][:, [19, col-1]] = test_set_surge[i][:, [col-1, 19]]\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)\n",
    "    \n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_surge)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_surge[j] = np.delete(test_set_surge[j], i, axis=1)\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)\n",
    "    \n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_surge[0][:, -1])\n",
    "plt.plot(test_set_surge[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu modelu: 2.7991883754730225s\n",
      "[0, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 0: mse: 0.03778648792688695, max_error: 0.30919605349140766\n",
      "Czas treningu modelu: 2.514986991882324s\n",
      "[1, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 1: mse: 0.001508266465446386, max_error: 0.20206095239679356\n",
      "Czas treningu modelu: 3.120988368988037s\n",
      "[2, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 2: mse: 0.0014940132946844784, max_error: 0.19800233063504324\n",
      "Czas treningu modelu: 2.687988519668579s\n",
      "[3, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 3: mse: 0.0026657904783219844, max_error: 0.20697085499693246\n",
      "Czas treningu modelu: 2.886056423187256s\n",
      "[4, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 4: mse: 0.0027428957753094832, max_error: 0.22569721468243786\n",
      "Czas treningu modelu: 2.238999128341675s\n",
      "[5, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 5: mse: 0.001637388349673635, max_error: 0.21611841803698162\n",
      "Czas treningu modelu: 3.026991605758667s\n",
      "[6, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 6: mse: 0.0019125546361416794, max_error: 0.2262502248192787\n",
      "Czas treningu modelu: 3.348997116088867s\n",
      "[7, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 7: mse: 0.0020039132699071994, max_error: 0.22480455251153497\n",
      "Czas treningu modelu: 2.9249916076660156s\n",
      "[8, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 8: mse: 0.0013671993596621186, max_error: 0.20609974652267682\n",
      "Czas treningu modelu: 3.563993453979492s\n",
      "[10, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 10: mse: 0.0014204023032457636, max_error: 0.19727522495518401\n",
      "Czas treningu modelu: 3.0889999866485596s\n",
      "[11, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 11: mse: 0.0019534177836451633, max_error: 0.2235912646780368\n",
      "Czas treningu modelu: 4.161994218826294s\n",
      "[12, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 12: mse: 0.0030532866671993428, max_error: 0.231557726991655\n",
      "Czas treningu modelu: 3.3979954719543457s\n",
      "[13, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 13: mse: 0.002082998272885708, max_error: 0.20645793399089796\n",
      "Czas treningu modelu: 3.378990888595581s\n",
      "[14, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 14: mse: 0.002084619668551805, max_error: 0.20632359469278816\n",
      "Czas treningu modelu: 3.0649943351745605s\n",
      "[15, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 15: mse: 0.0015908173472042398, max_error: 0.2279094858554972\n",
      "Czas treningu modelu: 2.6329941749572754s\n",
      "[16, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 16: mse: 0.0026615198674928764, max_error: 0.22430815089945833\n",
      "Czas treningu modelu: 2.235989570617676s\n",
      "[17, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 17: mse: 0.0010887973958540072, max_error: 0.18508146951127025\n",
      "Czas treningu modelu: 3.0119850635528564s\n",
      "[18, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 18: mse: 0.0021087126087132348, max_error: 0.20680965599038315\n",
      "Czas treningu modelu: 2.6159934997558594s\n",
      "[19, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 19: mse: 0.0013242475900545527, max_error: 0.18333685661106602\n",
      "Czas treningu modelu: 2.708388328552246s\n",
      "[20, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 20: mse: 0.001474628381468496, max_error: 0.21267717558606047\n",
      "Czas treningu modelu: 2.8149971961975098s\n",
      "[21, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 21: mse: 0.0014647193279062531, max_error: 0.21150119154687685\n",
      "Czas treningu modelu: 3.0349881649017334s\n",
      "[22, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 22: mse: 0.002506232363750025, max_error: 0.21336486522858178\n",
      "Czas treningu modelu: 3.273989677429199s\n",
      "[23, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 23: mse: 0.001924526640940447, max_error: 0.21184320494150177\n",
      "Czas treningu modelu: 3.2069976329803467s\n",
      "[24, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 24: mse: 0.0017384083930541447, max_error: 0.21788356395089803\n",
      "Czas treningu modelu: 2.5689942836761475s\n",
      "[28, 9, 25, 26, 27, 29, 30, 31]\n",
      "Test kolumny 28: mse: 0.0015108821902400503, max_error: 0.21792026506200313\n",
      "Najlepsza kolumna to: 17 z wynikiem: 0.0010887973958540072\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0])\n",
    "cols_taken = [9, 25, 26, 27, 29, col-2, col-1]\n",
    "try_add_column(cols_taken=cols_taken\n",
    "              ,test_col=9\n",
    "              ,criterion='mse'\n",
    "              ,train_set=train_set\n",
    "              ,test_set=test_set\n",
    "              ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu modelu: 3.4440040588378906s\n",
      "[25 26 27 28 29 30 31]\n",
      "Test kolumny 9: mse: 0.0011038996458295266, error_max: 0.20060263400878986\n",
      "Czas treningu modelu: 3.225996255874634s\n",
      "[ 9 26 27 28 29 30 31]\n",
      "Test kolumny 25: mse: 0.0010533939357619485, error_max: 0.20023742149147783\n",
      "Czas treningu modelu: 3.0929884910583496s\n",
      "[ 9 25 27 28 29 30 31]\n",
      "Test kolumny 26: mse: 0.0007829470812618104, error_max: 0.12124217502444423\n",
      "Czas treningu modelu: 4.3000006675720215s\n",
      "[ 9 25 26 28 29 30 31]\n",
      "Test kolumny 27: mse: 0.35811200920096015, error_max: 0.8848817570580891\n",
      "Czas treningu modelu: 3.3659961223602295s\n",
      "[ 9 25 26 27 29 30 31]\n",
      "Test kolumny 28: mse: 0.0011561342648048296, error_max: 0.20543162303903362\n",
      "Czas treningu modelu: 4.807165861129761s\n",
      "[ 9 25 26 27 28 30 31]\n",
      "Test kolumny 29: mse: 0.003319496383335782, error_max: 0.2786687473097841\n",
      "Czas treningu modelu: 7.968132734298706s\n",
      "[ 9 25 26 27 28 29 31]\n",
      "Test kolumny 30: mse: 0.06522512839338886, error_max: 0.57957546049263\n",
      "Czas treningu modelu: 8.74599027633667s\n",
      "[ 9 25 26 27 28 29 30]\n",
      "Test kolumny 31: mse: 0.06347519286391246, error_max: 0.5735853821210588\n",
      "Najlepsza kolumna to: 2 z wynikiem: 0.0007829470812618104\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0])\n",
    "base_cols = [9, 25, 26, 27, 28, 29,col-2, col-1]\n",
    "try_drop_column(start_cols=base_cols\n",
    "               ,test_col=9\n",
    "               ,criterion='mse'\n",
    "               ,train_set=train_set\n",
    "               ,test_set=test_set\n",
    "               ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 25, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0]) - 1\n",
    "#cols_taken = [17, 29, 5, 27, col-1, col]\n",
    "cols_taken = [9, 25, 27, 28, 29, col-1, col]\n",
    "print(cols_taken)\n",
    "\n",
    "train_set_trim = train_set[:, cols_taken]\n",
    "test_set_trim = []\n",
    "for test in test_set:\n",
    "    test_set_trim.append(test[:, cols_taken])\n",
    "    \n",
    "vdips_trim = []\n",
    "for test in test_set_vdips:\n",
    "    vdips_trim.append(test[:, cols_taken])\n",
    "    \n",
    "surge_trim = []\n",
    "for test in test_set_surge:\n",
    "    surge_trim.append(test[:, cols_taken])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_set_trim[15])):\n",
    "    plt.plot(test_set_trim[15][:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_set_trim[0][0])):\n",
    "    plt.plot(test_set_trim[0][:, i], label=f'{i}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas treningu modelu: 16.525001049041748s\n"
     ]
    }
   ],
   "source": [
    "model = train_SVR(C=0.325, epsilon=0.006, train_data=train_set_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_eps = generate_SVR_params(0.275, 0.4, 0.025\n",
    "                           ,0.006, 0.013, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trenuje SVR o C=0.275 i epsilon=0.006\n",
      "Czas treningu modelu: 18.15299654006958s\n",
      "Wynik biezacego SVRa: 0.0005331518393354558\n",
      "Trenuje SVR o C=0.275 i epsilon=0.007\n",
      "Czas treningu modelu: 17.649584770202637s\n",
      "Wynik biezacego SVRa: 0.0005771581958887306\n",
      "Trenuje SVR o C=0.275 i epsilon=0.008\n",
      "Czas treningu modelu: 14.523760080337524s\n",
      "Wynik biezacego SVRa: 0.0005758545753617385\n",
      "Trenuje SVR o C=0.275 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 11.853049278259277s\n",
      "Wynik biezacego SVRa: 0.0005932484979730749\n",
      "Trenuje SVR o C=0.275 i epsilon=0.01\n",
      "Czas treningu modelu: 10.296001672744751s\n",
      "Wynik biezacego SVRa: 0.0006253677277588499\n",
      "Trenuje SVR o C=0.275 i epsilon=0.011\n",
      "Czas treningu modelu: 8.34701418876648s\n",
      "Wynik biezacego SVRa: 0.0006614209711790274\n",
      "Trenuje SVR o C=0.275 i epsilon=0.012\n",
      "Czas treningu modelu: 8.188998222351074s\n",
      "Wynik biezacego SVRa: 0.0006868260221063819\n",
      "Trenuje SVR o C=0.275 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.766006708145142s\n",
      "Wynik biezacego SVRa: 0.0007185304072473087\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.006\n",
      "Czas treningu modelu: 20.302000999450684s\n",
      "Wynik biezacego SVRa: 0.0005345445431939076\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.007\n",
      "Czas treningu modelu: 20.599538326263428s\n",
      "Wynik biezacego SVRa: 0.0005697928770452777\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.008\n",
      "Czas treningu modelu: 14.185000658035278s\n",
      "Wynik biezacego SVRa: 0.0005963106663206482\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 11.912998676300049s\n",
      "Wynik biezacego SVRa: 0.0006129512765479844\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.01\n",
      "Czas treningu modelu: 10.355998992919922s\n",
      "Wynik biezacego SVRa: 0.0006296372948006498\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.011\n",
      "Czas treningu modelu: 10.626001596450806s\n",
      "Wynik biezacego SVRa: 0.000658851019912176\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.012\n",
      "Czas treningu modelu: 7.870998859405518s\n",
      "Wynik biezacego SVRa: 0.0006849561378669814\n",
      "Trenuje SVR o C=0.30000000000000004 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 7.319029331207275s\n",
      "Wynik biezacego SVRa: 0.0007294187764479658\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.006\n",
      "Czas treningu modelu: 19.247997760772705s\n",
      "Wynik biezacego SVRa: 0.0005232619679229373\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.007\n",
      "Czas treningu modelu: 17.62500500679016s\n",
      "Wynik biezacego SVRa: 0.0005673366197809955\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.008\n",
      "Czas treningu modelu: 14.360998392105103s\n",
      "Wynik biezacego SVRa: 0.0005882551074256929\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 11.188002109527588s\n",
      "Wynik biezacego SVRa: 0.0006102141886082815\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.01\n",
      "Czas treningu modelu: 9.48600172996521s\n",
      "Wynik biezacego SVRa: 0.0006477429671085307\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.011\n",
      "Czas treningu modelu: 8.888997554779053s\n",
      "Wynik biezacego SVRa: 0.0006604812454341807\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.012\n",
      "Czas treningu modelu: 7.142000436782837s\n",
      "Wynik biezacego SVRa: 0.000692631335429124\n",
      "Trenuje SVR o C=0.32500000000000007 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.616529703140259s\n",
      "Wynik biezacego SVRa: 0.0007342372630578562\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.006\n",
      "Czas treningu modelu: 18.95899486541748s\n",
      "Wynik biezacego SVRa: 0.0005396281923467063\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.007\n",
      "Czas treningu modelu: 17.843000650405884s\n",
      "Wynik biezacego SVRa: 0.0005649324570418566\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.008\n",
      "Czas treningu modelu: 12.86900019645691s\n",
      "Wynik biezacego SVRa: 0.0005883806743550552\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 11.342008590698242s\n",
      "Wynik biezacego SVRa: 0.0006115829398739736\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.01\n",
      "Czas treningu modelu: 10.634002685546875s\n",
      "Wynik biezacego SVRa: 0.0006377890388362191\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.011\n",
      "Czas treningu modelu: 7.919999837875366s\n",
      "Wynik biezacego SVRa: 0.0006719800217865904\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.012\n",
      "Czas treningu modelu: 7.766002655029297s\n",
      "Wynik biezacego SVRa: 0.0006973458051853038\n",
      "Trenuje SVR o C=0.3500000000000001 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.413992881774902s\n",
      "Wynik biezacego SVRa: 0.000723518669637258\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.006\n",
      "Czas treningu modelu: 20.051533699035645s\n",
      "Wynik biezacego SVRa: 0.0005413948945401455\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.007\n",
      "Czas treningu modelu: 17.158003330230713s\n",
      "Wynik biezacego SVRa: 0.0005671311864797964\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.008\n",
      "Czas treningu modelu: 14.084996461868286s\n",
      "Wynik biezacego SVRa: 0.0005905072355201187\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 10.679998874664307s\n",
      "Wynik biezacego SVRa: 0.0006063495165833897\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.01\n",
      "Czas treningu modelu: 9.627000570297241s\n",
      "Wynik biezacego SVRa: 0.0006525894218667432\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.011\n",
      "Czas treningu modelu: 8.768601655960083s\n",
      "Wynik biezacego SVRa: 0.0006706462260882558\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.012\n",
      "Czas treningu modelu: 7.641005992889404s\n",
      "Wynik biezacego SVRa: 0.0006945093455940271\n",
      "Trenuje SVR o C=0.3750000000000001 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.858995676040649s\n",
      "Wynik biezacego SVRa: 0.0007310914959129376\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.006\n",
      "Czas treningu modelu: 19.16300082206726s\n",
      "Wynik biezacego SVRa: 0.0005397599015111191\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.007\n",
      "Czas treningu modelu: 18.646538972854614s\n",
      "Wynik biezacego SVRa: 0.0005610755232911813\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.008\n",
      "Czas treningu modelu: 14.87952446937561s\n",
      "Wynik biezacego SVRa: 0.0005896760210014535\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 13.76755142211914s\n",
      "Wynik biezacego SVRa: 0.0006184057987119547\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.01\n",
      "Czas treningu modelu: 9.20699429512024s\n",
      "Wynik biezacego SVRa: 0.0006402283869079126\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.011\n",
      "Czas treningu modelu: 8.551993131637573s\n",
      "Wynik biezacego SVRa: 0.000668729929922737\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.012\n",
      "Czas treningu modelu: 7.457601070404053s\n",
      "Wynik biezacego SVRa: 0.0007082999374792844\n",
      "Trenuje SVR o C=0.40000000000000013 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.486000299453735s\n",
      "Wynik biezacego SVRa: 0.0007307950325216718\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.006\n",
      "Czas treningu modelu: 17.565988302230835s\n",
      "Wynik biezacego SVRa: 0.0005430368355602193\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.007\n",
      "Czas treningu modelu: 16.41600012779236s\n",
      "Wynik biezacego SVRa: 0.0005544587575414221\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.008\n",
      "Czas treningu modelu: 15.06999945640564s\n",
      "Wynik biezacego SVRa: 0.0005751802861643589\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.009000000000000001\n",
      "Czas treningu modelu: 11.031001806259155s\n",
      "Wynik biezacego SVRa: 0.0006106314306870642\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.01\n",
      "Czas treningu modelu: 9.503999948501587s\n",
      "Wynik biezacego SVRa: 0.0006518474886760354\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.011\n",
      "Czas treningu modelu: 8.325520753860474s\n",
      "Wynik biezacego SVRa: 0.0006753941195296712\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.012\n",
      "Czas treningu modelu: 6.828999757766724s\n",
      "Wynik biezacego SVRa: 0.0007022223728113875\n",
      "Trenuje SVR o C=0.42500000000000016 i epsilon=0.013000000000000001\n",
      "Czas treningu modelu: 6.367002487182617s\n",
      "Wynik biezacego SVRa: 0.0007380683448978744\n"
     ]
    }
   ],
   "source": [
    "model = multi_train_SVR(C_eps, train_set_trim, test_set_trim\n",
    "                        , 9 , 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=0.32500000000000007, cache_size=200, coef0=0.0, degree=3, epsilon=0.006,\n",
       "    gamma='scale', kernel='rbf', max_iter=-1, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_svr.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas symulacji scenariusza 9: 2.4120032787323s\n",
      "0.0005232619679229373\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005232619679229373"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "col = 9\n",
    "t_start = time.time()\n",
    "y = sim(model, test_set_trim, col)\n",
    "t_stop = time.time()\n",
    "print2(f'Czas symulacji scenariusza {col}: {t_stop-t_start}s')\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_svr.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0037895988344476715\n",
      "0.0028603410661145294\n",
      "0.0024475658469019384\n",
      "0.0027918221067253996\n",
      "0.0239212512637642\n",
      "0.0024475658469019384\n",
      "0.03723891018333845\n",
      "0.003158812231884584\n",
      "0.009821839095343635\n",
      "0.013322070157503108\n",
      "0.021739405163245778\n",
      "0.003158812231884584\n",
      "0.003621202812721833\n",
      "0.004029770838694836\n",
      "0.0036742556537940926\n",
      "0.04735584361903331\n",
      "0.002313377152532715\n",
      "0.002313377152532715\n",
      "0.0543239362391796\n",
      "0.011119051249364797\n",
      "0.001399829861498862\n",
      "0.002900832662578402\n",
      "0.0015426773625182536\n",
      "0.001399829861498862\n",
      "0.00604336567620871\n",
      "0.0037603589052262075\n",
      "0.0051326278955355145\n",
      "0.0016631240242084048\n",
      "0.011997940319579216\n",
      "0.0016631240242084048\n",
      "0.01034475167341345\n",
      "0.01404983695574597\n",
      "0.004744736198307425\n",
      "0.003800428033407099\n",
      "0.0019381996774156438\n",
      "0.0019381996774156438\n",
      "0.004510208989386137\n",
      "0.005908188458164138\n",
      "0.010520883794251333\n",
      "0.0012995717483740212\n",
      "0.005294708329408651\n",
      "0.0012995717483740212\n",
      "0.00182136572737779\n",
      "0.007345190863310463\n",
      "0.011402712833835474\n",
      "0.004455052938073557\n",
      "0.004695839816791725\n",
      "0.00182136572737779\n",
      "0.003866525821309752\n",
      "0.006896439063647462\n",
      "0.024275509761405627\n",
      "0.006767712839260063\n",
      "0.005844647430598042\n",
      "0.003866525821309752\n",
      "0.003489126202666241\n",
      "0.007894405657276829\n",
      "0.0035988267685927045\n",
      "0.0034524173054525237\n",
      "0.0019950840684336076\n",
      "0.0019950840684336076\n",
      "0.004793166929875144\n",
      "0.0051622542187532905\n",
      "0.01314888728876505\n",
      "0.0034362937190207153\n",
      "0.004607300488155425\n",
      "0.0034362937190207153\n",
      "0.007811413000485539\n",
      "0.003954162216193051\n",
      "0.001937998827444996\n",
      "0.010942381187994558\n",
      "0.012950772201093017\n",
      "0.001937998827444996\n",
      "0.00424890440699713\n",
      "0.0026652392953004822\n",
      "0.004624905129478562\n",
      "0.003677006399421959\n",
      "0.0036466927217692346\n",
      "0.0026652392953004822\n",
      "0.01715456182836138\n",
      "0.00932802551273147\n",
      "0.004494171772718067\n",
      "0.0017292662295954509\n",
      "0.003953464349811082\n",
      "0.0017292662295954509\n",
      "0.005307435148826274\n",
      "0.0023471715733151574\n",
      "0.005025750011546164\n",
      "0.051720088114213386\n",
      "0.003307858522701981\n",
      "0.0023471715733151574\n",
      "0.008764404858107455\n",
      "0.0018582857954496732\n",
      "0.004244458858143083\n",
      "0.005886988173931746\n",
      "0.009272429791651422\n",
      "0.0018582857954496732\n",
      "0.005723168180044068\n",
      "0.015519677724162469\n",
      "0.010709504926858467\n",
      "0.008521442051019549\n",
      "0.0514220166306711\n",
      "0.005723168180044068\n",
      "0.010877597333390575\n",
      "0.0037137862324207868\n",
      "0.0014453374118269515\n",
      "0.0032236163467709153\n",
      "0.008359241127292366\n",
      "0.0014453374118269515\n",
      "0.003728327255415401\n",
      "0.00797390019084658\n",
      "0.006140646031297571\n",
      "0.00631404732794443\n",
      "0.0022740384357442293\n",
      "0.0022740384357442293\n",
      "0.004525166180400847\n",
      "0.0033909070303610924\n",
      "0.023826572871875003\n",
      "0.017514726564645387\n",
      "0.011212457131747997\n",
      "0.0033909070303610924\n",
      "0.0033244753547194885\n",
      "0.00453997031161974\n",
      "0.018555264143826636\n",
      "0.024305494645457636\n",
      "0.0063775862211276615\n",
      "0.0033244753547194885\n",
      "0.03940812272983409\n",
      "0.004536759733107828\n",
      "0.06435618414380542\n",
      "0.00878251987267812\n",
      "0.003909248127428882\n",
      "0.003909248127428882\n",
      "0.0030595998566964127\n",
      "0.0034683244983238323\n",
      "0.003316453641409571\n",
      "0.005413892629386248\n",
      "0.004219159576724889\n",
      "0.0030595998566964127\n",
      "0.006501406006382027\n",
      "0.020019625910927555\n",
      "0.00911260260823957\n",
      "0.011120578741994225\n",
      "0.0038902517751195904\n",
      "0.0038902517751195904\n",
      "0.001930997969105524\n",
      "0.0039005123436482536\n",
      "0.010299822826689101\n",
      "0.0071566199106951845\n",
      "0.006760929474546201\n",
      "0.001930997969105524\n",
      "0.016781184725017273\n",
      "0.002559570865966284\n",
      "0.0026788578363143856\n",
      "0.007262110680547895\n",
      "0.004805616341046597\n",
      "0.002559570865966284\n",
      "0.026026473862287198\n",
      "0.0027936453712886482\n",
      "0.028376310608833583\n",
      "0.015655403525204774\n",
      "0.005049450469682138\n",
      "0.0027936453712886482\n",
      "0.011415704891636074\n",
      "0.0023391479253951812\n",
      "0.014721532858209473\n",
      "0.007681448777509037\n",
      "0.004262164546867016\n",
      "0.0023391479253951812\n",
      "0.038192825695921834\n",
      "0.00622500801069203\n",
      "0.005353492673776716\n",
      "0.005423012120238312\n",
      "0.0053412618528866675\n",
      "0.0053412618528866675\n",
      "0.05789152268388692\n",
      "0.009728839099229615\n",
      "0.004094811528941735\n",
      "0.0069668042168701365\n",
      "0.00869149813850653\n",
      "0.004094811528941735\n",
      "0.0051653684077503734\n",
      "0.019540502109165008\n",
      "0.0036743796940610224\n",
      "0.0026042846451458414\n",
      "0.006313812023375514\n",
      "0.0026042846451458414\n",
      "0.007634211329137619\n",
      "0.00422427363849539\n",
      "0.0020455759268566964\n",
      "0.023740151255528794\n",
      "0.006816771615713645\n",
      "0.0020455759268566964\n",
      "0.04818461874081374\n",
      "0.005669478991773413\n",
      "0.00937415698009\n",
      "0.0118750817896847\n",
      "0.004329809715014639\n",
      "0.004329809715014639\n",
      "0.006519814488790192\n",
      "0.004754134820196376\n",
      "0.012644187173019851\n",
      "0.003435399375944852\n",
      "0.0031842723695863777\n",
      "0.0031842723695863777\n",
      "0.004395816106493361\n",
      "0.0038589466864908063\n",
      "0.005842190487298543\n",
      "0.014054859497119826\n",
      "0.008040552408265605\n",
      "0.0038589466864908063\n",
      "0.005014283297601897\n",
      "0.01010176121542482\n",
      "0.0025224772163867605\n",
      "0.004693586205538194\n",
      "0.005091681372755352\n",
      "0.0025224772163867605\n",
      "0.004130425527654557\n",
      "0.010905999781686847\n",
      "0.003878520819487647\n",
      "0.002579085292239046\n",
      "0.03633256557317431\n",
      "0.002579085292239046\n",
      "0.003081833459773183\n",
      "0.002606157647514553\n",
      "0.004619346561854721\n",
      "0.005208419524678982\n",
      "0.0015768427554202949\n",
      "0.0015768427554202949\n",
      "0.00338973427699261\n",
      "0.002458029077384567\n",
      "0.0024746743866274935\n",
      "0.00528265675797774\n",
      "0.0038806086028305756\n",
      "0.002458029077384567\n",
      "0.015548208539612054\n",
      "0.01626489914090619\n",
      "0.004753194510054342\n",
      "0.003623988079249894\n",
      "0.008747568463251826\n",
      "0.003623988079249894\n",
      "0.023343905915546827\n",
      "0.0074900914145361285\n",
      "0.020831693547413443\n",
      "0.015240063096052693\n",
      "0.019093378070413343\n",
      "0.0074900914145361285\n",
      "0.004611201391738104\n",
      "0.0010590800003160337\n",
      "0.01102894959046618\n",
      "0.002678437516457526\n",
      "0.003841513112139591\n",
      "0.0010590800003160337\n",
      "0.006237048191251946\n",
      "0.022067461265039123\n",
      "0.004215792190972449\n",
      "0.0030621907209044106\n",
      "0.002245060727645663\n",
      "0.002245060727645663\n",
      "0.007313940868165546\n",
      "0.003925213490453747\n",
      "0.0021430505356801344\n",
      "0.005443686633215355\n",
      "0.003471407354123387\n",
      "0.0021430505356801344\n",
      "0.0020537307048483734\n",
      "0.015287486667129997\n",
      "0.0035035035516552725\n",
      "0.004945776093857602\n",
      "0.005230743438846587\n",
      "0.0020537307048483734\n",
      "0.015379016799746232\n",
      "0.0055123115454989\n",
      "0.004061178832509584\n",
      "0.012709514682376268\n",
      "0.0062415871124002625\n",
      "0.004061178832509584\n",
      "0.009336417866061599\n",
      "0.008260928703588504\n",
      "0.0033246583561111257\n",
      "0.040765377807860784\n",
      "0.0027921697212570395\n",
      "0.0027921697212570395\n",
      "0.005744412299268353\n",
      "0.014909348482714566\n",
      "0.006584450080060237\n",
      "0.0030666296227134134\n",
      "0.011054099287122613\n",
      "0.0030666296227134134\n",
      "0.0021227254554736135\n",
      "0.0030411339506658534\n",
      "0.009772429554076721\n",
      "0.009889141526343468\n",
      "0.004582349615381816\n",
      "0.0021227254554736135\n",
      "0.008767625270038111\n",
      "0.00503666110072629\n",
      "0.0242822851951094\n",
      "0.01297449087849749\n",
      "0.0027181441193019833\n",
      "0.0027181441193019833\n",
      "0.00491917971446354\n",
      "0.00467026973308447\n",
      "0.0055722729492272095\n",
      "0.004925834766235823\n",
      "0.010681891328447218\n",
      "0.00467026973308447\n",
      "0.0044665442317340575\n",
      "0.00634065350535224\n",
      "0.006496295905947129\n",
      "0.008801695818819098\n",
      "0.004149198800814481\n",
      "0.004149198800814481\n",
      "0.018861225467727616\n",
      "0.00382251640487485\n",
      "0.018812796171218605\n",
      "0.018743968173513665\n",
      "0.0024049889624702973\n",
      "0.0024049889624702973\n",
      "0.0028024020528798067\n",
      "0.009524886773605726\n",
      "0.0041131532540174525\n",
      "0.004847738180727789\n",
      "0.014276407446998714\n",
      "0.0028024020528798067\n",
      "0.009771700955656987\n",
      "0.005015522652525842\n",
      "0.007503513221748798\n",
      "0.006717027731552192\n",
      "0.023310680782603208\n",
      "0.005015522652525842\n",
      "0.005224202335053158\n",
      "0.006261271959937681\n",
      "0.004322209854277635\n",
      "0.024075264623348785\n",
      "0.011825524889880238\n",
      "0.004322209854277635\n",
      "0.004674502948975101\n",
      "0.0021043007832363307\n",
      "0.0041046067771992965\n",
      "0.0044597559503360956\n",
      "0.004451229227704189\n",
      "0.0021043007832363307\n",
      "0.010030272091999175\n",
      "0.0028269352008480223\n",
      "0.005146196144208176\n",
      "0.0037230864601065796\n",
      "0.01220993502802595\n",
      "0.0028269352008480223\n",
      "0.03796351764673878\n",
      "0.0037848771880470465\n",
      "0.008770714170649169\n",
      "0.003538442465706379\n",
      "0.004039261249083389\n",
      "0.003538442465706379\n",
      "0.004572837269661298\n",
      "0.00685352030716526\n",
      "0.007600128578533527\n",
      "0.004660780057144039\n",
      "0.0033629346021328485\n",
      "0.0033629346021328485\n",
      "0.0037516975017273936\n",
      "0.022576314671766674\n",
      "0.0040356730915578265\n",
      "0.0023754529720478638\n",
      "0.002154789167743368\n",
      "0.002154789167743368\n",
      "0.007399586265211567\n",
      "0.003790305526546687\n",
      "0.04685855466302976\n",
      "0.0029816093024434285\n",
      "0.023332731449626092\n",
      "0.0029816093024434285\n",
      "0.011562610908423159\n",
      "0.007858604577459308\n",
      "0.006460428766359252\n",
      "0.006952659664748345\n",
      "0.003973110218746819\n",
      "0.003973110218746819\n",
      "0.02257290022832069\n",
      "0.01142077383158197\n",
      "0.01177496617919357\n",
      "0.006938302233914435\n",
      "0.026181294988433863\n",
      "0.006938302233914435\n",
      "0.011032609255885248\n",
      "0.00941528931892405\n",
      "0.002887170511929218\n",
      "0.006647963204165066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004753749537346727\n",
      "0.002887170511929218\n",
      "0.007881753946106991\n",
      "0.005207791570875599\n",
      "0.05164276522285209\n",
      "0.0038681623713812956\n",
      "0.004085083987557311\n",
      "0.0038681623713812956\n",
      "0.01186363530044885\n",
      "0.0053713781490974215\n",
      "0.0098380873367515\n",
      "0.005922159863779712\n",
      "0.011840111517116805\n",
      "0.0053713781490974215\n",
      "0.010337372176659122\n",
      "0.020971143570840508\n",
      "0.003828787306598883\n",
      "0.005896492610665679\n",
      "0.0036792329592521753\n",
      "0.0036792329592521753\n",
      "0.013137696492450026\n",
      "0.0024728091825069955\n",
      "0.009748022672804231\n",
      "0.0023534659160687295\n",
      "0.0052482782810688786\n",
      "0.0023534659160687295\n",
      "0.027419427817633103\n",
      "0.00979593219857143\n",
      "0.004887591342953031\n",
      "0.007039283384802371\n",
      "0.012345935085711935\n",
      "0.004887591342953031\n",
      "0.03847669012379103\n",
      "0.002082247654161658\n",
      "0.0020148063009390728\n",
      "0.009209422614880176\n",
      "0.007793149282337102\n",
      "0.0020148063009390728\n",
      "0.016679144926200356\n",
      "0.008981850205743611\n",
      "0.004738173785985007\n",
      "0.005651547308169255\n",
      "0.0050331022547112705\n",
      "0.004738173785985007\n",
      "0.005903040574497961\n",
      "0.012608864605106448\n",
      "0.005819249696614109\n",
      "0.014297382422014056\n",
      "0.007442641818489142\n",
      "0.005819249696614109\n",
      "0.008568367565957772\n",
      "0.018918130697645492\n",
      "0.004602228789020789\n",
      "0.0062285707080149124\n",
      "0.008655178937347489\n",
      "0.004602228789020789\n",
      "0.005110016990935179\n",
      "0.004001524863178555\n",
      "0.024431246759647033\n",
      "0.002018255294876837\n",
      "0.00209002178777683\n",
      "0.002018255294876837\n",
      "0.014958112991678223\n",
      "0.01870084133485888\n",
      "0.004854793879522401\n",
      "0.005037007024375232\n",
      "0.004800554144834865\n",
      "0.004800554144834865\n",
      "0.0059407026619572225\n",
      "0.003422946673538501\n",
      "0.005630268830947056\n",
      "0.007919311374457866\n",
      "0.004161383459661222\n",
      "0.003422946673538501\n",
      "0.0027566090807822573\n",
      "0.0017128928683019036\n",
      "0.0046666436084909875\n",
      "0.003881091782019304\n",
      "0.01105645824212677\n",
      "0.0017128928683019036\n",
      "0.004175867788440139\n",
      "0.0046972321001131105\n",
      "0.010504061929029073\n",
      "0.0030874324020682634\n",
      "0.0028716900884420533\n",
      "0.0028716900884420533\n",
      "0.003886123392000436\n",
      "0.013357917921393273\n",
      "0.004721575280789351\n",
      "0.00278111079535395\n",
      "0.006829164907161109\n",
      "0.00278111079535395\n",
      "0.004897657912804337\n",
      "0.003853544014358824\n",
      "0.007926327781083604\n",
      "0.0032702771048064815\n",
      "0.005055147528248444\n",
      "0.0032702771048064815\n",
      "0.0031900633831768517\n",
      "0.007773649971191994\n",
      "0.012886298702492721\n",
      "0.0019388916380823104\n",
      "0.005460610192159898\n",
      "0.0019388916380823104\n",
      "0.0037655063616700278\n",
      "0.047617132378428045\n",
      "0.002093452748290091\n",
      "0.0046213661255141494\n",
      "0.0025818185333576094\n",
      "0.002093452748290091\n",
      "0.005596354279604949\n",
      "0.0029089356045046153\n",
      "0.008916932717518039\n",
      "0.00911870113610956\n",
      "0.007990676147293253\n",
      "0.0029089356045046153\n",
      "0.026046693866666828\n",
      "0.006616512680402788\n",
      "0.011718900839529943\n",
      "0.0038407965480635052\n",
      "0.02463576091075445\n",
      "0.0038407965480635052\n",
      "0.005259882696590691\n",
      "0.005919700804152265\n",
      "0.005558468597854674\n",
      "0.0019389050621686275\n",
      "0.016220591568951143\n",
      "0.0019389050621686275\n",
      "0.006172741128689229\n",
      "0.0031281165943006293\n",
      "0.022660700531922276\n",
      "0.005077939751047326\n",
      "0.00585829227867985\n",
      "0.0031281165943006293\n",
      "0.07893661826162668\n",
      "0.012215256017638818\n",
      "0.010134007150124066\n",
      "0.003349017042817019\n",
      "0.0033153364984429335\n",
      "0.0033153364984429335\n",
      "0.002245837823958879\n",
      "0.011081952815357838\n",
      "0.012944308131107364\n",
      "0.003359944303651148\n",
      "0.011169540638290546\n",
      "0.002245837823958879\n",
      "0.005226503243302722\n",
      "0.011013943420725952\n",
      "0.009356143035025029\n",
      "0.0056133716703234285\n",
      "0.008771164898606658\n",
      "0.005226503243302722\n",
      "0.00327702045440764\n",
      "0.0038169638723720787\n",
      "0.005320716793929069\n",
      "0.007315205207607459\n",
      "0.004884491802083835\n",
      "0.00327702045440764\n",
      "0.004645680522546399\n",
      "0.0027249760838916203\n",
      "0.00327495381843974\n",
      "0.003102866915137751\n",
      "0.005522163564618442\n",
      "0.0027249760838916203\n",
      "0.038424145764406434\n",
      "0.005043788239591454\n",
      "0.0053635359436532\n",
      "0.005030759175599727\n",
      "0.003613015382072062\n",
      "0.003613015382072062\n",
      "0.011791694773623344\n",
      "0.007545378683520836\n",
      "0.0071374482788137406\n",
      "0.004189245255000917\n",
      "0.004069490214430004\n",
      "0.004069490214430004\n",
      "0.008904509968949273\n",
      "0.02269058605604774\n",
      "0.03796790556573646\n",
      "0.008391538288219936\n",
      "0.003574765803634579\n",
      "0.003574765803634579\n",
      "0.0033766732715453263\n",
      "0.003618457434063904\n",
      "0.003269418181062141\n",
      "0.016588340609237206\n",
      "0.018498905787379654\n",
      "0.003269418181062141\n",
      "0.0019404053326111418\n",
      "0.025912503197589695\n",
      "0.003442348478629376\n",
      "0.022464921907853454\n",
      "0.0067703499267826334\n",
      "0.0019404053326111418\n",
      "0.04166503823539094\n",
      "0.006988729010914674\n",
      "0.0020046201476242535\n",
      "0.008013523268437043\n",
      "0.006516688473984052\n",
      "0.0020046201476242535\n",
      "0.006971034810317548\n",
      "0.010785279148057334\n",
      "0.0016725192941974185\n",
      "0.004648143444933369\n",
      "0.0015790591814580012\n",
      "0.0015790591814580012\n",
      "0.0218797000834863\n",
      "0.007377707615645397\n",
      "0.004688990163328016\n",
      "0.009909718161812199\n",
      "0.004427107654444695\n",
      "0.004427107654444695\n"
     ]
    }
   ],
   "source": [
    "cps = np.linspace(1e-7, 1e-5, 100)\n",
    "\n",
    "model = train_multi_trees(\n",
    "    alpha=cps\n",
    "    ,n=5\n",
    "    ,train_data=train_set_trim\n",
    "    ,test_data=test_set_trim\n",
    "    ,test_num=9\n",
    "    ,log_mode=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 8, leaves: 38\n"
     ]
    }
   ],
   "source": [
    "print2(f'Depth: {model.get_depth()}, leaves: {model.get_n_leaves()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas symulacji scenariusza 0: 9.914002180099487s\n",
      "0.004893233593612499\n",
      "Czas symulacji scenariusza 1: 28.378997087478638s\n",
      "0.0014849385808950366\n",
      "Czas symulacji scenariusza 2: 18.93700623512268s\n",
      "0.005246120928026027\n",
      "Czas symulacji scenariusza 3: 15.054997205734253s\n",
      "0.0003061177155920551\n",
      "Czas symulacji scenariusza 4: 17.220006227493286s\n",
      "0.0015997624134234218\n",
      "Czas symulacji scenariusza 5: 3.405001163482666s\n",
      "0.0012963516336210658\n",
      "Czas symulacji scenariusza 6: 8.733006238937378s\n",
      "0.004823799677436235\n",
      "Czas symulacji scenariusza 7: 3.5510013103485107s\n",
      "0.0005521408321462678\n",
      "Czas symulacji scenariusza 8: 8.313000440597534s\n",
      "0.0034717065334083693\n",
      "Czas symulacji scenariusza 9: 4.118005990982056s\n",
      "0.0010590800003160337\n",
      "Czas symulacji scenariusza 10: 5.04052209854126s\n",
      "0.009208143181497097\n",
      "Czas symulacji scenariusza 11: 2.9059994220733643s\n",
      "0.0027741192631227825\n",
      "Czas symulacji scenariusza 12: 1.338001012802124s\n",
      "0.0018306795963184078\n",
      "Czas symulacji scenariusza 13: 4.295001268386841s\n",
      "0.002228567490287405\n",
      "Czas symulacji scenariusza 14: 23.102997541427612s\n",
      "0.007204037458381544\n",
      "Czas symulacji scenariusza 15: 2.0900042057037354s\n",
      "0.002935667832723379\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for col in range(0, len(test_set_trim)):\n",
    "    plt.figure(col)\n",
    "    t_start = time.time()\n",
    "    y = sim(model, test_set_trim, col)\n",
    "    t_stop = time.time()\n",
    "    print2(f'Czas symulacji scenariusza {col}: {t_stop-t_start}s')\n",
    "    eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trees_models\\\\tree_cp4.2e-6_2020-5-27.pkl','wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trees_models\\\\tree_cp4.2e-6_2020-5-27.pkl','rb') as f:\n",
    "    model = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06309953910047268\n",
      "0.048704149745492496\n",
      "0.04826057203403253\n",
      "0.05353649709204792\n",
      "0.02748864185243514\n",
      "0.006081727212290804\n",
      "0.019633602977133357\n",
      "0.021697152902237175\n",
      "0.004874003846854947\n",
      "0.007015120569452342\n",
      "0.024147575774831568\n",
      "0.0067184894667324455\n",
      "0.002251998552104843\n",
      "0.003273436194355749\n",
      "0.005811203839737425\n",
      "0.005157509316469695\n",
      "0.007764434624665225\n",
      "0.0034213891099879323\n",
      "0.006787385002662386\n",
      "0.005755020750193826\n",
      "0.005525834056922648\n",
      "0.002098732290719803\n",
      "0.002883489639877885\n",
      "0.002698280984359089\n",
      "0.0033812795334863936\n",
      "0.02382397694429892\n",
      "0.01919919083442448\n",
      "0.03304921925024829\n",
      "0.032730855356478496\n",
      "0.025374847660900108\n",
      "0.0041111675592121755\n",
      "0.0156664242884168\n",
      "0.007405002753793891\n",
      "0.010137953538204417\n",
      "0.01243273663248223\n",
      "0.0037913415956511887\n",
      "0.0035474511988387906\n",
      "0.0037427869719932377\n",
      "0.0049634542989556996\n",
      "0.0040540672306148585\n",
      "0.003724754940326429\n",
      "0.002690671665862777\n",
      "0.003913219078685459\n",
      "0.0030792171142844807\n",
      "0.004253210409888791\n",
      "0.002379267307836897\n",
      "0.002300101818188423\n",
      "0.0025413323227470907\n",
      "0.00272426047738903\n",
      "0.0027185546718045458\n",
      "0.02779139519233998\n",
      "0.02301706342091818\n",
      "0.02606291209603998\n",
      "0.027418874518478038\n",
      "0.017354472677650806\n",
      "0.007709794342986071\n",
      "0.005201982640466642\n",
      "0.006298352641491482\n",
      "0.006944171549404821\n",
      "0.004586879272951011\n",
      "0.003444591117080443\n",
      "0.005081890976143779\n",
      "0.0032668912976868913\n",
      "0.003924452281784783\n",
      "0.003097989467373981\n",
      "0.0025091841557800853\n",
      "0.003503337328146713\n",
      "0.002004432146015521\n",
      "0.00210945035420014\n",
      "0.0023753803377153096\n",
      "0.002430677879665916\n",
      "0.0017779921015690218\n",
      "0.0021529750814305052\n",
      "0.0020361517412800343\n",
      "0.0020792874979795946\n",
      "0.02324591994200254\n",
      "0.029908866661979637\n",
      "0.021229278232361336\n",
      "0.026501759661025526\n",
      "0.023328456690993604\n",
      "0.004762991056458956\n",
      "0.005829569678026694\n",
      "0.006019981865378039\n",
      "0.004779685578588212\n",
      "0.005448124267134416\n",
      "0.0032006425086076253\n",
      "0.003132493297872478\n",
      "0.0039653067910093275\n",
      "0.0036430114220482603\n",
      "0.004054300344937091\n",
      "0.0027923544520910416\n",
      "0.002403847843093755\n",
      "0.002989389282969658\n",
      "0.002240474656630326\n",
      "0.0027269728996024084\n",
      "0.00219135209963064\n",
      "0.002074726838567039\n",
      "0.0018752331496343676\n",
      "0.0020448025100690027\n",
      "0.0019365964764806883\n",
      "0.022396507162454986\n",
      "0.02601066095767509\n",
      "0.024565658677856043\n",
      "0.023830247272360227\n",
      "0.025389815002545276\n",
      "0.005854603924488584\n",
      "0.008744452637263651\n",
      "0.005698808845847098\n",
      "0.004427830722785359\n",
      "0.0040907690235378645\n",
      "0.0037073328176474296\n",
      "0.00407221239453004\n",
      "0.0037231492522224884\n",
      "0.0029488928324842623\n",
      "0.005107358247545225\n",
      "0.002382400412674434\n",
      "0.002508530664803293\n",
      "0.00385944617330438\n",
      "0.0026622204320189678\n",
      "0.0029081983287922697\n",
      "0.002112446640932203\n",
      "0.0021733736830490524\n",
      "0.0021453987831025127\n",
      "0.0023681841453912613\n",
      "0.001984628337780075\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log = datetime.now().strftime('ada_logs\\\\adaBoost_%Y_%m_%d_%H_%M_%S.log')\n",
    "\n",
    "model = train_multi_adaBoost(\n",
    "    trees=[5, 20, 50, 100, 150]\n",
    "    ,lr=[0.65]\n",
    "    ,depth=[1, 2, 3, 4, 5]\n",
    "    ,n=5\n",
    "    ,train_data=train_set_trim\n",
    "    ,test_data=test_set_trim\n",
    "    ,test_num=9\n",
    "    ,log_name=log\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth: 5, leaves: 17\n",
      "Depth: 5, leaves: 17\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 21\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 27\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 18\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 24\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 24\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 23\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 21\n",
      "Depth: 5, leaves: 22\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 20\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 27\n",
      "Depth: 5, leaves: 24\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 25\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 23\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 21\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 28\n",
      "Depth: 5, leaves: 29\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 31\n",
      "Depth: 5, leaves: 22\n",
      "Depth: 5, leaves: 30\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 32\n",
      "Depth: 5, leaves: 21\n"
     ]
    }
   ],
   "source": [
    "for tree in model:\n",
    "    print2(f'Depth: {tree.get_depth()}, leaves: {tree.get_n_leaves()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas symulacji scenariusza 0: 409.4590632915497s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:64: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:75: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00031762635453650687\n",
      "Czas symulacji scenariusza 1: 1182.9121596813202s\n",
      "0.0003945863355087147\n",
      "Czas symulacji scenariusza 2: 1090.4695074558258s\n",
      "0.00012267379436371685\n",
      "Czas symulacji scenariusza 3: 1372.7531218528748s\n",
      "0.001905886168150061\n",
      "Czas symulacji scenariusza 4: 1099.9024262428284s\n",
      "0.002718642336562959\n",
      "Czas symulacji scenariusza 5: 195.56273770332336s\n",
      "0.0005587790382683142\n",
      "Czas symulacji scenariusza 6: 505.54857540130615s\n",
      "0.0038043527176668548\n",
      "Czas symulacji scenariusza 7: 221.68070340156555s\n",
      "0.0002894502338433314\n",
      "Czas symulacji scenariusza 8: 451.8817801475525s\n",
      "0.0005769781722042446\n",
      "Czas symulacji scenariusza 9: 232.3356306552887s\n",
      "0.001544286201663375\n",
      "Czas symulacji scenariusza 10: 207.60454535484314s\n",
      "0.002345241279211729\n",
      "Czas symulacji scenariusza 11: 96.1155195236206s\n",
      "0.0022235424990415453\n",
      "Czas symulacji scenariusza 12: 51.185176372528076s\n",
      "0.002352380437416914\n",
      "Czas symulacji scenariusza 13: 167.36398720741272s\n",
      "0.00037590516723865153\n",
      "Czas symulacji scenariusza 14: 916.1893835067749s\n",
      "0.00095299112430425\n",
      "Czas symulacji scenariusza 15: 108.17241215705872s\n",
      "0.0016589473550084313\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for col in range(0, len(test_set_trim)):\n",
    "    plt.figure(col)\n",
    "    t_start = time.time()\n",
    "    y = sim(model, test_set_trim, col)\n",
    "    t_stop = time.time()\n",
    "    print2(f'Czas symulacji scenariusza {col}: {t_stop-t_start}s')\n",
    "    eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ada_models\\\\ada_105_5_0.65_2020-10-14.pkl','wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 0.0158 - val_loss: 0.0095\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 0.0015 - val_loss: 0.0053\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 6.6819e-04 - val_loss: 0.0042\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 7us/step - loss: 4.9754e-04 - val_loss: 0.0037\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 3.8764e-04 - val_loss: 0.0031\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.9369e-04 - val_loss: 0.0023\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.1407e-04 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.4907e-04 - val_loss: 0.0012\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.0074e-04 - val_loss: 8.6793e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 6.6944e-05 - val_loss: 5.1968e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.4758e-05 - val_loss: 3.5560e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.0203e-05 - val_loss: 2.0995e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.0705e-05 - val_loss: 1.3080e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.4463e-05 - val_loss: 8.7507e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.0346e-05 - val_loss: 5.8656e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.6908e-06 - val_loss: 4.2026e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 5.9559e-06 - val_loss: 3.1941e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.7470e-06 - val_loss: 2.7239e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.9179e-06 - val_loss: 2.2535e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.2388e-06 - val_loss: 1.9902e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.7439e-06 - val_loss: 1.7966e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.3871e-06 - val_loss: 1.6220e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0360e-06 - val_loss: 1.3475e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.8144e-06 - val_loss: 1.1874e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.6403e-06 - val_loss: 9.8644e-06\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.4206e-06 - val_loss: 8.9984e-06\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.2223e-06 - val_loss: 7.5550e-06\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.2117e-06 - val_loss: 7.1966e-06\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.0848e-06 - val_loss: 6.2117e-06\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.0273e-06 - val_loss: 6.4501e-06\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 9.2262e-07 - val_loss: 5.0826e-06\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 9.3309e-07 - val_loss: 4.9050e-06\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 9.3866e-07 - val_loss: 4.7676e-06\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 8.3581e-07 - val_loss: 5.6804e-06\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.8228e-07 - val_loss: 5.0606e-06\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 9.3207e-07 - val_loss: 4.3545e-06\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 8.3756e-07 - val_loss: 4.4080e-06\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.6669e-07 - val_loss: 4.4967e-06\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 8.3503e-07 - val_loss: 4.1511e-06\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.2103e-07 - val_loss: 4.3489e-06\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 9.3413e-07 - val_loss: 4.8630e-06\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.7294e-07 - val_loss: 4.2255e-06\n",
      "Epoch 00042: early stopping\n",
      "0.0007323001718233062\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 0.0854 - val_loss: 0.0077\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 0.0016 - val_loss: 0.0049\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 9.7376e-04 - val_loss: 0.0040\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 7.5177e-04 - val_loss: 0.0038\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 6.6760e-04 - val_loss: 0.0037\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 6.1635e-04 - val_loss: 0.0035\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 5.6819e-04 - val_loss: 0.0033\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 5.1804e-04 - val_loss: 0.0031\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 4.6577e-04 - val_loss: 0.0028\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 4.1182e-04 - val_loss: 0.0026\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.5714e-04 - val_loss: 0.0023\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.0300e-04 - val_loss: 0.0021\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.5065e-04 - val_loss: 0.0018\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.0185e-04 - val_loss: 0.0015\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.5811e-04 - val_loss: 0.0012\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.2073e-04 - val_loss: 9.4883e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 9.0432e-05 - val_loss: 6.8487e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 6.6804e-05 - val_loss: 5.2617e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 4.8960e-05 - val_loss: 3.6182e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.6030e-05 - val_loss: 2.5027e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.6770e-05 - val_loss: 1.8113e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0204e-05 - val_loss: 1.2986e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.5634e-05 - val_loss: 9.9605e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.2455e-05 - val_loss: 6.9287e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.0193e-05 - val_loss: 6.0261e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 8.6731e-06 - val_loss: 5.1969e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 7.5668e-06 - val_loss: 4.5901e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 6.8221e-06 - val_loss: 4.1397e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 6.2276e-06 - val_loss: 3.7633e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 2s 9us/step - loss: 5.7829e-06 - val_loss: 3.7029e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 5.4310e-06 - val_loss: 3.6537e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 5.1474e-06 - val_loss: 3.3863e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 4.8795e-06 - val_loss: 3.3039e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.6403e-06 - val_loss: 3.0630e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.6041e-06 - val_loss: 3.0626e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.2542e-06 - val_loss: 2.7696e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.1379e-06 - val_loss: 2.5985e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 4.0058e-06 - val_loss: 2.4408e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.7344e-06 - val_loss: 2.2884e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.5738e-06 - val_loss: 2.2457e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.4362e-06 - val_loss: 2.0540e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 3.2886e-06 - val_loss: 1.8787e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.1188e-06 - val_loss: 1.8607e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 3.0519e-06 - val_loss: 1.8257e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.9386e-06 - val_loss: 1.7108e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 2.8840e-06 - val_loss: 1.5486e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.7960e-06 - val_loss: 1.4543e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.5304e-06 - val_loss: 1.3669e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.3809e-06 - val_loss: 1.8400e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.4038e-06 - val_loss: 1.2863e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.3106e-06 - val_loss: 1.1938e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.1521e-06 - val_loss: 1.1472e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.1030e-06 - val_loss: 1.1204e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 2s 10us/step - loss: 2.2026e-06 - val_loss: 1.0539e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.0241e-06 - val_loss: 1.4528e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.0739e-06 - val_loss: 1.0818e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.0152e-06 - val_loss: 1.1981e-05\n",
      "Epoch 00057: early stopping\n",
      "0.5786939048437328\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 0.0094 - val_loss: 0.0043\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.4317e-04 - val_loss: 0.0033\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.7678e-04 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.9516e-04 - val_loss: 0.0024\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.4387e-04 - val_loss: 0.0021\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.0797e-04 - val_loss: 0.0018\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.7903e-04 - val_loss: 0.0016\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.5387e-04 - val_loss: 0.0013\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.3155e-04 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.1182e-04 - val_loss: 0.0010\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 9.4785e-05 - val_loss: 9.2456e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 8.0216e-05 - val_loss: 8.3730e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 6.8309e-05 - val_loss: 7.3765e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.8516e-05 - val_loss: 7.0250e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.0530e-05 - val_loss: 6.6560e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.4114e-05 - val_loss: 6.5606e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.8930e-05 - val_loss: 6.2129e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.4716e-05 - val_loss: 6.0137e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.1381e-05 - val_loss: 5.7838e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.8604e-05 - val_loss: 5.7526e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.6265e-05 - val_loss: 5.4399e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.4224e-05 - val_loss: 5.4264e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.2673e-05 - val_loss: 5.1868e-04\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.1253e-05 - val_loss: 4.9063e-04\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.0141e-05 - val_loss: 4.7282e-04\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.9031e-05 - val_loss: 4.4268e-04\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.8055e-05 - val_loss: 4.2733e-04\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.7046e-05 - val_loss: 3.9999e-04\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.6103e-05 - val_loss: 3.7676e-04\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5311e-05 - val_loss: 3.5047e-04\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4280e-05 - val_loss: 3.2383e-04\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.3800e-05 - val_loss: 3.0432e-04\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.2536e-05 - val_loss: 2.8212e-04\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.1926e-05 - val_loss: 2.6232e-04\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.1170e-05 - val_loss: 2.3681e-04\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.0244e-05 - val_loss: 2.2370e-04\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 9.6080e-06 - val_loss: 2.0104e-04\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 9.0883e-06 - val_loss: 1.8947e-04\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 8.1941e-06 - val_loss: 1.7638e-04\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 7.9665e-06 - val_loss: 1.5592e-04\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 7.1772e-06 - val_loss: 1.4680e-04\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 1s 5us/step - loss: 6.9177e-06 - val_loss: 1.3642e-04\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 6.3169e-06 - val_loss: 1.2628e-04\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.8689e-06 - val_loss: 1.2132e-04\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 5.4840e-06 - val_loss: 1.1126e-04\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 5.0930e-06 - val_loss: 1.0400e-04\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 4.8965e-06 - val_loss: 9.8757e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 4.4509e-06 - val_loss: 9.4795e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.2816e-06 - val_loss: 8.9368e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.9802e-06 - val_loss: 8.3830e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.9785e-06 - val_loss: 7.8848e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.6776e-06 - val_loss: 7.8271e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.4672e-06 - val_loss: 7.0509e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.3057e-06 - val_loss: 6.7859e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.1176e-06 - val_loss: 6.6733e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.9740e-06 - val_loss: 6.2733e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.9651e-06 - val_loss: 6.0765e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.7178e-06 - val_loss: 5.8936e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.6928e-06 - val_loss: 5.6180e-05\n",
      "Epoch 60/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.5604e-06 - val_loss: 5.3792e-05\n",
      "Epoch 61/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.4454e-06 - val_loss: 5.1980e-05\n",
      "Epoch 62/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.6557e-06 - val_loss: 5.0731e-05\n",
      "Epoch 63/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.3148e-06 - val_loss: 4.9534e-05\n",
      "Epoch 64/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.4245e-06 - val_loss: 4.8913e-05\n",
      "Epoch 65/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0687e-06 - val_loss: 4.7184e-05\n",
      "Epoch 66/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.2249e-06 - val_loss: 4.6413e-05\n",
      "Epoch 67/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0791e-06 - val_loss: 4.4941e-05\n",
      "Epoch 68/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0517e-06 - val_loss: 4.3865e-05\n",
      "Epoch 69/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.1467e-06 - val_loss: 4.2634e-05\n",
      "Epoch 70/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.9121e-06 - val_loss: 4.0873e-05\n",
      "Epoch 71/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.9628e-06 - val_loss: 4.0704e-05\n",
      "Epoch 72/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.9787e-06 - val_loss: 3.9372e-05\n",
      "Epoch 73/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.8744e-06 - val_loss: 4.0799e-05\n",
      "Epoch 74/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 2.0402e-06 - val_loss: 3.8083e-05\n",
      "Epoch 75/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.8051e-06 - val_loss: 3.6873e-05\n",
      "Epoch 76/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.7901e-06 - val_loss: 3.6325e-05\n",
      "Epoch 77/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.7152e-06 - val_loss: 4.1208e-05\n",
      "Epoch 78/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.8349e-06 - val_loss: 3.5049e-05\n",
      "Epoch 79/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.7644e-06 - val_loss: 3.4027e-05\n",
      "Epoch 80/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.7029e-06 - val_loss: 3.5014e-05\n",
      "Epoch 81/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.7724e-06 - val_loss: 3.3635e-05\n",
      "Epoch 82/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.6100e-06 - val_loss: 3.2383e-05\n",
      "Epoch 83/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5853e-06 - val_loss: 3.2558e-05\n",
      "Epoch 84/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5993e-06 - val_loss: 3.1741e-05\n",
      "Epoch 85/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5672e-06 - val_loss: 3.1456e-05\n",
      "Epoch 86/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.8900e-06 - val_loss: 3.0681e-05\n",
      "Epoch 87/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4794e-06 - val_loss: 3.0773e-05\n",
      "Epoch 88/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5350e-06 - val_loss: 3.0540e-05\n",
      "Epoch 89/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.5984e-06 - val_loss: 3.1813e-05\n",
      "Epoch 90/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4909e-06 - val_loss: 2.9575e-05\n",
      "Epoch 91/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.4680e-06 - val_loss: 2.8882e-05\n",
      "Epoch 92/100\n",
      "229500/229500 [==============================] - 2s 7us/step - loss: 1.7490e-06 - val_loss: 2.7600e-05\n",
      "Epoch 93/100\n",
      "229500/229500 [==============================] - 2s 8us/step - loss: 1.4162e-06 - val_loss: 2.7835e-05\n",
      "Epoch 94/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4704e-06 - val_loss: 2.7284e-05\n",
      "Epoch 95/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4626e-06 - val_loss: 2.6783e-05\n",
      "Epoch 96/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.4513e-06 - val_loss: 2.6523e-05\n",
      "Epoch 97/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.3668e-06 - val_loss: 2.8537e-05\n",
      "Epoch 98/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.5527e-06 - val_loss: 2.5532e-05\n",
      "Epoch 99/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.3810e-06 - val_loss: 2.5487e-05\n",
      "Epoch 100/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.3436e-06 - val_loss: 2.8440e-05\n",
      "0.0002707371282367404\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 0.0514 - val_loss: 0.0025\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.1429e-04 - val_loss: 0.0017\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.9826e-04 - val_loss: 0.0015\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.4800e-04 - val_loss: 0.0013\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.1370e-04 - val_loss: 0.0012\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8901e-04 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6961e-04 - val_loss: 0.0010\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.5264e-04 - val_loss: 9.6307e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3688e-04 - val_loss: 8.9435e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2163e-04 - val_loss: 8.1958e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0708e-04 - val_loss: 7.5622e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 9.3165e-05 - val_loss: 6.9819e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.0307e-05 - val_loss: 6.4713e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.8586e-05 - val_loss: 5.9446e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.8309e-05 - val_loss: 5.5768e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.9489e-05 - val_loss: 5.2167e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.2133e-05 - val_loss: 4.9245e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.6185e-05 - val_loss: 4.6610e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.1323e-05 - val_loss: 4.4453e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.7335e-05 - val_loss: 4.2166e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 2.3950e-05 - val_loss: 3.9572e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.0895e-05 - val_loss: 3.6777e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 1.8165e-05 - val_loss: 3.3870e-04\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.5758e-05 - val_loss: 3.1046e-04\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3680e-05 - val_loss: 2.8641e-04\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1960e-05 - val_loss: 2.6204e-04\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0576e-05 - val_loss: 2.4247e-04\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 9.4419e-06 - val_loss: 2.2468e-04\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.5831e-06 - val_loss: 2.0966e-04\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.9120e-06 - val_loss: 1.9322e-04\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.2478e-06 - val_loss: 1.7762e-04\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.7810e-06 - val_loss: 1.6369e-04\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.3014e-06 - val_loss: 1.5420e-04\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.8998e-06 - val_loss: 1.3853e-04\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.5154e-06 - val_loss: 1.2550e-04\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.2112e-06 - val_loss: 1.1830e-04\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 4.9030e-06 - val_loss: 1.0509e-04\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.5242e-06 - val_loss: 9.7477e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.2891e-06 - val_loss: 8.4452e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.1733e-06 - val_loss: 7.0878e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.9384e-06 - val_loss: 5.9551e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.6048e-06 - val_loss: 5.4958e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.5091e-06 - val_loss: 4.8677e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.3871e-06 - val_loss: 4.2455e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.2509e-06 - val_loss: 3.6446e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.1566e-06 - val_loss: 3.3199e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.0118e-06 - val_loss: 3.2382e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.8219e-06 - val_loss: 2.7731e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.7864e-06 - val_loss: 2.3568e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.6605e-06 - val_loss: 2.2898e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.7024e-06 - val_loss: 2.0299e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.5194e-06 - val_loss: 2.1180e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.6201e-06 - val_loss: 2.0736e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3407e-06 - val_loss: 1.7867e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2917e-06 - val_loss: 1.8440e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3939e-06 - val_loss: 1.8268e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.1928e-06 - val_loss: 1.4518e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.2886e-06 - val_loss: 1.6694e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.2708e-06 - val_loss: 1.4600e-05\n",
      "Epoch 60/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.1207e-06 - val_loss: 1.5192e-05\n",
      "Epoch 00060: early stopping\n",
      "0.00012573077297576898\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 0.0238 - val_loss: 0.0046\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.8870e-04 - val_loss: 0.0021\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9991e-04 - val_loss: 0.0013\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0626e-04 - val_loss: 7.2187e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.7139e-05 - val_loss: 4.2459e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.7691e-05 - val_loss: 2.8479e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.7268e-05 - val_loss: 2.0016e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.1267e-05 - val_loss: 1.6412e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.7308e-05 - val_loss: 1.3689e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.4185e-05 - val_loss: 1.2099e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.1386e-05 - val_loss: 1.0089e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.8783e-05 - val_loss: 8.7986e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6363e-05 - val_loss: 7.4386e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4092e-05 - val_loss: 6.4082e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2005e-05 - val_loss: 5.6364e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0188e-05 - val_loss: 4.3077e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.6020e-06 - val_loss: 4.1932e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.2292e-06 - val_loss: 3.3155e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 6.1575e-06 - val_loss: 3.0480e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 5.3138e-06 - val_loss: 2.7028e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.5877e-06 - val_loss: 2.5079e-05\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.1023e-06 - val_loss: 2.3352e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.7050e-06 - val_loss: 2.2839e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.3946e-06 - val_loss: 2.3163e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.2047e-06 - val_loss: 2.3581e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.0107e-06 - val_loss: 2.3492e-05\n",
      "Epoch 00026: early stopping\n",
      "0.23979807653802782\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 0.0047 - val_loss: 0.0013\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3620e-04 - val_loss: 9.3487e-04\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4841e-04 - val_loss: 6.9986e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0007e-04 - val_loss: 5.3381e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.9822e-05 - val_loss: 3.8997e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.8926e-05 - val_loss: 2.8749e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.3978e-05 - val_loss: 1.9245e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3815e-05 - val_loss: 1.4107e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6980e-05 - val_loss: 1.0621e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2371e-05 - val_loss: 8.0631e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 9.4370e-06 - val_loss: 6.3486e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.3282e-06 - val_loss: 5.0072e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.9426e-06 - val_loss: 4.1208e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.9623e-06 - val_loss: 3.8241e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.1644e-06 - val_loss: 2.8471e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.6607e-06 - val_loss: 2.9117e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.3275e-06 - val_loss: 2.2169e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.1254e-06 - val_loss: 2.3398e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.8421e-06 - val_loss: 2.0677e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.6634e-06 - val_loss: 1.9597e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.6428e-06 - val_loss: 2.2953e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.4217e-06 - val_loss: 2.3569e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3921e-06 - val_loss: 2.1604e-05\n",
      "Epoch 00023: early stopping\n",
      "0.0011148291620087496\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 6us/step - loss: 0.0447 - val_loss: 0.0098\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0017 - val_loss: 0.0024\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.9854e-04 - val_loss: 0.0012\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2600e-04 - val_loss: 8.2084e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 8.6941e-05 - val_loss: 5.6021e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.1882e-05 - val_loss: 3.7302e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.3924e-05 - val_loss: 2.4210e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 3.1830e-05 - val_loss: 1.6374e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.4201e-05 - val_loss: 1.1865e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.9527e-05 - val_loss: 9.2207e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6694e-05 - val_loss: 8.2039e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4881e-05 - val_loss: 7.7244e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3568e-05 - val_loss: 7.3473e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2480e-05 - val_loss: 6.9173e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1508e-05 - val_loss: 6.6284e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0641e-05 - val_loss: 6.2711e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 9.7633e-06 - val_loss: 5.8477e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.9551e-06 - val_loss: 5.6182e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.1976e-06 - val_loss: 5.0942e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.5505e-06 - val_loss: 4.8652e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.8628e-06 - val_loss: 4.4405e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.3178e-06 - val_loss: 4.2943e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.8059e-06 - val_loss: 3.8747e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.2457e-06 - val_loss: 3.5959e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.8260e-06 - val_loss: 3.3397e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.3733e-06 - val_loss: 3.1445e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.9868e-06 - val_loss: 2.8591e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.6596e-06 - val_loss: 2.6971e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.2915e-06 - val_loss: 2.4430e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.9888e-06 - val_loss: 2.2551e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.7524e-06 - val_loss: 2.2009e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.4577e-06 - val_loss: 1.9134e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3514e-06 - val_loss: 1.8557e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.0837e-06 - val_loss: 1.9324e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9579e-06 - val_loss: 1.7977e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8681e-06 - val_loss: 1.5215e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.7106e-06 - val_loss: 1.5480e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6028e-06 - val_loss: 1.3588e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4573e-06 - val_loss: 1.3402e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.5369e-06 - val_loss: 1.2585e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4059e-06 - val_loss: 1.1613e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.3191e-06 - val_loss: 1.1303e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3178e-06 - val_loss: 1.0717e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3735e-06 - val_loss: 1.3127e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.2701e-06 - val_loss: 1.0675e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.2050e-06 - val_loss: 9.5549e-06\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.1690e-06 - val_loss: 1.0738e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2422e-06 - val_loss: 9.9687e-06\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1300e-06 - val_loss: 8.3097e-06\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 1.1024e-06 - val_loss: 8.5434e-06\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1317e-06 - val_loss: 9.9300e-06\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0545e-06 - val_loss: 8.5055e-06\n",
      "Epoch 00052: early stopping\n",
      "0.0956610541995736\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 0.0458 - val_loss: 0.0077\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0027 - val_loss: 0.0046\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0017 - val_loss: 0.0031\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0011 - val_loss: 0.0024\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.7420e-04 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.6560e-04 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.4351e-04 - val_loss: 0.0023\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.7178e-04 - val_loss: 0.0023\n",
      "Epoch 00008: early stopping\n",
      "0.012214015738790895\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 0.0652 - val_loss: 0.0072\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0013 - val_loss: 0.0018\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.5346e-04 - val_loss: 8.2148e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2772e-04 - val_loss: 7.1278e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.0870e-04 - val_loss: 6.7277e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9551e-04 - val_loss: 6.3736e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8327e-04 - val_loss: 6.0166e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.7158e-04 - val_loss: 5.6596e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6031e-04 - val_loss: 5.2949e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.4941e-04 - val_loss: 4.9384e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3879e-04 - val_loss: 4.5961e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2843e-04 - val_loss: 4.2511e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1847e-04 - val_loss: 3.9263e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0894e-04 - val_loss: 3.6235e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 9.9690e-05 - val_loss: 3.3526e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 9.0900e-05 - val_loss: 3.1138e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.2534e-05 - val_loss: 2.8566e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.4409e-05 - val_loss: 2.6424e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.6658e-05 - val_loss: 2.4066e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.9333e-05 - val_loss: 2.2138e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.2118e-05 - val_loss: 1.9689e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.5460e-05 - val_loss: 1.7668e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.9171e-05 - val_loss: 1.5508e-04\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.3305e-05 - val_loss: 1.3464e-04\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.7863e-05 - val_loss: 1.1517e-04\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3120e-05 - val_loss: 9.9426e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9079e-05 - val_loss: 8.6577e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.5636e-05 - val_loss: 7.1705e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2692e-05 - val_loss: 6.1474e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0427e-05 - val_loss: 5.2037e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 8.5408e-06 - val_loss: 4.6880e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.1438e-06 - val_loss: 3.9525e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 6.0928e-06 - val_loss: 3.7110e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.2887e-06 - val_loss: 3.3753e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 4.6131e-06 - val_loss: 3.0465e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.2270e-06 - val_loss: 2.8341e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.7855e-06 - val_loss: 2.6638e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.5431e-06 - val_loss: 2.6861e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.4448e-06 - val_loss: 2.4083e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.1428e-06 - val_loss: 2.4665e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.9574e-06 - val_loss: 2.3491e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.8561e-06 - val_loss: 2.1915e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.8487e-06 - val_loss: 2.1344e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.5841e-06 - val_loss: 2.0908e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.4737e-06 - val_loss: 1.9219e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.5055e-06 - val_loss: 2.0202e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2741e-06 - val_loss: 1.7901e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.1963e-06 - val_loss: 2.0819e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.1392e-06 - val_loss: 1.6443e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 5us/step - loss: 2.2501e-06 - val_loss: 1.6634e-05\n",
      "Epoch 51/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.0558e-06 - val_loss: 1.5529e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9108e-06 - val_loss: 1.9050e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8922e-06 - val_loss: 1.5492e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8300e-06 - val_loss: 1.4889e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9000e-06 - val_loss: 1.6089e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.9776e-06 - val_loss: 1.8575e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.7239e-06 - val_loss: 1.4367e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5692e-06 - val_loss: 1.6727e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.6117e-06 - val_loss: 1.4849e-05\n",
      "Epoch 60/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5802e-06 - val_loss: 1.2912e-05\n",
      "Epoch 61/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4335e-06 - val_loss: 1.2687e-05\n",
      "Epoch 62/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5086e-06 - val_loss: 1.3286e-05\n",
      "Epoch 63/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.3715e-06 - val_loss: 1.9428e-05\n",
      "Epoch 64/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5274e-06 - val_loss: 1.1768e-05\n",
      "Epoch 65/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3593e-06 - val_loss: 1.1533e-05\n",
      "Epoch 66/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4219e-06 - val_loss: 1.1709e-05\n",
      "Epoch 67/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4375e-06 - val_loss: 1.1445e-05\n",
      "Epoch 68/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2525e-06 - val_loss: 1.0890e-05\n",
      "Epoch 69/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2115e-06 - val_loss: 1.0542e-05\n",
      "Epoch 70/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2197e-06 - val_loss: 1.0606e-05\n",
      "Epoch 71/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2766e-06 - val_loss: 9.9696e-06\n",
      "Epoch 72/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2946e-06 - val_loss: 1.4134e-05\n",
      "Epoch 73/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1596e-06 - val_loss: 1.2154e-05\n",
      "Epoch 74/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.0838e-06 - val_loss: 1.2355e-05\n",
      "Epoch 00074: early stopping\n",
      "0.006558386763337863\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0105 - val_loss: 0.0040\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.6125e-04 - val_loss: 0.0023\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 5.0722e-04 - val_loss: 0.0018\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.9203e-04 - val_loss: 0.0016\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.0720e-04 - val_loss: 0.0014\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.3700e-04 - val_loss: 0.0011\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.7568e-04 - val_loss: 8.9044e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.2159e-04 - val_loss: 5.6124e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 7.6789e-05 - val_loss: 3.4755e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.3310e-05 - val_loss: 1.8102e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2217e-05 - val_loss: 8.7188e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.1156e-05 - val_loss: 3.8746e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 6.3823e-06 - val_loss: 2.5025e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 4.5092e-06 - val_loss: 2.1901e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.6945e-06 - val_loss: 2.1078e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 3.2189e-06 - val_loss: 2.0902e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.8768e-06 - val_loss: 1.9933e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.5286e-06 - val_loss: 1.9774e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2682e-06 - val_loss: 2.0117e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.0385e-06 - val_loss: 2.0137e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 1.8324e-06 - val_loss: 2.0109e-05\n",
      "Epoch 00021: early stopping\n",
      "0.00191411182676903\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0580 - val_loss: 0.0045\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.9461e-04 - val_loss: 0.0026\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.9534e-04 - val_loss: 0.0025\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4980e-04 - val_loss: 0.0023\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1221e-04 - val_loss: 0.0022\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7519e-04 - val_loss: 0.0020\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3846e-04 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0218e-04 - val_loss: 0.0015\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6684e-04 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3351e-04 - val_loss: 0.0010\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0295e-04 - val_loss: 7.5624e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.6286e-05 - val_loss: 5.7128e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4087e-05 - val_loss: 4.0139e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.6753e-05 - val_loss: 2.5297e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4002e-05 - val_loss: 1.5250e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5284e-05 - val_loss: 9.1902e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.6410e-06 - val_loss: 5.8184e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.2471e-06 - val_loss: 3.3181e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2920e-06 - val_loss: 2.5172e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2395e-06 - val_loss: 2.0364e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7036e-06 - val_loss: 1.9531e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4218e-06 - val_loss: 1.9737e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 2.2579e-06 - val_loss: 2.0264e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1449e-06 - val_loss: 2.0328e-05\n",
      "Epoch 00024: early stopping\n",
      "0.0013502897836926847\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0240 - val_loss: 0.0047\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.1145e-04 - val_loss: 0.0016\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3404e-04 - val_loss: 7.9602e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.0440e-05 - val_loss: 5.6472e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.9496e-05 - val_loss: 4.9274e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.0470e-05 - val_loss: 4.6196e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4877e-05 - val_loss: 4.5356e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0342e-05 - val_loss: 4.4222e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6397e-05 - val_loss: 4.3996e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2843e-05 - val_loss: 4.3428e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9610e-05 - val_loss: 4.2692e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6731e-05 - val_loss: 4.2082e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4167e-05 - val_loss: 4.1360e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1987e-05 - val_loss: 4.0548e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0126e-05 - val_loss: 3.9025e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.5585e-06 - val_loss: 3.8022e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.3583e-06 - val_loss: 3.6952e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.4473e-06 - val_loss: 3.6049e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.7746e-06 - val_loss: 3.4857e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.2739e-06 - val_loss: 3.4112e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.9506e-06 - val_loss: 3.2186e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.7147e-06 - val_loss: 3.1526e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.5018e-06 - val_loss: 3.0568e-04\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2915e-06 - val_loss: 2.8589e-04\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.1763e-06 - val_loss: 2.7053e-04\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.0682e-06 - val_loss: 2.5732e-04\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.8761e-06 - val_loss: 2.5284e-04\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.7747e-06 - val_loss: 2.2665e-04\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.7018e-06 - val_loss: 2.1386e-04\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4154e-06 - val_loss: 1.9335e-04\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4554e-06 - val_loss: 1.9101e-04\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.3209e-06 - val_loss: 1.6772e-04\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0783e-06 - val_loss: 1.5395e-04\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9875e-06 - val_loss: 1.4436e-04\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9836e-06 - val_loss: 1.4425e-04\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7340e-06 - val_loss: 1.2516e-04\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9411e-06 - val_loss: 1.1463e-04\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.8554e-06 - val_loss: 1.0968e-04\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3944e-06 - val_loss: 9.9935e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6289e-06 - val_loss: 9.5559e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3687e-06 - val_loss: 8.4165e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4633e-06 - val_loss: 7.7265e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0054e-06 - val_loss: 7.4092e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4698e-06 - val_loss: 7.0963e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0539e-06 - val_loss: 7.8071e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1660e-06 - val_loss: 6.6132e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8491e-06 - val_loss: 5.6665e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2996e-06 - val_loss: 5.3985e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6148e-06 - val_loss: 4.9999e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8969e-06 - val_loss: 4.8048e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8598e-06 - val_loss: 4.7536e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6631e-06 - val_loss: 4.1433e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6171e-06 - val_loss: 4.0138e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9977e-06 - val_loss: 3.8393e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3794e-06 - val_loss: 3.9449e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5713e-06 - val_loss: 3.6561e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6827e-06 - val_loss: 3.5153e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3100e-06 - val_loss: 3.2291e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6825e-06 - val_loss: 2.9353e-05\n",
      "Epoch 60/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2732e-06 - val_loss: 2.8433e-05\n",
      "Epoch 61/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6159e-06 - val_loss: 2.9402e-05\n",
      "Epoch 62/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4148e-06 - val_loss: 2.6222e-05\n",
      "Epoch 63/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5043e-06 - val_loss: 2.8846e-05\n",
      "Epoch 64/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2337e-06 - val_loss: 2.4160e-05\n",
      "Epoch 65/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5026e-06 - val_loss: 2.1947e-05\n",
      "Epoch 66/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1412e-06 - val_loss: 2.2694e-05\n",
      "Epoch 67/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3543e-06 - val_loss: 2.1076e-05\n",
      "Epoch 68/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5020e-06 - val_loss: 2.0041e-05\n",
      "Epoch 69/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3405e-06 - val_loss: 1.9520e-05\n",
      "Epoch 70/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3042e-06 - val_loss: 1.8988e-05\n",
      "Epoch 71/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.3085e-07 - val_loss: 2.1206e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4329e-06 - val_loss: 1.8633e-05\n",
      "Epoch 73/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0503e-06 - val_loss: 1.7920e-05\n",
      "Epoch 74/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2566e-06 - val_loss: 1.6460e-05\n",
      "Epoch 75/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4556e-06 - val_loss: 1.7213e-05\n",
      "Epoch 76/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.5747e-07 - val_loss: 1.7216e-05\n",
      "Epoch 77/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2446e-06 - val_loss: 1.4670e-05\n",
      "Epoch 78/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0400e-06 - val_loss: 1.6053e-05\n",
      "Epoch 79/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3223e-06 - val_loss: 1.3939e-05\n",
      "Epoch 80/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1071e-06 - val_loss: 1.3508e-05\n",
      "Epoch 81/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6317e-06 - val_loss: 1.3338e-05\n",
      "Epoch 82/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.9164e-07 - val_loss: 1.2767e-05\n",
      "Epoch 83/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0399e-06 - val_loss: 1.3387e-05\n",
      "Epoch 84/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1244e-06 - val_loss: 1.6749e-05\n",
      "Epoch 85/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1328e-06 - val_loss: 1.2525e-05\n",
      "Epoch 86/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5522e-06 - val_loss: 1.5697e-05\n",
      "Epoch 87/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.5714e-07 - val_loss: 1.1704e-05\n",
      "Epoch 88/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.5136e-07 - val_loss: 1.1822e-05\n",
      "Epoch 89/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.9885e-07 - val_loss: 1.1843e-05\n",
      "Epoch 90/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0976e-06 - val_loss: 1.0706e-05\n",
      "Epoch 91/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.9246e-07 - val_loss: 1.2696e-05\n",
      "Epoch 92/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1670e-06 - val_loss: 9.9784e-06\n",
      "Epoch 93/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0381e-06 - val_loss: 1.0565e-05\n",
      "Epoch 94/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.6437e-07 - val_loss: 1.0513e-05\n",
      "Epoch 95/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1111e-06 - val_loss: 9.7356e-06\n",
      "Epoch 96/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.5645e-07 - val_loss: 1.2477e-05\n",
      "Epoch 97/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2737e-06 - val_loss: 9.3296e-06\n",
      "Epoch 98/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.8192e-07 - val_loss: 9.0742e-06\n",
      "Epoch 99/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.8844e-07 - val_loss: 8.9847e-06\n",
      "Epoch 100/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.0735e-07 - val_loss: 9.4997e-06\n",
      "0.0008703173632270065\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0369 - val_loss: 0.0058\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.0212e-04 - val_loss: 0.0049\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.8281e-04 - val_loss: 0.0043\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4663e-04 - val_loss: 0.0035\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2420e-04 - val_loss: 0.0028\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1501e-04 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2501e-04 - val_loss: 0.0015\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5717e-04 - val_loss: 9.9867e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0856e-04 - val_loss: 6.1457e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.5700e-05 - val_loss: 4.0826e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3861e-05 - val_loss: 2.6263e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.9444e-05 - val_loss: 1.8380e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9744e-05 - val_loss: 1.2970e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2890e-05 - val_loss: 1.1025e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7894e-05 - val_loss: 9.5014e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3961e-05 - val_loss: 8.6519e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0996e-05 - val_loss: 8.0602e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.7265e-06 - val_loss: 7.5733e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.0113e-06 - val_loss: 7.1400e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.7269e-06 - val_loss: 7.0295e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.8118e-06 - val_loss: 6.6805e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.1235e-06 - val_loss: 6.3828e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.6401e-06 - val_loss: 6.2696e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2994e-06 - val_loss: 6.2608e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9909e-06 - val_loss: 6.1989e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7704e-06 - val_loss: 5.7557e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5747e-06 - val_loss: 5.6657e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4072e-06 - val_loss: 5.4460e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3934e-06 - val_loss: 5.2507e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1623e-06 - val_loss: 5.3067e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0891e-06 - val_loss: 4.8368e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9499e-06 - val_loss: 4.9671e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8549e-06 - val_loss: 4.7069e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8782e-06 - val_loss: 4.4424e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8176e-06 - val_loss: 4.4402e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7416e-06 - val_loss: 4.4642e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6639e-06 - val_loss: 4.5920e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7132e-06 - val_loss: 3.9087e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6142e-06 - val_loss: 3.6685e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6320e-06 - val_loss: 3.7323e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5451e-06 - val_loss: 3.4185e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6381e-06 - val_loss: 3.6497e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4921e-06 - val_loss: 3.2724e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5427e-06 - val_loss: 3.3245e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4229e-06 - val_loss: 3.0739e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4537e-06 - val_loss: 3.3878e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4218e-06 - val_loss: 2.9380e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3718e-06 - val_loss: 3.0606e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4127e-06 - val_loss: 2.5776e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3694e-06 - val_loss: 2.7451e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3310e-06 - val_loss: 2.6088e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4483e-06 - val_loss: 2.7046e-05\n",
      "Epoch 00052: early stopping\n",
      "0.0006060913484501897\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0127 - val_loss: 0.0015\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2553e-04 - val_loss: 6.5497e-04\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3852e-04 - val_loss: 5.6909e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.6005e-05 - val_loss: 4.6697e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.0047e-05 - val_loss: 3.8046e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.4006e-05 - val_loss: 2.8145e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.3252e-05 - val_loss: 2.3316e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5850e-05 - val_loss: 1.9319e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0698e-05 - val_loss: 1.7667e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6935e-05 - val_loss: 1.6856e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4140e-05 - val_loss: 1.6349e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1953e-05 - val_loss: 1.5959e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0216e-05 - val_loss: 1.5694e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.7997e-06 - val_loss: 1.5539e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.7333e-06 - val_loss: 1.5308e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.8731e-06 - val_loss: 1.4902e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.1665e-06 - val_loss: 1.4445e-04\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.6359e-06 - val_loss: 1.4119e-04\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.1174e-06 - val_loss: 1.3493e-04\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.7406e-06 - val_loss: 1.2955e-04\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.3234e-06 - val_loss: 1.2334e-04\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.9505e-06 - val_loss: 1.1350e-04\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.6205e-06 - val_loss: 1.0612e-04\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.3620e-06 - val_loss: 9.9737e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1350e-06 - val_loss: 9.3329e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9109e-06 - val_loss: 8.6176e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5972e-06 - val_loss: 7.9110e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4736e-06 - val_loss: 7.2016e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3813e-06 - val_loss: 6.7123e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2434e-06 - val_loss: 6.2503e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2340e-06 - val_loss: 5.8630e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1719e-06 - val_loss: 5.6107e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1867e-06 - val_loss: 5.2030e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9749e-06 - val_loss: 4.8659e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1137e-06 - val_loss: 4.6464e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8171e-06 - val_loss: 4.4606e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8101e-06 - val_loss: 4.2981e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8637e-06 - val_loss: 3.9809e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8253e-06 - val_loss: 4.2729e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9453e-06 - val_loss: 4.9283e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8857e-06 - val_loss: 3.8853e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5908e-06 - val_loss: 3.3789e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9057e-06 - val_loss: 3.3836e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8047e-06 - val_loss: 3.3363e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6755e-06 - val_loss: 3.2423e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6867e-06 - val_loss: 2.9115e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6597e-06 - val_loss: 3.2413e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7160e-06 - val_loss: 2.7624e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6718e-06 - val_loss: 2.7779e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6351e-06 - val_loss: 2.5021e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5020e-06 - val_loss: 2.6979e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6278e-06 - val_loss: 2.3861e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6066e-06 - val_loss: 2.5998e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6135e-06 - val_loss: 2.2607e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5678e-06 - val_loss: 2.1603e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4576e-06 - val_loss: 2.0680e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5461e-06 - val_loss: 1.9968e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6156e-06 - val_loss: 2.1794e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4438e-06 - val_loss: 1.9105e-05\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5904e-06 - val_loss: 1.9898e-05\n",
      "Epoch 61/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4265e-06 - val_loss: 1.9641e-05\n",
      "Epoch 62/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3856e-06 - val_loss: 2.4766e-05\n",
      "Epoch 00062: early stopping\n",
      "0.00056876883269788\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0075 - val_loss: 0.0022\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4300e-04 - val_loss: 0.0018\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0099e-04 - val_loss: 0.0014\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6462e-04 - val_loss: 0.0011\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3318e-04 - val_loss: 8.4207e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0713e-04 - val_loss: 6.2272e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.5816e-05 - val_loss: 5.0814e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.8428e-05 - val_loss: 3.8488e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4614e-05 - val_loss: 2.9402e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.3249e-05 - val_loss: 2.2676e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4392e-05 - val_loss: 1.7123e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7255e-05 - val_loss: 1.4582e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1781e-05 - val_loss: 1.1814e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7486e-05 - val_loss: 1.0473e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4309e-05 - val_loss: 9.3166e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1805e-05 - val_loss: 8.5019e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0002e-05 - val_loss: 8.0197e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.5981e-06 - val_loss: 7.6222e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.5157e-06 - val_loss: 7.5383e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.6795e-06 - val_loss: 7.5581e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.9117e-06 - val_loss: 7.6382e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3434e-06 - val_loss: 7.7620e-05\n",
      "Epoch 00022: early stopping\n",
      "0.001407798067150434\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0067 - val_loss: 7.2587e-04\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.3368e-05 - val_loss: 6.0930e-04\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4620e-05 - val_loss: 4.4864e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2559e-05 - val_loss: 3.4630e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.3111e-05 - val_loss: 2.5045e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5404e-05 - val_loss: 1.8040e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9350e-05 - val_loss: 1.3325e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4872e-05 - val_loss: 1.0496e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1798e-05 - val_loss: 8.2127e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.6927e-06 - val_loss: 6.8516e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.1949e-06 - val_loss: 6.1294e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.0386e-06 - val_loss: 5.5171e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.1221e-06 - val_loss: 4.9720e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3360e-06 - val_loss: 4.4759e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.6059e-06 - val_loss: 4.1059e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.0903e-06 - val_loss: 3.7192e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.6394e-06 - val_loss: 3.7107e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2670e-06 - val_loss: 3.4467e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9565e-06 - val_loss: 3.4007e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7533e-06 - val_loss: 3.1519e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5178e-06 - val_loss: 3.3986e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4525e-06 - val_loss: 2.9841e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2320e-06 - val_loss: 2.9872e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1807e-06 - val_loss: 2.9074e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9772e-06 - val_loss: 3.0739e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8662e-06 - val_loss: 2.8416e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8802e-06 - val_loss: 2.9112e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9086e-06 - val_loss: 2.6505e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7561e-06 - val_loss: 2.4464e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9200e-06 - val_loss: 2.4980e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7922e-06 - val_loss: 2.8230e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5652e-06 - val_loss: 2.7574e-05\n",
      "Epoch 00032: early stopping\n",
      "1.1289522596257164\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0090 - val_loss: 0.0016\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5707e-04 - val_loss: 0.0013\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9152e-04 - val_loss: 9.5386e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3368e-04 - val_loss: 6.3200e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.7285e-05 - val_loss: 3.7996e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4045e-05 - val_loss: 2.1483e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2700e-05 - val_loss: 1.2353e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0224e-05 - val_loss: 8.9918e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3443e-05 - val_loss: 6.8271e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.9150e-06 - val_loss: 5.9040e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.0240e-06 - val_loss: 5.2286e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.8137e-06 - val_loss: 4.8205e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.9780e-06 - val_loss: 4.3222e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.2712e-06 - val_loss: 3.9233e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.6862e-06 - val_loss: 3.3852e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.1523e-06 - val_loss: 3.0402e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.7116e-06 - val_loss: 2.6261e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2781e-06 - val_loss: 2.3265e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9114e-06 - val_loss: 2.0565e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5900e-06 - val_loss: 1.8061e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3204e-06 - val_loss: 1.5954e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0824e-06 - val_loss: 1.4388e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8597e-06 - val_loss: 1.3624e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7037e-06 - val_loss: 1.2904e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5900e-06 - val_loss: 1.2988e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5751e-06 - val_loss: 1.2067e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3320e-06 - val_loss: 1.1683e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3325e-06 - val_loss: 1.0933e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4624e-06 - val_loss: 1.1814e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4353e-06 - val_loss: 1.4937e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5735e-06 - val_loss: 1.1259e-05\n",
      "Epoch 00031: early stopping\n",
      "0.7908899735418574\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0094 - val_loss: 0.0056\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.2755e-04 - val_loss: 0.0037\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.9632e-04 - val_loss: 0.0023\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1953e-04 - val_loss: 0.0012\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0072e-04 - val_loss: 5.0709e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.0442e-05 - val_loss: 2.0076e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8734e-05 - val_loss: 1.0121e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1690e-05 - val_loss: 6.5772e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.9008e-06 - val_loss: 5.9930e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.5963e-06 - val_loss: 4.9093e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.8387e-06 - val_loss: 4.5199e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.3193e-06 - val_loss: 4.2249e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.8518e-06 - val_loss: 4.2763e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3959e-06 - val_loss: 4.0206e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.9660e-06 - val_loss: 4.0509e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.5639e-06 - val_loss: 4.0452e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2042e-06 - val_loss: 3.8653e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.8045e-06 - val_loss: 3.8461e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.5301e-06 - val_loss: 3.9398e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1308e-06 - val_loss: 4.0305e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0458e-06 - val_loss: 4.0661e-05\n",
      "Epoch 00021: early stopping\n",
      "0.0005704674295021663\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0045 - val_loss: 0.0031\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4649e-04 - val_loss: 0.0018\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4151e-04 - val_loss: 0.0010\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.1605e-05 - val_loss: 5.2772e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.7779e-05 - val_loss: 2.8043e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.8847e-05 - val_loss: 1.5971e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8366e-05 - val_loss: 1.1794e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2639e-05 - val_loss: 7.4507e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.2590e-06 - val_loss: 6.3554e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.1406e-06 - val_loss: 5.1152e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.7965e-06 - val_loss: 4.3099e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.8997e-06 - val_loss: 3.7347e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.1946e-06 - val_loss: 3.4849e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.7361e-06 - val_loss: 2.9377e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.3924e-06 - val_loss: 2.8328e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0320e-06 - val_loss: 2.5423e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6425e-06 - val_loss: 2.3378e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9074e-06 - val_loss: 2.1507e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5158e-06 - val_loss: 1.9930e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6842e-06 - val_loss: 1.9397e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6440e-06 - val_loss: 2.8520e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7086e-06 - val_loss: 2.0350e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0222e-06 - val_loss: 2.0755e-05\n",
      "Epoch 00023: early stopping\n",
      "0.0003382565596760914\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0310 - val_loss: 0.0028\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1321e-04 - val_loss: 0.0023\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4621e-04 - val_loss: 0.0018\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9442e-04 - val_loss: 0.0014\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5438e-04 - val_loss: 0.0011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2260e-04 - val_loss: 8.5078e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.7545e-05 - val_loss: 6.5921e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.7446e-05 - val_loss: 5.3689e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.1424e-05 - val_loss: 4.2661e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.8649e-05 - val_loss: 3.4106e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.8311e-05 - val_loss: 2.7866e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9912e-05 - val_loss: 2.1135e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3082e-05 - val_loss: 1.7651e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7605e-05 - val_loss: 1.4371e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3257e-05 - val_loss: 1.2306e-04\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 9.9097e-06 - val_loss: 1.0962e-04\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.4665e-06 - val_loss: 9.9138e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.6481e-06 - val_loss: 8.9258e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.4204e-06 - val_loss: 8.6303e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.5724e-06 - val_loss: 8.4079e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0388e-06 - val_loss: 8.2455e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6640e-06 - val_loss: 8.1134e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3672e-06 - val_loss: 8.0693e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1677e-06 - val_loss: 7.8795e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0753e-06 - val_loss: 7.7104e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0228e-06 - val_loss: 7.6113e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9159e-06 - val_loss: 7.3494e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8304e-06 - val_loss: 7.0246e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8135e-06 - val_loss: 7.0688e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7871e-06 - val_loss: 6.7007e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9471e-06 - val_loss: 6.6232e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7565e-06 - val_loss: 6.3828e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8628e-06 - val_loss: 6.2578e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0971e-06 - val_loss: 6.8062e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1966e-06 - val_loss: 6.0026e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3042e-06 - val_loss: 5.9722e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8164e-06 - val_loss: 6.1553e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0800e-06 - val_loss: 5.7576e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1971e-06 - val_loss: 5.6853e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9117e-06 - val_loss: 5.5203e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9713e-06 - val_loss: 5.9535e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8587e-06 - val_loss: 5.5434e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9860e-06 - val_loss: 5.2326e-05\n",
      "Epoch 44/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2566e-06 - val_loss: 5.3687e-05\n",
      "Epoch 45/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7118e-06 - val_loss: 5.0791e-05\n",
      "Epoch 46/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6608e-06 - val_loss: 4.9862e-05\n",
      "Epoch 47/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1173e-06 - val_loss: 5.1108e-05\n",
      "Epoch 48/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6407e-06 - val_loss: 5.6988e-05\n",
      "Epoch 49/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2664e-06 - val_loss: 4.8233e-05\n",
      "Epoch 50/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2171e-06 - val_loss: 4.7225e-05\n",
      "Epoch 51/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0826e-06 - val_loss: 4.6688e-05\n",
      "Epoch 52/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4090e-06 - val_loss: 5.0474e-05\n",
      "Epoch 53/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6817e-06 - val_loss: 4.9666e-05\n",
      "Epoch 54/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9049e-06 - val_loss: 4.5183e-05\n",
      "Epoch 55/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3261e-06 - val_loss: 4.4406e-05\n",
      "Epoch 56/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3962e-06 - val_loss: 4.6119e-05\n",
      "Epoch 57/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9298e-06 - val_loss: 4.5041e-05\n",
      "Epoch 58/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6196e-06 - val_loss: 4.3069e-05\n",
      "Epoch 59/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6849e-06 - val_loss: 4.1757e-05\n",
      "Epoch 60/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3127e-06 - val_loss: 4.0475e-05\n",
      "Epoch 61/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7934e-06 - val_loss: 4.2425e-05\n",
      "Epoch 62/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2951e-06 - val_loss: 4.4797e-05\n",
      "Epoch 63/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6050e-06 - val_loss: 3.9952e-05\n",
      "Epoch 64/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5505e-06 - val_loss: 4.2469e-05\n",
      "Epoch 65/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4951e-06 - val_loss: 3.7989e-05\n",
      "Epoch 66/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3157e-06 - val_loss: 4.2263e-05\n",
      "Epoch 67/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3769e-06 - val_loss: 3.8027e-05\n",
      "Epoch 68/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4919e-06 - val_loss: 3.7882e-05\n",
      "Epoch 69/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2135e-06 - val_loss: 3.6682e-05\n",
      "Epoch 70/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5602e-06 - val_loss: 3.6769e-05\n",
      "Epoch 71/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4823e-06 - val_loss: 3.6394e-05\n",
      "Epoch 72/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1880e-06 - val_loss: 3.4914e-05\n",
      "Epoch 73/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4460e-06 - val_loss: 4.0887e-05\n",
      "Epoch 74/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3968e-06 - val_loss: 3.7821e-05\n",
      "Epoch 75/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2819e-06 - val_loss: 3.4523e-05\n",
      "Epoch 76/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4252e-06 - val_loss: 3.3751e-05\n",
      "Epoch 77/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3233e-06 - val_loss: 3.2988e-05\n",
      "Epoch 78/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3134e-06 - val_loss: 3.3242e-05\n",
      "Epoch 79/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0653e-06 - val_loss: 3.3533e-05\n",
      "Epoch 80/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5647e-06 - val_loss: 3.3174e-05\n",
      "Epoch 00080: early stopping\n",
      "0.011680346238094991\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0130 - val_loss: 0.0014\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9832e-04 - val_loss: 0.0012\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9276e-04 - val_loss: 9.0329e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1932e-04 - val_loss: 5.7417e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.9202e-05 - val_loss: 3.3510e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.0223e-05 - val_loss: 1.9930e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5432e-05 - val_loss: 1.3459e-04\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8140e-05 - val_loss: 9.6826e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4249e-05 - val_loss: 7.6325e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.1828e-05 - val_loss: 6.1464e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0080e-05 - val_loss: 5.3093e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.7372e-06 - val_loss: 4.6458e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.6215e-06 - val_loss: 4.1106e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.7109e-06 - val_loss: 3.6527e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.9884e-06 - val_loss: 3.1739e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3183e-06 - val_loss: 2.8699e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.7237e-06 - val_loss: 2.5342e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2294e-06 - val_loss: 2.4694e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.8576e-06 - val_loss: 2.2616e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4994e-06 - val_loss: 2.1736e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1344e-06 - val_loss: 2.1342e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.9360e-06 - val_loss: 2.0931e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6305e-06 - val_loss: 2.1408e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4091e-06 - val_loss: 2.1352e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2021e-06 - val_loss: 2.1465e-05\n",
      "Epoch 00025: early stopping\n",
      "0.00024154309268451332\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0054 - val_loss: 0.0024\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8039e-04 - val_loss: 0.0015\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0440e-04 - val_loss: 8.3218e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.6422e-05 - val_loss: 4.3017e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.8663e-05 - val_loss: 2.0482e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4358e-05 - val_loss: 1.0477e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.8573e-06 - val_loss: 5.3926e-05\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.1884e-06 - val_loss: 3.4164e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.1119e-06 - val_loss: 2.4025e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4975e-06 - val_loss: 1.9307e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0812e-06 - val_loss: 1.5928e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7459e-06 - val_loss: 1.4481e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4070e-06 - val_loss: 1.1782e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1298e-06 - val_loss: 1.1940e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9367e-06 - val_loss: 9.2586e-06\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7083e-06 - val_loss: 9.7017e-06\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6272e-06 - val_loss: 8.3131e-06\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4622e-06 - val_loss: 8.6852e-06\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3378e-06 - val_loss: 8.3523e-06\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2702e-06 - val_loss: 9.4474e-06\n",
      "Epoch 00020: early stopping\n",
      "0.0002222104362326766\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0020 - val_loss: 6.5234e-04\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.9318e-05 - val_loss: 4.7804e-04\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4337e-05 - val_loss: 2.3235e-04\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8117e-05 - val_loss: 1.1474e-04\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0387e-05 - val_loss: 6.6466e-05\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.5088e-06 - val_loss: 4.5548e-05\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.4284e-06 - val_loss: 3.7218e-05\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2761e-06 - val_loss: 3.5322e-05\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5936e-06 - val_loss: 3.5366e-05\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1744e-06 - val_loss: 3.5554e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9345e-06 - val_loss: 3.5539e-05\n",
      "Epoch 00011: early stopping\n",
      "0.0008180551151002296\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0050 - val_loss: 0.0028\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1802e-04 - val_loss: 0.0022\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4527e-04 - val_loss: 0.0017\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7757e-04 - val_loss: 0.0012\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2215e-04 - val_loss: 8.2211e-04\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.1524e-05 - val_loss: 5.6442e-04\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.4224e-05 - val_loss: 3.3860e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.6756e-05 - val_loss: 2.0922e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5791e-05 - val_loss: 1.3515e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8649e-05 - val_loss: 9.0683e-05\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.3892e-05 - val_loss: 6.1828e-05\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0496e-05 - val_loss: 4.7098e-05\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.0376e-06 - val_loss: 3.7026e-05\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.3701e-06 - val_loss: 3.2253e-05\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.1175e-06 - val_loss: 3.0597e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.3491e-06 - val_loss: 2.9757e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.7729e-06 - val_loss: 2.9225e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4192e-06 - val_loss: 2.7525e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0942e-06 - val_loss: 2.6049e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7899e-06 - val_loss: 2.8233e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7075e-06 - val_loss: 2.3608e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3927e-06 - val_loss: 2.4044e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2915e-06 - val_loss: 2.1223e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2681e-06 - val_loss: 2.5364e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0655e-06 - val_loss: 2.4415e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1119e-06 - val_loss: 1.8293e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9801e-06 - val_loss: 2.0720e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.3772e-06 - val_loss: 2.0924e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9756e-06 - val_loss: 1.6602e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8706e-06 - val_loss: 2.0935e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1844e-06 - val_loss: 1.6374e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.9096e-06 - val_loss: 2.0837e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0485e-06 - val_loss: 1.5915e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7914e-06 - val_loss: 2.0038e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.0204e-06 - val_loss: 1.5709e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8973e-06 - val_loss: 1.4417e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5927e-06 - val_loss: 1.6254e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.8753e-06 - val_loss: 1.4145e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5111e-06 - val_loss: 1.3255e-05\n",
      "Epoch 40/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.7283e-06 - val_loss: 1.3049e-05\n",
      "Epoch 41/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6541e-06 - val_loss: 1.7788e-05\n",
      "Epoch 42/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6675e-06 - val_loss: 1.4801e-05\n",
      "Epoch 43/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.4760e-06 - val_loss: 1.5573e-05\n",
      "Epoch 00043: early stopping\n",
      "0.0005535204350925015\n",
      "Train on 229500 samples, validate on 123578 samples\n",
      "Epoch 1/100\n",
      "229500/229500 [==============================] - 1s 4us/step - loss: 0.0297 - val_loss: 0.0044\n",
      "Epoch 2/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 0.0010 - val_loss: 0.0039\n",
      "Epoch 3/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.3260e-04 - val_loss: 0.0034\n",
      "Epoch 4/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.2435e-04 - val_loss: 0.0028\n",
      "Epoch 5/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0537e-04 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.2168e-04 - val_loss: 0.0018\n",
      "Epoch 7/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.5747e-04 - val_loss: 0.0013\n",
      "Epoch 8/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0800e-04 - val_loss: 9.0135e-04\n",
      "Epoch 9/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.2032e-05 - val_loss: 6.3316e-04\n",
      "Epoch 10/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.7305e-05 - val_loss: 3.8409e-04\n",
      "Epoch 11/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.1360e-05 - val_loss: 2.7462e-04\n",
      "Epoch 12/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.1729e-05 - val_loss: 1.9193e-04\n",
      "Epoch 13/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.6131e-05 - val_loss: 1.4849e-04\n",
      "Epoch 14/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.2717e-05 - val_loss: 1.1293e-04\n",
      "Epoch 15/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 1.0478e-05 - val_loss: 9.2087e-05\n",
      "Epoch 16/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 8.7736e-06 - val_loss: 8.0324e-05\n",
      "Epoch 17/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 7.3874e-06 - val_loss: 6.9386e-05\n",
      "Epoch 18/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 6.2503e-06 - val_loss: 5.8736e-05\n",
      "Epoch 19/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 5.3585e-06 - val_loss: 5.1781e-05\n",
      "Epoch 20/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 4.5478e-06 - val_loss: 4.5612e-05\n",
      "Epoch 21/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.9978e-06 - val_loss: 4.0365e-05\n",
      "Epoch 22/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.5269e-06 - val_loss: 3.6917e-05\n",
      "Epoch 23/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.2199e-06 - val_loss: 3.5904e-05\n",
      "Epoch 24/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0059e-06 - val_loss: 3.1513e-05\n",
      "Epoch 25/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.8058e-06 - val_loss: 3.2014e-05\n",
      "Epoch 26/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6673e-06 - val_loss: 2.8275e-05\n",
      "Epoch 27/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4968e-06 - val_loss: 2.7436e-05\n",
      "Epoch 28/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6041e-06 - val_loss: 2.7034e-05\n",
      "Epoch 29/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6184e-06 - val_loss: 2.6970e-05\n",
      "Epoch 30/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.5655e-06 - val_loss: 2.4754e-05\n",
      "Epoch 31/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6800e-06 - val_loss: 2.7042e-05\n",
      "Epoch 32/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6152e-06 - val_loss: 3.6529e-05\n",
      "Epoch 33/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.4186e-06 - val_loss: 2.3701e-05\n",
      "Epoch 34/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4007e-06 - val_loss: 2.2111e-05\n",
      "Epoch 35/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.7051e-06 - val_loss: 2.1523e-05\n",
      "Epoch 36/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.4927e-06 - val_loss: 2.1330e-05\n",
      "Epoch 37/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 2.6853e-06 - val_loss: 3.2732e-05\n",
      "Epoch 38/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0382e-06 - val_loss: 2.4643e-05\n",
      "Epoch 39/100\n",
      "229500/229500 [==============================] - 1s 3us/step - loss: 3.0726e-06 - val_loss: 2.3190e-05\n",
      "Epoch 00039: early stopping\n",
      "0.0010684893986728885\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log_name = datetime.now().strftime('mlp_lgs\\\\mlp_twolayers_%Y_%m_%d_%H.%M.%S.log')\n",
    "\n",
    "model = multi_train_NLP(structs_list    = [[4,8,4], [6, 4, 6], [10, 8, 8], [12,16, 10], [16, 12, 14]], \n",
    "                        patience        = 3, \n",
    "                        batch_size      = 1000, \n",
    "                        epochs          = 100, \n",
    "                        train_data      = train_set_trim, \n",
    "                        test_data       = test_set_trim, \n",
    "                        set_num         = 9,\n",
    "                        criterion       = 'mse', \n",
    "                        single_times    = 5, \n",
    "                        dropout_2list   = [[0,0,0],[0,0,0],[0,0,0],[0,0,0],[0,0,0]], \n",
    "                        log_name        = log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model.get_config()['layers']) - 1):\n",
    "    if i%2 == 0:\n",
    "        field = 'units'\n",
    "        log = f'Liczba neuronow w {int(i/2) + 1} warstwie ukrytej:'\n",
    "    else:\n",
    "        field = 'rate'\n",
    "        log = f'Szansa opuszczenia neuronu w {int(i/2) + 1} warstwie ukrytej:'\n",
    "    val  = model.get_config()['layers'][i]['config'][field]\n",
    "    print(f'{log} {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 4\n",
    "y = sim(model, test_set_trim, col)\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(0, len(test_set_trim)):\n",
    "    plt.figure(col)\n",
    "    y = sim(model, test_set_trim, col)\n",
    "    eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "y = sim(model, surge_trim, num)\n",
    "eval_prediction(y, surge_trim[num][:, -1], 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "y = sim(model, vdips_trim, num)\n",
    "eval_prediction(y, vdips_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLPs\\\\model_MLP_6_8_5_2020-5-5.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_model(\n",
    "    models_list\n",
    "    ,test_data\n",
    "    ,log_name=None\n",
    "):\n",
    "    best_score = 16\n",
    "    best_model = None\n",
    "    idx = 0\n",
    "    best_idx = 0\n",
    "    \n",
    "    for model in models_list:\n",
    "        print2(f'Evaluating model with index: {idx}', log_name)\n",
    "        \n",
    "        sum_score = 0\n",
    "        \n",
    "        for scenario_id in range(0, len(test_data)):\n",
    "            y = sim(model, test_data, scenario_id)\n",
    "            curr_score = eval_prediction(y, test_data[scenario_id][:, -1], 0)\n",
    "            print2(f'Score on scenario {scenario_id}: {curr_score}', log_name)\n",
    "            sum_score += curr_score\n",
    "            \n",
    "        \n",
    "        print2(f'General score of the model: {sum_score}', log_name)\n",
    "        if sum_score < best_score:\n",
    "            best_model = model\n",
    "            best_score = sum_score\n",
    "            best_idx   = idx\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    print2(f\"Best's model index: {best_idx}, with score: {best_score}\", log_name)\n",
    "    print2(best_model.summary(), log_name)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import dill\n",
    "\n",
    "path = \"MLPS\\\\\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.pkl\")) #make list of paths\n",
    "\n",
    "models = []\n",
    "for spk in all_files:\n",
    "    with open(spk, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.668665899414138e-05\n",
      "0.000324611065497408\n",
      "6.089822858965521e-05\n",
      "0.0004303460551209218\n",
      "0.0003561443535230201\n",
      "0.0001737762285685472\n",
      "0.0004814176734143251\n",
      "0.00014746946971354923\n",
      "0.00019282942585771535\n",
      "0.00026994204805746145\n",
      "0.0008549491988767695\n",
      "0.00019155690396469214\n",
      "0.00017590018720682675\n",
      "3.4123827095483945e-05\n",
      "0.0007993190939314791\n",
      "0.0003716858879391246\n",
      "8.265416911905337e-05\n",
      "6.639686899251771e-05\n",
      "3.053358210739439e-05\n",
      "5.799670771516662e-05\n",
      "0.00017480198306495527\n",
      "0.00015447509408439484\n",
      "0.00010534106326921171\n",
      "0.00017176620645170033\n",
      "0.0001704241340369379\n",
      "0.00013820554785524717\n",
      "0.00022496005228993084\n",
      "6.135102977347904e-05\n",
      "0.00023715004659219458\n",
      "2.715251586237322e-05\n",
      "0.00022717463674860107\n",
      "0.0005062127407162339\n",
      "0.0016838068904255798\n",
      "0.0015671847601340905\n",
      "0.002240168483258396\n",
      "0.000403355612116042\n",
      "0.003219607291204743\n",
      "0.0008698219383058694\n",
      "0.00011460709067563267\n",
      "0.0065706473290571025\n",
      "0.003984599452048737\n",
      "0.00017217173200015858\n",
      "0.0004051004568049649\n",
      "0.00019826278686739277\n",
      "0.0006348379339742901\n",
      "0.00013231183089131436\n",
      "0.00044075013545582164\n",
      "0.024801623742090658\n",
      "4.4010332864275104e-05\n",
      "5.601771379782115e-05\n",
      "2.6849659064250945e-05\n",
      "5.725184917440555e-05\n",
      "0.00019518979784297598\n",
      "7.218479688185288e-05\n",
      "2.746547848374115e-05\n",
      "5.2013539384848816e-05\n",
      "4.409018847657157e-05\n",
      "0.00015133066755228624\n",
      "6.905232154829694e-05\n",
      "0.00013924533011791212\n",
      "0.00011471426708853204\n",
      "4.12877663511177e-05\n",
      "0.00011302627881852304\n",
      "0.0006117847800819529\n",
      "8.01349899784123e-05\n",
      "5.278857984104748e-05\n",
      "3.899983669849453e-05\n",
      "3.15296256668272e-05\n",
      "0.00011173736957940743\n",
      "3.732993258117782e-05\n",
      "1.9552475528321623e-05\n",
      "0.00017671146215503597\n",
      "0.00012232415588120656\n",
      "5.764915167802717e-05\n",
      "3.634009836653986e-05\n",
      "3.6439177145041286e-05\n",
      "7.605145912895258e-05\n",
      "1.5479614650754913e-05\n",
      "7.225303394640462e-05\n",
      "0.0012262923651121243\n",
      "3.883045836735598e-05\n",
      "3.9626492278048115e-05\n",
      "1.741570004662799e-05\n",
      "5.010785734836885e-05\n",
      "0.00015871640384210222\n",
      "7.919271318169069e-05\n",
      "3.1387239594492375e-05\n",
      "4.214083534006736e-05\n",
      "4.616469560600478e-05\n",
      "5.1804437165018447e-05\n",
      "0.000168326643206532\n",
      "5.751881808985992e-05\n",
      "4.602862110441123e-05\n",
      "1.0465129835864488e-05\n",
      "5.28018076321852e-05\n",
      "0.023526553452705684\n",
      "4.6678016917121285e-05\n",
      "5.02937493185195e-05\n",
      "1.6406254010338027e-05\n",
      "3.671173128933106e-05\n",
      "9.553473609801637e-05\n",
      "4.481347738398279e-05\n",
      "3.172618674018919e-05\n",
      "9.80791223027673e-05\n",
      "4.713672293859422e-05\n",
      "8.358831321892073e-05\n",
      "3.35816315433139e-05\n",
      "3.822346285897531e-05\n",
      "0.0001667624933722447\n",
      "7.87572606648392e-05\n",
      "0.00013938012944100784\n",
      "0.0006816303409233761\n",
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_328 (Dense)            (None, 8)                 56        \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log_name = datetime.now().strftime('evaluation\\\\MLP_eval_%Y_%m_%d_%H.%M.%S.log')\n",
    "\n",
    "best_model = pick_best_model(\n",
    "    models_list = models\n",
    "    ,test_data  = test_set_trim\n",
    "    ,log_name   = log_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_110\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_328 (Dense)            (None, 8)                 56        \n",
      "_________________________________________________________________\n",
      "dropout_219 (Dropout)        (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_329 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 6)                 0         \n",
      "_________________________________________________________________\n",
      "dense_330 (Dense)            (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 117\n",
      "Trainable params: 117\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use whole series\n",
    "train_series = []\n",
    "\n",
    "indices = pd.read_csv('indices.csv')\n",
    "indices = indices.values\n",
    "for i in range(0,len(indices)):\n",
    "    train_series.append(train_set_trim[indices[i, 0]:indices[i, 1], :])\n",
    "\n",
    "max_len = 0\n",
    "for i in range(0,len(train_series)):\n",
    "    if len(train_series[i]) > max_len:\n",
    "        max_len = len(train_series[i])\n",
    "        \n",
    "for i in range(0, len(train_series)):\n",
    "    train_series[i]=np.pad(train_series[i]\n",
    "                 ,((0,max_len - len(train_series[i])),(0,0))\n",
    "                 ,'constant'\n",
    "                 , constant_values=-10)\n",
    "\n",
    "train_series = np.dstack(train_series)\n",
    "train_series = np.moveaxis(train_series, 2, 0)\n",
    "\n",
    "Xs = train_series[:, :, :-1]\n",
    "Ys = train_series[:, :, -1]\n",
    "Ys = np.reshape(Ys, (-1, max_len, 1))\n",
    "\n",
    "print(f'Wymiar danych treningowych: {Xs.shape}')\n",
    "print(f'Wymiar targetów: {Ys.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_timeseries(Xs, Ys, TXs, TYs, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SXs, SYs = prepare_surge(surge_trim, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_surges(SXs, SYs, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = load_timeseries(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SXs, SYs = load_surges(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SXs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 0.0828 - val_loss: 0.0043\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 8.8895e-04 - val_loss: 0.0046\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 5.8020e-04 - val_loss: 0.0037\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 4.3629e-04 - val_loss: 0.0030\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 3.3657e-04 - val_loss: 0.0025\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.6642e-04 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1485e-04 - val_loss: 0.0015\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7467e-04 - val_loss: 0.0012\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4286e-04 - val_loss: 9.1713e-04\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1904e-04 - val_loss: 7.6405e-04\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.0261e-04 - val_loss: 6.9690e-04\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 9.1046e-05 - val_loss: 6.5806e-04\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.1772e-05 - val_loss: 6.5294e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 7.3810e-05 - val_loss: 6.4810e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 6.6988e-05 - val_loss: 6.4657e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 6.0698e-05 - val_loss: 6.5798e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.5537e-05 - val_loss: 6.7098e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.0665e-05 - val_loss: 6.7264e-04\n",
      "Epoch 00018: early stopping\n",
      "0.0017638070361166913\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 0.0147 - val_loss: 0.0084\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 9.7346e-04 - val_loss: 0.0048\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.1195e-04 - val_loss: 0.0040\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 3.6299e-04 - val_loss: 0.0034\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.8323e-04 - val_loss: 0.0031\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.3398e-04 - val_loss: 0.0029\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.9948e-04 - val_loss: 0.0026\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7327e-04 - val_loss: 0.0025\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5254e-04 - val_loss: 0.0023\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.3534e-04 - val_loss: 0.0022\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2041e-04 - val_loss: 0.0021\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.0717e-04 - val_loss: 0.0021\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 9.5259e-05 - val_loss: 0.0020\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 8.4337e-05 - val_loss: 0.0019\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 7.4600e-05 - val_loss: 0.0019\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 6.6285e-05 - val_loss: 0.0018\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.9210e-05 - val_loss: 0.0018\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.3447e-05 - val_loss: 0.0017\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.9186e-05 - val_loss: 0.0017\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.6489e-05 - val_loss: 0.0016\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.4626e-05 - val_loss: 0.0016\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.3455e-05 - val_loss: 0.0015\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.2499e-05 - val_loss: 0.0014\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.1764e-05 - val_loss: 0.0013\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.1031e-05 - val_loss: 0.0013\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.0093e-05 - val_loss: 0.0013\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.9255e-05 - val_loss: 0.0012\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.8049e-05 - val_loss: 0.0011\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.7059e-05 - val_loss: 9.9564e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.5967e-05 - val_loss: 9.3541e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.4451e-05 - val_loss: 7.8815e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.2514e-05 - val_loss: 6.6400e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.1023e-05 - val_loss: 5.4460e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.8856e-05 - val_loss: 4.3624e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.6578e-05 - val_loss: 3.2761e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.4591e-05 - val_loss: 2.3612e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.2921e-05 - val_loss: 2.0511e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1472e-05 - val_loss: 1.7752e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0448e-05 - val_loss: 1.6622e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9411e-05 - val_loss: 1.7348e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8616e-05 - val_loss: 1.6716e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7736e-05 - val_loss: 1.7004e-04\n",
      "Epoch 00042: early stopping\n",
      "0.00043576686415988945\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 0.0270 - val_loss: 0.0071\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.0788e-04 - val_loss: 0.0036\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 3.7354e-04 - val_loss: 0.0031\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.3644e-04 - val_loss: 0.0028\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.0814e-04 - val_loss: 0.0027\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.8021e-04 - val_loss: 0.0025\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5276e-04 - val_loss: 0.0024\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.2599e-04 - val_loss: 0.0022\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0031e-04 - val_loss: 0.0020\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7597e-04 - val_loss: 0.0018\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5335e-04 - val_loss: 0.0017\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3249e-04 - val_loss: 0.0016\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1401e-04 - val_loss: 0.0015\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 9.8004e-05 - val_loss: 0.0014\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.4538e-05 - val_loss: 0.0013\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 7.3639e-05 - val_loss: 0.0013\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.5162e-05 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.8773e-05 - val_loss: 0.0012\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.4036e-05 - val_loss: 0.0012\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.0421e-05 - val_loss: 0.0012\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.7587e-05 - val_loss: 0.0012\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 4.5329e-05 - val_loss: 0.0012\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.3338e-05 - val_loss: 0.0011\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.1466e-05 - val_loss: 0.0011\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.9690e-05 - val_loss: 0.0011\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.8125e-05 - val_loss: 0.0011\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.6566e-05 - val_loss: 0.0010\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.5099e-05 - val_loss: 9.9452e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.3587e-05 - val_loss: 9.5974e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.2270e-05 - val_loss: 9.2096e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.1021e-05 - val_loss: 8.8122e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.9764e-05 - val_loss: 8.5072e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.8610e-05 - val_loss: 8.1498e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.7574e-05 - val_loss: 7.8652e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.6667e-05 - val_loss: 7.5721e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5738e-05 - val_loss: 7.3422e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5021e-05 - val_loss: 6.9853e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.4307e-05 - val_loss: 6.7370e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.3642e-05 - val_loss: 6.5414e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.3158e-05 - val_loss: 6.3592e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.2684e-05 - val_loss: 6.1588e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.2310e-05 - val_loss: 6.0066e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 2.1790e-05 - val_loss: 5.8406e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 2.1556e-05 - val_loss: 5.7103e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1115e-05 - val_loss: 5.6343e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.0888e-05 - val_loss: 5.4690e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0563e-05 - val_loss: 5.2860e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0131e-05 - val_loss: 5.2100e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0061e-05 - val_loss: 5.1380e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.9751e-05 - val_loss: 5.0554e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.9587e-05 - val_loss: 4.8923e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.9324e-05 - val_loss: 4.8388e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9235e-05 - val_loss: 4.7003e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.8808e-05 - val_loss: 4.6474e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.8648e-05 - val_loss: 4.6126e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8395e-05 - val_loss: 4.5254e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.8113e-05 - val_loss: 4.4194e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.7965e-05 - val_loss: 4.3919e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7673e-05 - val_loss: 4.2760e-04\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7521e-05 - val_loss: 4.2569e-04\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7423e-05 - val_loss: 4.1859e-04\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7266e-05 - val_loss: 4.0118e-04\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.7074e-05 - val_loss: 3.9708e-04\n",
      "Epoch 64/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.6829e-05 - val_loss: 3.8910e-04oss: 1.7168\n",
      "Epoch 65/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.6692e-05 - val_loss: 3.8866e-04\n",
      "Epoch 66/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.6491e-05 - val_loss: 3.8039e-04\n",
      "Epoch 67/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6253e-05 - val_loss: 3.6961e-04\n",
      "Epoch 68/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6237e-05 - val_loss: 3.6561e-04\n",
      "Epoch 69/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6027e-05 - val_loss: 3.5744e-04\n",
      "Epoch 70/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5882e-05 - val_loss: 3.5342e-04\n",
      "Epoch 71/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5916e-05 - val_loss: 3.4631e-04\n",
      "Epoch 72/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5541e-05 - val_loss: 3.4276e-04\n",
      "Epoch 73/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5436e-05 - val_loss: 3.3304e-04\n",
      "Epoch 74/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5314e-05 - val_loss: 3.2620e-04\n",
      "Epoch 75/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5201e-05 - val_loss: 3.2223e-04\n",
      "Epoch 76/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5103e-05 - val_loss: 3.1769e-04\n",
      "Epoch 77/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5003e-05 - val_loss: 3.1544e-04\n",
      "Epoch 78/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.4902e-05 - val_loss: 3.1167e-04\n",
      "Epoch 79/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4824e-05 - val_loss: 3.0594e-04\n",
      "Epoch 80/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.4684e-05 - val_loss: 2.9509e-04\n",
      "Epoch 81/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.4609e-05 - val_loss: 2.9356e-04\n",
      "Epoch 82/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4403e-05 - val_loss: 2.8854e-04\n",
      "Epoch 83/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4421e-05 - val_loss: 2.8208e-04\n",
      "Epoch 84/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4241e-05 - val_loss: 2.7543e-04\n",
      "Epoch 85/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4243e-05 - val_loss: 2.7121e-04\n",
      "Epoch 86/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4120e-05 - val_loss: 2.6659e-04\n",
      "Epoch 87/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3913e-05 - val_loss: 2.6732e-04\n",
      "Epoch 88/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3824e-05 - val_loss: 2.5931e-04\n",
      "Epoch 89/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3927e-05 - val_loss: 2.5645e-04\n",
      "Epoch 90/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3741e-05 - val_loss: 2.4934e-04\n",
      "Epoch 91/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3657e-05 - val_loss: 2.4899e-04\n",
      "Epoch 92/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3691e-05 - val_loss: 2.4635e-04\n",
      "Epoch 93/100\n",
      "229497/229497 [==============================] - 3s 11us/step - loss: 1.3448e-05 - val_loss: 2.4214e-04\n",
      "Epoch 94/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3419e-05 - val_loss: 2.3728e-04\n",
      "Epoch 95/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3348e-05 - val_loss: 2.2981e-04\n",
      "Epoch 96/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3244e-05 - val_loss: 2.2752e-04\n",
      "Epoch 97/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3177e-05 - val_loss: 2.2243e-04\n",
      "Epoch 98/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3261e-05 - val_loss: 2.1894e-04\n",
      "Epoch 99/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.2938e-05 - val_loss: 2.1455e-04\n",
      "Epoch 100/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3009e-05 - val_loss: 2.1194e-04\n",
      "0.0004074070710566217\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 0.0130 - val_loss: 0.0048\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.9189e-04 - val_loss: 0.0043\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.6986e-04 - val_loss: 0.0037\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.7252e-04 - val_loss: 0.0031\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.9466e-04 - val_loss: 0.0026\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.3104e-04 - val_loss: 0.0021\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7939e-04 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.3812e-04 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0590e-04 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.1746e-05 - val_loss: 9.4155e-04\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.4040e-05 - val_loss: 7.4101e-04\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.1670e-05 - val_loss: 6.0756e-04\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.3241e-05 - val_loss: 5.1094e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.7689e-05 - val_loss: 4.4120e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.4118e-05 - val_loss: 3.9012e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.1782e-05 - val_loss: 3.6396e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.9961e-05 - val_loss: 3.4914e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.8642e-05 - val_loss: 3.2118e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.7435e-05 - val_loss: 3.1076e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.6493e-05 - val_loss: 2.9092e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5203e-05 - val_loss: 2.8065e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.4209e-05 - val_loss: 2.7449e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.3423e-05 - val_loss: 2.6160e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.2648e-05 - val_loss: 2.5724e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1950e-05 - val_loss: 2.4052e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1142e-05 - val_loss: 2.4730e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0498e-05 - val_loss: 2.3260e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9898e-05 - val_loss: 2.2548e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.9400e-05 - val_loss: 2.1961e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.8684e-05 - val_loss: 2.1756e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.8296e-05 - val_loss: 2.1258e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.7914e-05 - val_loss: 2.0922e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.7471e-05 - val_loss: 2.0138e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.7269e-05 - val_loss: 1.9921e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.6744e-05 - val_loss: 1.9774e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.6437e-05 - val_loss: 1.9346e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.6288e-05 - val_loss: 1.8955e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.6078e-05 - val_loss: 1.8822e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5695e-05 - val_loss: 1.8160e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5481e-05 - val_loss: 1.8331e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5242e-05 - val_loss: 1.7836e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5104e-05 - val_loss: 1.7567e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5002e-05 - val_loss: 1.7810e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5163e-05 - val_loss: 1.6951e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4505e-05 - val_loss: 1.7247e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4373e-05 - val_loss: 1.6580e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4344e-05 - val_loss: 1.6385e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4264e-05 - val_loss: 1.6353e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.4094e-05 - val_loss: 1.6016e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3964e-05 - val_loss: 1.6009e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4298e-05 - val_loss: 1.5697e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3620e-05 - val_loss: 1.5975e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3726e-05 - val_loss: 1.5701e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3523e-05 - val_loss: 1.5555e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3458e-05 - val_loss: 1.5436e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.3481e-05 - val_loss: 1.5298e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3288e-05 - val_loss: 1.5157e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3123e-05 - val_loss: 1.5037e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3069e-05 - val_loss: 1.5051e-04\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2989e-05 - val_loss: 1.4820e-04\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3186e-05 - val_loss: 1.5169e-04\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2963e-05 - val_loss: 1.5009e-04\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2822e-05 - val_loss: 1.5206e-04\n",
      "Epoch 00063: early stopping\n",
      "0.00014087497426635556\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 0.0680 - val_loss: 0.0113\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 0.0031 - val_loss: 0.0078\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 11us/step - loss: 0.0020 - val_loss: 0.0072\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 0.0015 - val_loss: 0.0067\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 2s 9us/step - loss: 0.0013 - val_loss: 0.0063\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 0.0011 - val_loss: 0.0060\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 8.9848e-04 - val_loss: 0.0056\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 7.7278e-04 - val_loss: 0.0052\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 6.6724e-04 - val_loss: 0.0049\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.7610e-04 - val_loss: 0.0046\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.9557e-04 - val_loss: 0.0043\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.2364e-04 - val_loss: 0.0040\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.5925e-04 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.0147e-04 - val_loss: 0.0035\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.5016e-04 - val_loss: 0.0032\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.0620e-04 - val_loss: 0.0030\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.6942e-04 - val_loss: 0.0028\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4027e-04 - val_loss: 0.0027\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.1835e-04 - val_loss: 0.0025\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 2s 9us/step - loss: 1.0260e-04 - val_loss: 0.0024\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 2s 9us/step - loss: 9.1068e-05 - val_loss: 0.0022\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 8.3068e-05 - val_loss: 0.0021\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 7.6528e-05 - val_loss: 0.0020\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 7.1242e-05 - val_loss: 0.0019\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 6.6867e-05 - val_loss: 0.0018\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 2s 11us/step - loss: 6.3185e-05 - val_loss: 0.0017\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 2s 11us/step - loss: 5.9552e-05 - val_loss: 0.0016\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.6182e-05 - val_loss: 0.0015\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.3012e-05 - val_loss: 0.0014\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 5.0068e-05 - val_loss: 0.0014\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.7158e-05 - val_loss: 0.0013\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.4058e-05 - val_loss: 0.0013\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 4.0816e-05 - val_loss: 0.0012\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.9134e-05 - val_loss: 0.0012\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.7850e-05 - val_loss: 0.0011\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.6255e-05 - val_loss: 0.0011\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.4809e-05 - val_loss: 0.0010\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.3047e-05 - val_loss: 9.7711e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.1950e-05 - val_loss: 9.2081e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 3.0363e-05 - val_loss: 8.6101e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.9121e-05 - val_loss: 8.1666e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.7703e-05 - val_loss: 7.8474e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.6648e-05 - val_loss: 7.2931e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.5604e-05 - val_loss: 7.0134e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.4329e-05 - val_loss: 6.5716e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.3435e-05 - val_loss: 6.3363e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.2645e-05 - val_loss: 5.9447e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 2.2407e-05 - val_loss: 5.7930e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 3s 11us/step - loss: 2.1177e-05 - val_loss: 5.5941e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0737e-05 - val_loss: 5.4400e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9937e-05 - val_loss: 5.2851e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9581e-05 - val_loss: 5.1489e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9072e-05 - val_loss: 5.0380e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8613e-05 - val_loss: 4.8356e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8589e-05 - val_loss: 4.6915e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8023e-05 - val_loss: 4.6299e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7847e-05 - val_loss: 4.5019e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7629e-05 - val_loss: 4.3166e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7292e-05 - val_loss: 4.3330e-04\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6793e-05 - val_loss: 4.1696e-04\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7115e-05 - val_loss: 4.1605e-04\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6600e-05 - val_loss: 4.0189e-04\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6275e-05 - val_loss: 3.9791e-04\n",
      "Epoch 64/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6036e-05 - val_loss: 3.8921e-04\n",
      "Epoch 65/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5833e-05 - val_loss: 3.6882e-04\n",
      "Epoch 66/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5959e-05 - val_loss: 3.7867e-04\n",
      "Epoch 67/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5894e-05 - val_loss: 3.7048e-04\n",
      "Epoch 68/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5648e-05 - val_loss: 3.6148e-04\n",
      "Epoch 69/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5411e-05 - val_loss: 3.5824e-04\n",
      "Epoch 70/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5171e-05 - val_loss: 3.5713e-04\n",
      "Epoch 71/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5112e-05 - val_loss: 3.4267e-04\n",
      "Epoch 72/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.5089e-05 - val_loss: 3.3711e-04\n",
      "Epoch 73/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4722e-05 - val_loss: 3.3511e-04\n",
      "Epoch 74/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4770e-05 - val_loss: 3.2979e-04\n",
      "Epoch 75/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4441e-05 - val_loss: 3.2372e-04\n",
      "Epoch 76/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4502e-05 - val_loss: 3.1852e-04\n",
      "Epoch 77/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4165e-05 - val_loss: 3.1442e-04\n",
      "Epoch 78/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.4223e-05 - val_loss: 3.0920e-04\n",
      "Epoch 79/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3925e-05 - val_loss: 3.0017e-04\n",
      "Epoch 80/100\n",
      "229497/229497 [==============================] - 2s 9us/step - loss: 1.4030e-05 - val_loss: 3.0297e-04\n",
      "Epoch 81/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3864e-05 - val_loss: 2.9766e-04\n",
      "Epoch 82/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3852e-05 - val_loss: 2.8813e-04\n",
      "Epoch 83/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3681e-05 - val_loss: 2.8805e-04\n",
      "Epoch 84/100\n",
      "229497/229497 [==============================] - 2s 9us/step - loss: 1.3552e-05 - val_loss: 2.9129e-04\n",
      "Epoch 85/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3493e-05 - val_loss: 2.7973e-04\n",
      "Epoch 86/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3571e-05 - val_loss: 2.7977e-04\n",
      "Epoch 87/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3283e-05 - val_loss: 2.7520e-04\n",
      "Epoch 88/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.3081e-05 - val_loss: 2.6952e-04\n",
      "Epoch 89/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2869e-05 - val_loss: 2.6776e-04\n",
      "Epoch 90/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2882e-05 - val_loss: 2.6120e-04\n",
      "Epoch 91/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2935e-05 - val_loss: 2.6580e-04\n",
      "Epoch 92/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2643e-05 - val_loss: 2.6024e-04\n",
      "Epoch 93/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2649e-05 - val_loss: 2.5351e-04\n",
      "Epoch 94/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2532e-05 - val_loss: 2.4903e-04\n",
      "Epoch 95/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2416e-05 - val_loss: 2.4939e-04\n",
      "Epoch 96/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2483e-05 - val_loss: 2.4585e-04\n",
      "Epoch 97/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2524e-05 - val_loss: 2.4186e-04\n",
      "Epoch 98/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2354e-05 - val_loss: 2.4295e-04\n",
      "Epoch 99/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2211e-05 - val_loss: 2.2900e-04\n",
      "Epoch 100/100\n",
      "229497/229497 [==============================] - 2s 10us/step - loss: 1.2227e-05 - val_loss: 2.4356e-04\n",
      "0.0006139874439792416\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 0.0170 - val_loss: 0.0063\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.3349e-04 - val_loss: 0.0039\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.2410e-04 - val_loss: 0.0033\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.2176e-04 - val_loss: 0.0028\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.4886e-04 - val_loss: 0.0024\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9301e-04 - val_loss: 0.0020\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5013e-04 - val_loss: 0.0016\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1874e-04 - val_loss: 0.0014\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 9.7822e-05 - val_loss: 0.0013\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.3753e-05 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 7.3363e-05 - val_loss: 0.0011\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.4203e-05 - val_loss: 9.9011e-04\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.5382e-05 - val_loss: 8.8977e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.7110e-05 - val_loss: 7.6064e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.0183e-05 - val_loss: 6.4281e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 3.4985e-05 - val_loss: 5.2670e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.0962e-05 - val_loss: 4.2482e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.7438e-05 - val_loss: 3.3838e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.5009e-05 - val_loss: 2.6734e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.2886e-05 - val_loss: 2.1949e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.1475e-05 - val_loss: 1.8521e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 2.0077e-05 - val_loss: 1.6221e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9194e-05 - val_loss: 1.4725e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.8414e-05 - val_loss: 1.3622e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.7754e-05 - val_loss: 1.2696e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7054e-05 - val_loss: 1.3253e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 1.6767e-05 - val_loss: 1.3178e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 1.6325e-05 - val_loss: 1.1841e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.5816e-05 - val_loss: 1.2360e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.5876e-05 - val_loss: 1.1302e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.5530e-05 - val_loss: 1.0857e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5444e-05 - val_loss: 1.0725e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5421e-05 - val_loss: 1.0648e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5176e-05 - val_loss: 1.0891e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5367e-05 - val_loss: 1.1032e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4886e-05 - val_loss: 1.0496e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4904e-05 - val_loss: 1.0302e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4804e-05 - val_loss: 1.0360e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4497e-05 - val_loss: 1.0237e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4374e-05 - val_loss: 1.0230e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4437e-05 - val_loss: 1.0083e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.4827e-05 - val_loss: 1.0291e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 5s 23us/step - loss: 1.3844e-05 - val_loss: 1.0092e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 5s 22us/step - loss: 1.3983e-05 - val_loss: 1.0039e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.3977e-05 - val_loss: 1.0038e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.3709e-05 - val_loss: 1.0054e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.3677e-05 - val_loss: 1.0086e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3530e-05 - val_loss: 9.9821e-05\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3633e-05 - val_loss: 9.9329e-05\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3718e-05 - val_loss: 9.8353e-05\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.3083e-05 - val_loss: 1.0362e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 1.3305e-05 - val_loss: 9.8212e-05\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3206e-05 - val_loss: 9.7652e-05\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2967e-05 - val_loss: 9.7729e-05\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3372e-05 - val_loss: 9.9471e-05\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2922e-05 - val_loss: 9.7396e-05\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2862e-05 - val_loss: 9.7220e-05\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2689e-05 - val_loss: 9.7190e-05\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2631e-05 - val_loss: 9.7031e-05\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2800e-05 - val_loss: 9.9030e-05\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2742e-05 - val_loss: 9.8572e-05\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2512e-05 - val_loss: 9.6219e-05\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2525e-05 - val_loss: 9.9339e-05\n",
      "Epoch 64/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.3059e-05 - val_loss: 9.5285e-05\n",
      "Epoch 65/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2370e-05 - val_loss: 9.7117e-05\n",
      "Epoch 66/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2294e-05 - val_loss: 9.4743e-05\n",
      "Epoch 67/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2254e-05 - val_loss: 9.4867e-05\n",
      "Epoch 68/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2208e-05 - val_loss: 9.6904e-05\n",
      "Epoch 69/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2319e-05 - val_loss: 9.4822e-05\n",
      "Epoch 00069: early stopping\n",
      "0.00019132093357993371\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 0.0326 - val_loss: 0.0049\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 6.1803e-04 - val_loss: 0.0041\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.4684e-04 - val_loss: 0.0031\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 3.0875e-04 - val_loss: 0.0022\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0551e-04 - val_loss: 0.0013\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3854e-04 - val_loss: 8.2929e-04\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1066e-04 - val_loss: 6.5509e-04\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 9.5336e-05 - val_loss: 6.0150e-04\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 8.2681e-05 - val_loss: 5.4547e-04\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 7.1912e-05 - val_loss: 5.1848e-04\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.2593e-05 - val_loss: 4.9008e-04\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.4875e-05 - val_loss: 4.7168e-04\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.8456e-05 - val_loss: 4.6608e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.3372e-05 - val_loss: 4.6225e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 3.9330e-05 - val_loss: 4.5799e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.6284e-05 - val_loss: 4.5487e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.3839e-05 - val_loss: 4.5230e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.2177e-05 - val_loss: 4.5318e-04- ETA: 0s - loss: 3.2392e-\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.0780e-05 - val_loss: 4.4787e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.9662e-05 - val_loss: 4.2967e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.8790e-05 - val_loss: 4.2709e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.8045e-05 - val_loss: 3.9356e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.7512e-05 - val_loss: 3.9077e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.6539e-05 - val_loss: 3.7270e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 2.5762e-05 - val_loss: 3.5631e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5048e-05 - val_loss: 3.2021e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 2.4424e-05 - val_loss: 3.1611e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.3810e-05 - val_loss: 2.8381e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.3455e-05 - val_loss: 2.7882e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.2546e-05 - val_loss: 2.5764e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.2087e-05 - val_loss: 2.2600e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1555e-05 - val_loss: 2.0776e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1583e-05 - val_loss: 1.9516e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.0595e-05 - val_loss: 1.9315e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.0144e-05 - val_loss: 1.8944e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.0025e-05 - val_loss: 1.7342e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9737e-05 - val_loss: 1.6138e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9423e-05 - val_loss: 1.5553e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 1.9113e-05 - val_loss: 1.4927e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 1.8936e-05 - val_loss: 1.4436e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.8872e-05 - val_loss: 1.2983e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.8279e-05 - val_loss: 1.2797e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.8283e-05 - val_loss: 1.2746e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7999e-05 - val_loss: 1.2716e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7917e-05 - val_loss: 1.1901e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7708e-05 - val_loss: 1.1799e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.7313e-05 - val_loss: 1.2809e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.7277e-05 - val_loss: 1.1510e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.6866e-05 - val_loss: 1.0860e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.6963e-05 - val_loss: 1.0789e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6724e-05 - val_loss: 1.0922e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6453e-05 - val_loss: 1.0576e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6355e-05 - val_loss: 1.0591e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6545e-05 - val_loss: 1.0451e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6271e-05 - val_loss: 1.0354e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5874e-05 - val_loss: 1.0284e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5788e-05 - val_loss: 1.0065e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5433e-05 - val_loss: 1.3038e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5786e-05 - val_loss: 9.9044e-05\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5112e-05 - val_loss: 9.9439e-05\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5266e-05 - val_loss: 9.7448e-05\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5262e-05 - val_loss: 9.7985e-05\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4929e-05 - val_loss: 9.6059e-05\n",
      "Epoch 64/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4976e-05 - val_loss: 9.6654e-05\n",
      "Epoch 65/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4872e-05 - val_loss: 9.5327e-05\n",
      "Epoch 66/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4666e-05 - val_loss: 9.5990e-05\n",
      "Epoch 67/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4241e-05 - val_loss: 9.5070e-05\n",
      "Epoch 68/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4389e-05 - val_loss: 9.6774e-05\n",
      "Epoch 69/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4318e-05 - val_loss: 9.4636e-05\n",
      "Epoch 70/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4083e-05 - val_loss: 1.0395e-04\n",
      "Epoch 71/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.3967e-05 - val_loss: 9.6996e-05\n",
      "Epoch 72/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.3757e-05 - val_loss: 9.6349e-05\n",
      "Epoch 00072: early stopping\n",
      "0.00016102454797439628\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 0.0247 - val_loss: 0.0108\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 0.0018 - val_loss: 0.0038\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 5.5557e-04 - val_loss: 0.0034\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.0837e-04 - val_loss: 0.0034\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.7665e-04 - val_loss: 0.0033e\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 3.4966e-04 - val_loss: 0.0032\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 3.2390e-04 - val_loss: 0.0031\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.9895e-04 - val_loss: 0.0030\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.7371e-04 - val_loss: 0.0028\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 2.4778e-04 - val_loss: 0.0026\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1923e-04 - val_loss: 0.0025\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8726e-04 - val_loss: 0.0022\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5089e-04 - val_loss: 0.0019\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1298e-04 - val_loss: 0.0017\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 8.1875e-05 - val_loss: 0.0015\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.1904e-05 - val_loss: 0.0014\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.0397e-05 - val_loss: 0.0012\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.2815e-05 - val_loss: 0.0011\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.7655e-05 - val_loss: 0.0010\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.3542e-05 - val_loss: 9.6766e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.0732e-05 - val_loss: 8.8989e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.8534e-05 - val_loss: 8.3139e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.6910e-05 - val_loss: 7.4402e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5957e-05 - val_loss: 6.9115e-04\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.4902e-05 - val_loss: 6.4445e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.3823e-05 - val_loss: 5.9616e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.2729e-05 - val_loss: 5.6683e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.2437e-05 - val_loss: 5.1669e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1240e-05 - val_loss: 4.9098e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0908e-05 - val_loss: 4.5904e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9655e-05 - val_loss: 4.4045e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.9403e-05 - val_loss: 4.0817e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8718e-05 - val_loss: 3.8133e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.8306e-05 - val_loss: 3.6128e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7866e-05 - val_loss: 3.4476e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.7438e-05 - val_loss: 3.3033e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6856e-05 - val_loss: 3.1156e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6597e-05 - val_loss: 2.9431e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.6313e-05 - val_loss: 2.8272e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5820e-05 - val_loss: 2.6953e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5507e-05 - val_loss: 2.7574e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.5524e-05 - val_loss: 2.5104e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4843e-05 - val_loss: 2.4499e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4824e-05 - val_loss: 2.3207e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4491e-05 - val_loss: 2.2815e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4692e-05 - val_loss: 2.1811e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4407e-05 - val_loss: 2.1730e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3886e-05 - val_loss: 2.1157e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3755e-05 - val_loss: 2.0524e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3541e-05 - val_loss: 1.9446e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3431e-05 - val_loss: 1.9373e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3513e-05 - val_loss: 1.9032e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3413e-05 - val_loss: 1.8365e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2996e-05 - val_loss: 1.7963e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3054e-05 - val_loss: 1.7590e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2991e-05 - val_loss: 1.7615e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3032e-05 - val_loss: 1.6956e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2802e-05 - val_loss: 1.6922e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.2661e-05 - val_loss: 1.6565e-04\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2795e-05 - val_loss: 1.6120e-04\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2394e-05 - val_loss: 1.6021e-04\n",
      "Epoch 62/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2487e-05 - val_loss: 1.6315e-04\n",
      "Epoch 63/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.2556e-05 - val_loss: 1.5537e-04\n",
      "Epoch 64/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2260e-05 - val_loss: 1.5388e-04\n",
      "Epoch 65/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.2282e-05 - val_loss: 1.5175e-04\n",
      "Epoch 66/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2407e-05 - val_loss: 1.5070e-04\n",
      "Epoch 67/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1980e-05 - val_loss: 1.4932e-04\n",
      "Epoch 68/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2386e-05 - val_loss: 1.4748e-04\n",
      "Epoch 69/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1937e-05 - val_loss: 1.4664e-04\n",
      "Epoch 70/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1715e-05 - val_loss: 1.4390e-04\n",
      "Epoch 71/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1675e-05 - val_loss: 1.4142e-04\n",
      "Epoch 72/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.2278e-05 - val_loss: 1.4180e-04\n",
      "Epoch 73/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1651e-05 - val_loss: 1.4285e-04\n",
      "Epoch 74/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1723e-05 - val_loss: 1.4096e-04\n",
      "Epoch 75/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1829e-05 - val_loss: 1.3918e-04\n",
      "Epoch 76/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1437e-05 - val_loss: 1.3808e-04\n",
      "Epoch 77/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1969e-05 - val_loss: 1.3643e-04\n",
      "Epoch 78/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1206e-05 - val_loss: 1.3734e-04\n",
      "Epoch 79/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.1397e-05 - val_loss: 1.3730e-04\n",
      "Epoch 80/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1577e-05 - val_loss: 1.3464e-04\n",
      "Epoch 81/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1130e-05 - val_loss: 1.3358e-04\n",
      "Epoch 82/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1398e-05 - val_loss: 1.3345e-04\n",
      "Epoch 83/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1389e-05 - val_loss: 1.3482e-04\n",
      "Epoch 84/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.1267e-05 - val_loss: 1.3171e-04\n",
      "Epoch 85/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1385e-05 - val_loss: 1.3180e-04\n",
      "Epoch 86/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1184e-05 - val_loss: 1.3002e-04\n",
      "Epoch 87/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0903e-05 - val_loss: 1.2992e-04\n",
      "Epoch 88/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0738e-05 - val_loss: 1.2857e-04\n",
      "Epoch 89/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1233e-05 - val_loss: 1.2851e-04\n",
      "Epoch 90/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1061e-05 - val_loss: 1.2715e-04\n",
      "Epoch 91/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0795e-05 - val_loss: 1.2745e-04\n",
      "Epoch 92/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1404e-05 - val_loss: 1.2633e-04\n",
      "Epoch 93/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0821e-05 - val_loss: 1.2554e-04\n",
      "Epoch 94/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0977e-05 - val_loss: 1.2672e-04\n",
      "Epoch 95/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.1140e-05 - val_loss: 1.2562e-04\n",
      "Epoch 96/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.0702e-05 - val_loss: 1.2655e-04\n",
      "Epoch 00096: early stopping\n",
      "0.00020474789826125948\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 0.0203 - val_loss: 0.0042\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 6.5071e-04 - val_loss: 0.0036\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 4.5721e-04 - val_loss: 0.0033\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 3.8009e-04 - val_loss: 0.0029\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 3.1157e-04 - val_loss: 0.0025\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.5082e-04 - val_loss: 0.0022\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.9833e-04 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.5411e-04 - val_loss: 0.0016\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.2063e-04 - val_loss: 0.0014\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 9.7608e-05 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 8.2239e-05 - val_loss: 0.0011\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 7.0917e-05 - val_loss: 9.3727e-04\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 6.1870e-05 - val_loss: 8.0151e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 5.4466e-05 - val_loss: 6.9840e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.8930e-05 - val_loss: 5.8854e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.3901e-05 - val_loss: 5.0248e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 4.0006e-05 - val_loss: 4.3035e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.6568e-05 - val_loss: 3.7648e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.3766e-05 - val_loss: 3.2808e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.1705e-05 - val_loss: 2.8346e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.9537e-05 - val_loss: 2.4610e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 2.7884e-05 - val_loss: 2.2190e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.6568e-05 - val_loss: 1.9927e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.5250e-05 - val_loss: 1.7788e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.4205e-05 - val_loss: 1.6630e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.3165e-05 - val_loss: 1.5376e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.2824e-05 - val_loss: 1.4723e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.2100e-05 - val_loss: 1.3821e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1348e-05 - val_loss: 1.3279e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0845e-05 - val_loss: 1.3009e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.0952e-05 - val_loss: 1.2559e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9837e-05 - val_loss: 1.2407e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9662e-05 - val_loss: 1.2773e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.9281e-05 - val_loss: 1.1894e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.8974e-05 - val_loss: 1.1594e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.8460e-05 - val_loss: 1.1891e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.8449e-05 - val_loss: 1.2467e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.8701e-05 - val_loss: 1.1673e-04\n",
      "Epoch 00038: early stopping\n",
      "0.00015034105380655103\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 0.0112 - val_loss: 0.0032\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 7.9915e-04 - val_loss: 0.0030\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 5.7065e-04 - val_loss: 0.0028\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 4.2236e-04 - val_loss: 0.0026\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 3.0915e-04 - val_loss: 0.0023\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 2.0848e-04 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.3001e-04 - val_loss: 0.0015\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 9.6219e-05 - val_loss: 0.0013\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 3s 14us/step - loss: 8.0646e-05 - val_loss: 0.0012\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 7.1053e-05 - val_loss: 0.0012\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 6.3642e-05 - val_loss: 0.0011\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 5.7654e-05 - val_loss: 0.0010\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 5.1676e-05 - val_loss: 9.4405e-04\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 4.6149e-05 - val_loss: 8.7869e-04\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 4.1383e-05 - val_loss: 8.2261e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.6969e-05 - val_loss: 7.7679e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 3.2832e-05 - val_loss: 7.3202e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.9965e-05 - val_loss: 7.0691e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.7566e-05 - val_loss: 6.4901e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.5586e-05 - val_loss: 5.9937e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.3728e-05 - val_loss: 5.6585e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.2807e-05 - val_loss: 5.2052e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.1670e-05 - val_loss: 4.7465e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 2.0716e-05 - val_loss: 4.2152e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 2.0112e-05 - val_loss: 3.9094e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.9476e-05 - val_loss: 3.6339e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.8822e-05 - val_loss: 3.2613e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.8353e-05 - val_loss: 3.0513e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.7979e-05 - val_loss: 2.8567e-04\n",
      "Epoch 30/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.7351e-05 - val_loss: 2.7392e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7113e-05 - val_loss: 2.5022e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.6827e-05 - val_loss: 2.3341e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.7176e-05 - val_loss: 2.2699e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6709e-05 - val_loss: 2.1195e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6104e-05 - val_loss: 2.0658e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.6557e-05 - val_loss: 1.9674e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.6315e-05 - val_loss: 1.8726e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5841e-05 - val_loss: 1.8742e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5575e-05 - val_loss: 1.7633e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5783e-05 - val_loss: 1.7527e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5715e-05 - val_loss: 1.7334e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4953e-05 - val_loss: 1.6509e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5575e-05 - val_loss: 1.6579e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4852e-05 - val_loss: 1.6142e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4939e-05 - val_loss: 1.5660e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5149e-05 - val_loss: 1.5629e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4687e-05 - val_loss: 1.5303e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4708e-05 - val_loss: 1.4804e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5022e-05 - val_loss: 1.4682e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4729e-05 - val_loss: 1.4818e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.4348e-05 - val_loss: 1.4411e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4258e-05 - val_loss: 1.4042e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.4348e-05 - val_loss: 1.4180e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.4074e-05 - val_loss: 1.4097e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.4177e-05 - val_loss: 1.3764e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.3691e-05 - val_loss: 1.4170e-04\n",
      "Epoch 57/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.3976e-05 - val_loss: 1.3435e-04\n",
      "Epoch 58/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.3791e-05 - val_loss: 1.3026e-04\n",
      "Epoch 59/100\n",
      "229497/229497 [==============================] - 3s 12us/step - loss: 1.4026e-05 - val_loss: 1.3461e-04\n",
      "Epoch 60/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3832e-05 - val_loss: 1.3205e-04\n",
      "Epoch 61/100\n",
      "229497/229497 [==============================] - 3s 13us/step - loss: 1.3573e-05 - val_loss: 1.3310e-04\n",
      "Epoch 00061: early stopping\n",
      "0.00024761716939628617\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 0.0264 - val_loss: 0.0042\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 4.7179e-04 - val_loss: 0.0033\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 3.2886e-04 - val_loss: 0.0027\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.5815e-04 - val_loss: 0.0024\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.0925e-04 - val_loss: 0.0021e-0\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 1.7194e-04 - val_loss: 0.0019\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.4141e-04 - val_loss: 0.0018\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 4s 20us/step - loss: 1.1585e-04 - val_loss: 0.0016- ETA: 1\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 9.5273e-05 - val_loss: 0.0016\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 7.9698e-05 - val_loss: 0.0015\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 6.8150e-05 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 5.9772e-05 - val_loss: 0.0013- loss: 5.500 - ETA: 0s - loss: 5.8494\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 5.2977e-05 - val_loss: 0.0012\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 4.7871e-05 - val_loss: 0.0011e\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 4.2500e-05 - val_loss: 9.6636e-04\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 3.8384e-05 - val_loss: 8.4455e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 3.4987e-05 - val_loss: 7.5660e-04\n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 3.1753e-05 - val_loss: 6.3459e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.9507e-05 - val_loss: 5.6411e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.7411e-05 - val_loss: 4.9396e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.5773e-05 - val_loss: 4.3373e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.4628e-05 - val_loss: 3.6582e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.4146e-05 - val_loss: 3.2118e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.2530e-05 - val_loss: 2.9493e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.1494e-05 - val_loss: 2.8913e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.1175e-05 - val_loss: 2.4630e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.0636e-05 - val_loss: 2.3310e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 1.9860e-05 - val_loss: 2.1175e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.9533e-05 - val_loss: 2.0252e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.9064e-05 - val_loss: 1.9347e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.8707e-05 - val_loss: 1.8251e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.8334e-05 - val_loss: 1.7664e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.8965e-05 - val_loss: 1.7603e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.7802e-05 - val_loss: 1.6436e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.8089e-05 - val_loss: 1.6414e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.7142e-05 - val_loss: 1.5833e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.7764e-05 - val_loss: 1.5647e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.7484e-05 - val_loss: 1.5469e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.6888e-05 - val_loss: 1.4997e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.7039e-05 - val_loss: 1.5455e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.6226e-05 - val_loss: 1.4777e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.6555e-05 - val_loss: 1.4700e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.6654e-05 - val_loss: 1.4294e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.5956e-05 - val_loss: 1.4251e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5697e-05 - val_loss: 1.3945e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.5795e-05 - val_loss: 1.4087e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5674e-05 - val_loss: 1.3912e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5568e-05 - val_loss: 1.3868e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5091e-05 - val_loss: 1.3767e-04\n",
      "Epoch 50/100\n",
      "229497/229497 [==============================] - 4s 18us/step - loss: 1.5453e-05 - val_loss: 1.3520e-04\n",
      "Epoch 51/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.5014e-05 - val_loss: 1.3682e-04\n",
      "Epoch 52/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.5060e-05 - val_loss: 1.4166e-04\n",
      "Epoch 53/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.4737e-05 - val_loss: 1.3322e-04\n",
      "Epoch 54/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.5148e-05 - val_loss: 1.3507e-04\n",
      "Epoch 55/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.4618e-05 - val_loss: 1.3404e-04\n",
      "Epoch 56/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.4853e-05 - val_loss: 1.3686e-04\n",
      "Epoch 00056: early stopping\n",
      "0.00019216613278740855\n",
      "Train on 229497 samples, validate on 123576 samples\n",
      "Epoch 1/100\n",
      "229497/229497 [==============================] - 5s 23us/step - loss: 0.0390 - val_loss: 0.0060\n",
      "Epoch 2/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 8.1555e-04 - val_loss: 0.0055\n",
      "Epoch 3/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 6.1484e-04 - val_loss: 0.0046\n",
      "Epoch 4/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 4.8474e-04 - val_loss: 0.0037\n",
      "Epoch 5/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 3.8337e-04 - val_loss: 0.0030\n",
      "Epoch 6/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 3.0288e-04 - val_loss: 0.0024\n",
      "Epoch 7/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.3631e-04 - val_loss: 0.0020\n",
      "Epoch 8/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.8029e-04 - val_loss: 0.0017\n",
      "Epoch 9/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 1.3544e-04 - val_loss: 0.0015\n",
      "Epoch 10/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 1.0405e-04 - val_loss: 0.0015\n",
      "Epoch 11/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 8.3967e-05 - val_loss: 0.0014\n",
      "Epoch 12/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 7.1153e-05 - val_loss: 0.0013\n",
      "Epoch 13/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 6.1922e-05 - val_loss: 0.0012\n",
      "Epoch 14/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 5.5273e-05 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 4.9408e-05 - val_loss: 0.0010\n",
      "Epoch 16/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 4.4832e-05 - val_loss: 9.2570e-04\n",
      "Epoch 17/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 4.0741e-05 - val_loss: 8.2219e-04 ETA: 2s - los - \n",
      "Epoch 18/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 3.6975e-05 - val_loss: 7.2089e-04\n",
      "Epoch 19/100\n",
      "229497/229497 [==============================] - 5s 22us/step - loss: 3.3940e-05 - val_loss: 6.4768e-04\n",
      "Epoch 20/100\n",
      "229497/229497 [==============================] - 5s 22us/step - loss: 3.1427e-05 - val_loss: 5.4318e-04\n",
      "Epoch 21/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 2.9798e-05 - val_loss: 5.0552e-04\n",
      "Epoch 22/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 2.7691e-05 - val_loss: 4.2830e-04\n",
      "Epoch 23/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 2.6095e-05 - val_loss: 3.6682e-04\n",
      "Epoch 24/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.4962e-05 - val_loss: 3.2286e-04\n",
      "Epoch 25/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.3888e-05 - val_loss: 2.7849e-04\n",
      "Epoch 26/100\n",
      "229497/229497 [==============================] - 5s 21us/step - loss: 2.3232e-05 - val_loss: 2.5771e-04\n",
      "Epoch 27/100\n",
      "229497/229497 [==============================] - 5s 20us/step - loss: 2.2105e-05 - val_loss: 2.3991e-04\n",
      "Epoch 28/100\n",
      "229497/229497 [==============================] - 4s 19us/step - loss: 2.1575e-05 - val_loss: 2.2406e-04\n",
      "Epoch 29/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 2.0881e-05 - val_loss: 2.3957e-04\n",
      "Epoch 30/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 2.0401e-05 - val_loss: 2.1644e-04\n",
      "Epoch 31/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.9405e-05 - val_loss: 1.9635e-04\n",
      "Epoch 32/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.8968e-05 - val_loss: 1.8598e-04\n",
      "Epoch 33/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.8961e-05 - val_loss: 1.8106e-04\n",
      "Epoch 34/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.8257e-05 - val_loss: 1.6243e-04\n",
      "Epoch 35/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.7596e-05 - val_loss: 1.5996e-04\n",
      "Epoch 36/100\n",
      "229497/229497 [==============================] - 3s 15us/step - loss: 1.7293e-05 - val_loss: 1.5554e-04\n",
      "Epoch 37/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.7741e-05 - val_loss: 1.4380e-04\n",
      "Epoch 38/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.6659e-05 - val_loss: 1.4411e-04\n",
      "Epoch 39/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.7246e-05 - val_loss: 1.3493e-04\n",
      "Epoch 40/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.6375e-05 - val_loss: 1.3491e-04\n",
      "Epoch 41/100\n",
      "229497/229497 [==============================] - 4s 15us/step - loss: 1.6146e-05 - val_loss: 1.3115e-04\n",
      "Epoch 42/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.6195e-05 - val_loss: 1.2933e-04\n",
      "Epoch 43/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5975e-05 - val_loss: 1.2205e-04\n",
      "Epoch 44/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5610e-05 - val_loss: 1.2250e-04\n",
      "Epoch 45/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.5052e-05 - val_loss: 1.2051e-04\n",
      "Epoch 46/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.5494e-05 - val_loss: 1.2114e-04\n",
      "Epoch 47/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.4976e-05 - val_loss: 1.1903e-04\n",
      "Epoch 48/100\n",
      "229497/229497 [==============================] - 4s 16us/step - loss: 1.4829e-05 - val_loss: 1.1659e-04\n",
      "Epoch 49/100\n",
      "229497/229497 [==============================] - 4s 17us/step - loss: 1.4669e-05 - val_loss: 1.3617e-04\n",
      "Epoch 50/100\n",
      "228000/229497 [============================>.] - ETA: 0s - loss: 1.5242e-05"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log_name = datetime.now().strftime('LSTMlogs\\\\lstm_5_1layer_%Y_%m_%d_%H.%M.%S.log')\n",
    "\n",
    "structures = [[4], [8], [12], [16], [24], [28], [32]]\n",
    "dropouts = []\n",
    "rec_dropouts = []\n",
    "\n",
    "for struc in structures:\n",
    "    el = []\n",
    "    for els in struc:\n",
    "        el.append(0)\n",
    "    dropouts.append(el)\n",
    "    rec_dropouts.append(el)\n",
    "    \n",
    "model = multi_train_LSTM(structure_list = structures\n",
    "                         ,patience      = 3\n",
    "                         ,dropouts      = dropouts\n",
    "                         ,rec_dropouts  = rec_dropouts\n",
    "                         ,epochs        = 100\n",
    "                         ,batch_size    = 1000\n",
    "                         ,train_single  = 5\n",
    "                         ,train_X       = Xs\n",
    "                         ,train_Y       = Ys\n",
    "                         ,test_X        = TXs\n",
    "                         ,test_Y        = TYs\n",
    "                         ,set_num       = 9\n",
    "                         ,criterion     = 'mse'\n",
    "                         ,log_name      = log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.521166671500649e-05\n",
      "0.00017682079836724558\n",
      "1.1646836074795547e-05\n",
      "0.00017286302088245816\n",
      "0.00024863666264679263\n",
      "0.00010976100171061029\n",
      "0.00025113245052671743\n",
      "2.735436431136019e-05\n",
      "0.0001351544051633492\n",
      "0.00029261651827685664\n",
      "0.0005126433756900264\n",
      "0.0002557176747446508\n",
      "0.00023340904721880286\n",
      "4.3539524558956775e-05\n",
      "0.0006179225758162637\n",
      "0.0004054456725002084\n"
     ]
    }
   ],
   "source": [
    "col = 0\n",
    "for col in range(0, len(TXs)-0):\n",
    "    plt.figure(col)\n",
    "    pred = model.predict(TXs[col])\n",
    "    eval_prediction(pred, TYs[col], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTM7\\\\model_LSTM_6_4_2020-5-23.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = load_timeseries(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import dill\n",
    "\n",
    "path = \"LSTM12\\\\\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.pkl\")) #make list of paths\n",
    "\n",
    "models = []\n",
    "for spk in all_files:\n",
    "    with open(spk, 'rb') as f:\n",
    "        model = dill.load(f)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_model(\n",
    "    models_list\n",
    "    ,test_input\n",
    "    ,test_target\n",
    "    ,log_name=None\n",
    "):\n",
    "    best_score = 16\n",
    "    best_model = None\n",
    "    idx = 0\n",
    "    best_idx = 0\n",
    "    \n",
    "    for model in models_list:\n",
    "        print2(f'Evaluating model with index: {idx}', log_name)\n",
    "        \n",
    "        sum_score = 0\n",
    "        \n",
    "        for scenario_id in range(0, len(test_input)):\n",
    "            y = model.predict(test_input[scenario_id])\n",
    "            curr_score = eval_prediction(y, test_target[scenario_id], 0)\n",
    "            print2(f'Score on scenario {scenario_id}: {curr_score}', log_name)\n",
    "            sum_score += curr_score\n",
    "            \n",
    "        \n",
    "        print2(f'General score of the model: {sum_score}', log_name)\n",
    "        if sum_score < best_score:\n",
    "            best_model = model\n",
    "            best_score = sum_score\n",
    "            best_idx   = idx\n",
    "        \n",
    "        idx += 1\n",
    "        \n",
    "    print2(f\"Best's model index: {best_idx}, with score: {best_score}\", log_name)\n",
    "    print2(best_model.summary(), log_name)\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3713434862484437e-05\n",
      "3.7952588531729344e-05\n",
      "5.948403410912946e-06\n",
      "4.31724221207634e-05\n",
      "0.00014259768919608259\n",
      "5.553954736421614e-05\n",
      "4.166898212316852e-05\n",
      "8.055561247493374e-06\n",
      "1.7787179736332105e-05\n",
      "6.804808927461771e-05\n",
      "1.2413196294227974e-05\n",
      "4.6941130980104465e-05\n",
      "2.045155853209337e-05\n",
      "1.1008365308196802e-05\n",
      "9.296323161107965e-05\n",
      "0.0002886091466188648\n",
      "2.2222800308592775e-05\n",
      "6.500390158666297e-05\n",
      "1.5355710279881956e-05\n",
      "8.684121117404969e-05\n",
      "0.0003024445578092628\n",
      "0.0001214729338846148\n",
      "9.035978889205687e-05\n",
      "1.8712126694143125e-05\n",
      "3.495408690412918e-05\n",
      "8.33725342683657e-05\n",
      "3.324980940571896e-05\n",
      "1.7584367665259274e-05\n",
      "2.37897458278519e-05\n",
      "3.337383702627487e-05\n",
      "0.00015655713880933776\n",
      "0.00018588537816194062\n",
      "1.1870224969288246e-05\n",
      "2.0153692024736843e-05\n",
      "5.546766362176752e-06\n",
      "1.5291880947640695e-05\n",
      "7.599770789959532e-05\n",
      "2.0974918994878747e-05\n",
      "2.6794520710773915e-05\n",
      "1.0539595537691976e-05\n",
      "2.864262407126824e-05\n",
      "7.733688705016717e-05\n",
      "5.236439393484888e-05\n",
      "8.352931555491308e-05\n",
      "3.652885253269281e-05\n",
      "1.0397572833034874e-05\n",
      "4.805025042091112e-05\n",
      "0.00022790917180802548\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_9 (Masking)          (None, 12, 5)             0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 6)                 288       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 295\n",
      "Trainable params: 295\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "log_name = datetime.now().strftime('evaluation\\\\LSTM12_eval_%Y_%m_%d_%H.%M.%S.log')\n",
    "\n",
    "best_model = pick_best_model(\n",
    "    models_list  = models\n",
    "    ,test_input  = TXs\n",
    "    ,test_target = TYs\n",
    "    ,log_name    = log_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_latex(\n",
    "    model\n",
    "    ,test_data\n",
    "    ,multiplier = 1\n",
    "    ,log_name=None\n",
    "):\n",
    "    import time\n",
    "    from sklearn.metrics import max_error as merror\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    \n",
    "    score_mse    = []\n",
    "    score_merror = []\n",
    "    sim_times    = []\n",
    "    \n",
    "    for i in range(0, len(test_data)):\n",
    "        \n",
    "        t_start = time.time()\n",
    "        y = sim(model, test_data, i)\n",
    "        t_stop = time.time()\n",
    "        \n",
    "        sim_times.append(round(t_stop - t_start, 2))\n",
    "        score_mse.append(round(mse(y[:, 0], test_data[i][:,-1])*multiplier,2))\n",
    "        score_merror.append(round(abs(merror(y[:, 0], test_data[i][:, -1])),3))\n",
    "        \n",
    "    str1 = \"Numer scenariusza\"\n",
    "    str2 = \"MSE $\\\\cdot 10^{-4}$\"\n",
    "    str3 = \"max error\"\n",
    "    str4 = \"Czas symulacji [s]\"\n",
    "    \n",
    "    if len(test_data) > 8:\n",
    "        end = 8\n",
    "    else:\n",
    "        end = len(test_data)\n",
    "        \n",
    "    for i in range(0, end):\n",
    "        str1 += f\"& {i}\"\n",
    "        str2 += f'& {score_mse[i]}'\n",
    "        str3 += f'& {score_merror[i]}'\n",
    "        str4 += f'& {sim_times[i]}'\n",
    "        \n",
    "    str1 += '\\\\\\\\\\\\hline'\n",
    "    str2 += '\\\\\\\\\\\\hline'\n",
    "    str3 += '\\\\\\\\\\\\hline'\n",
    "    str4 += '\\\\\\\\\\\\hline'\n",
    "    \n",
    "    print2(str1,log_name)\n",
    "    print2(str2,log_name)\n",
    "    print2(str3,log_name)\n",
    "    print2(str4,log_name)\n",
    "    \n",
    "    str1 = \"Numer scenariusza\"\n",
    "    str2 = \"MSE $\\\\cdot 10^{-4}$\"\n",
    "    str3 = \"max error\"\n",
    "    str4 = \"Czas symulacji [s]\"\n",
    "    \n",
    "    for i in range(8, len(test_data)):\n",
    "        str1 += f\"& {i}\"\n",
    "        str2 += f'& {score_mse[i]}'\n",
    "        str3 += f'& {score_merror[i]}'\n",
    "        str4 += f'& {sim_times[i]}'\n",
    "        \n",
    "    str1 += '\\\\\\\\\\\\hline'\n",
    "    str2 += '\\\\\\\\\\\\hline'\n",
    "    str3 += '\\\\\\\\\\\\hline'\n",
    "    str4 += '\\\\\\\\\\\\hline'\n",
    "    \n",
    "    print2('', log_name)\n",
    "    print2(str1,log_name)\n",
    "    print2(str2,log_name)\n",
    "    print2(str3,log_name)\n",
    "    print2(str4,log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "datetime.now()\n",
    "log_name = datetime.now().strftime('evaluation\\\\SVR_surge_%Y_%m_%d_%H.%M.%S.log')\n",
    "\n",
    "with open('model_svr.pkl', 'rb') as f:\n",
    "    model = dill.load(f)\n",
    "    \n",
    "evaluate_model_latex(\n",
    "    model = model\n",
    "#     ,test_data = test_set_trim\n",
    "    ,test_data = surge_trim\n",
    "    ,multiplier = 1e4\n",
    "    ,log_name = log_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas symulacji scenariusza 4: 29.929999828338623s\n",
      "0.0003619899224666365\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0003619899224666365"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "col = 4\n",
    "t_start = time.time()\n",
    "y = sim(model, test_set_trim, col)\n",
    "t_stop = time.time()\n",
    "print2(f'Czas symulacji scenariusza {col}: {t_stop-t_start}s')\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czas symulacji scenariusza 6: 34.343122720718384s\n",
      "0.018107290910973357\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.018107290910973357"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:106: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  app.exec_()\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:106: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  app.exec_()\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "col = 6\n",
    "t_start = time.time()\n",
    "y = sim(model, surge_trim, col)\n",
    "t_stop = time.time()\n",
    "print2(f'Czas symulacji scenariusza {col}: {t_stop-t_start}s')\n",
    "eval_prediction(y, surge_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
