{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib Qt\n",
    "manager = plt.get_current_fig_manager()\n",
    "manager.window.showMaximized()\n",
    "\n",
    "#init cell\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data.values[:, col_num], linewidth=0.5)\n",
    "\t\n",
    "#obserwujemy przebiegi sygnałów aby namierzyć outliery\n",
    "import matplotlib.pyplot as plotter\n",
    "def plot_column2(data, col_num):\n",
    "    %matplotlib qt\n",
    "    figManager = plotter.get_current_fig_manager()\n",
    "    figManager.window.showMaximized()\n",
    "    plotter.plot(data[:, col_num], linewidth=0.5)\n",
    "    \n",
    "\n",
    "import numpy as np\n",
    "def sim(model, test_set, set_num):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0:\n",
    "            sample = np.copy(test_set[set_num][0:1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "\n",
    "def sim_with_refreshes(model, test_set\n",
    "                       ,set_num, refr_period):\n",
    "    col = len(test_set[set_num][0])\n",
    "    y_serie = []\n",
    "    for i in range(0, len(test_set[set_num])):\n",
    "        if i == 0 or i%refr_period == 0:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            y = model.predict(sample)\n",
    "        else:\n",
    "            sample = np.copy(test_set[set_num][i:i+1, 0:col-1])\n",
    "            sample[0, -1] = y\n",
    "            y = model.predict(sample)\n",
    "        y_serie.append(y)\n",
    "    y_serie=np.array(y_serie)\n",
    "    return y_serie\n",
    "\n",
    "\n",
    "def eval_prediction(y_pred, y_target, do_plot):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    if do_plot == 1:\n",
    "\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(y_pred[:, 0], label='Prediction', lw=1)\n",
    "        plt.plot(y_target, label='Target', lw=1)\n",
    "        plt.legend()\n",
    "        plt.grid(b=True, which='both', axis='y')\n",
    "\n",
    "        residuals = np.copy(y_pred[:, 0])\n",
    "        for i in range(0, len(residuals)):\n",
    "            residuals[i] = residuals[i] - y_target[i]\n",
    "        residuals = residuals.flatten()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.fill_between(range(0, len(residuals))\n",
    "                             ,residuals, label='Residuals', lw=1)\n",
    "        plt.legend()\n",
    "        plt.grid(b=True, which='both', axis='y')\n",
    "        plt.show()\n",
    "    \n",
    "    score = mse(y_pred[:, 0], y_target)\n",
    "    print(score)\n",
    "    return score\n",
    "    \n",
    "def multi_train_NLP(structs_list, patience, batch_size, \n",
    "                    epochs, train_data, test_data, set_num\n",
    "                   ,criterion, single_times, dropout_2list):\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "    sys.stdout = open('log.txt', 'w')\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score=1\n",
    "    \n",
    "    struct_num = 0\n",
    "    for structure in structs_list:\n",
    "        print(f'Trenuje siec o struturze {structure}')\n",
    "        curr_model = train_MLP_Ntimes(N=single_times\n",
    "                                     ,structure=structure\n",
    "                                     ,patience=patience\n",
    "                                     ,batch_size=batch_size\n",
    "                                     ,epochs=epochs\n",
    "                                     ,set_num=set_num\n",
    "                                     ,train_data=train_data\n",
    "                                     ,test_data=test_data\n",
    "                                     ,debug_log='no'\n",
    "                                     ,criterion=criterion\n",
    "                                     ,dropout_list=dropout_2list[struct_num])\n",
    "    \n",
    "        y = sim(curr_model, test_data, set_num)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            curr_score = mse(y[:, 0], test_data[set_num][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for i in range(0, len(residuals)):\n",
    "                residuals[i] = residuals[i] - test_data[set_num][i, -1]\n",
    "            curr_score = np.max(residuals)      \n",
    "            \n",
    "        print(f'Wynik biezącej sieci: {curr_score}')\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_model = curr_model\n",
    "            best_score = curr_score\n",
    "            \n",
    "        struct_num = struct_num + 1\n",
    "    sys.stdout = bck_stdout\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    return(best_model)\n",
    "\n",
    "def train_MLP_Ntimes(N, structure, patience, batch_size, epochs\n",
    "                    ,train_data, test_data, set_num\n",
    "                    ,debug_log, criterion, dropout_list):\n",
    "    best_score = 1\n",
    "    nets_trained = 0\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        import sys\n",
    "        bck_stdout = sys.stdout\n",
    "        sys.stdout = open('log.txt', 'w')\n",
    "    \n",
    "    while nets_trained < N:\n",
    "        model = train_NLP(structure=structure\n",
    "                         ,patience=patience\n",
    "                         ,batch_size=batch_size\n",
    "                         ,epochs=epochs\n",
    "                         ,train_data=train_data\n",
    "                         ,dropout_list=dropout_list)\n",
    "        y = sim(model, test_data, set_num)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = eval_prediction(y, test_set[set_num][:, -1], 0)\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_data[set_num][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "        print(f\"Wynik sieci nr {nets_trained+1}: {score}\")\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "        del model\n",
    "        nets_trained = nets_trained + 1\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'sound.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        sys.stdout = bck_stdout\n",
    "    return best_model\n",
    "\n",
    "def train_NLP(structure, patience,batch_size\n",
    "              ,epochs, train_data, dropout_list):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, Dropout\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    from keras import regularizers\n",
    "    from keras.optimizers import SGD\n",
    "    \n",
    "    inputs_count = len(train_data[0]) - 1\n",
    "    layer_num = 1\n",
    "    model = Sequential()\n",
    "    \n",
    "    for layer_size in structure:\n",
    "        if layer_num == 1:\n",
    "            model.add(Dense(layer_size, input_dim=inputs_count, \n",
    "                            activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "            \n",
    "        else:\n",
    "            model.add(Dense(layer_size, activation='tanh'))\n",
    "            model.add(Dropout(dropout_list[layer_num - 1]))\n",
    "            \n",
    "        layer_num = layer_num + 1\n",
    "    model.add(Dense(1, activation = 'linear'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', \n",
    "                   verbose=1, patience=patience)\n",
    "\n",
    "    model.fit(train_data[:, :-1],\n",
    "              train_data[:, -1], batch_size=batch_size,\n",
    "              epochs=epochs, validation_split = 0.35,\n",
    "              callbacks=[es], workers=8, use_multiprocessing=1)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_SVR(C, epsilon, train_data):\n",
    "    from sklearn.svm import SVR as svr\n",
    "    model = svr(kernel='rbf', gamma='scale', C=C, epsilon=epsilon)\n",
    "    model.fit(train_data[:, :-1], train_data[:, -1])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def multi_train_SVR(C_eps_pairs\n",
    "                   ,train_data\n",
    "                   ,test_data\n",
    "                   ,set_num\n",
    "                   ,criterion):\n",
    "    import sys\n",
    "    bck_stdout = sys.stdout\n",
    "   # sys.stdout = open('log.txt', 'w')\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error as mse \n",
    "    best_score = 1\n",
    "\n",
    "    for pair in C_eps_pairs:\n",
    "        print(f'Trenuje SVR o C={pair[0]} i epsilon={pair[1]}')\n",
    "        curr_model = train_SVR(C=pair[0]\n",
    "                               ,epsilon=pair[1]\n",
    "                               ,train_data=train_data)\n",
    "    \n",
    "        y = sim(curr_model, test_data, set_num)\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            curr_score = mse(y[:, 0], test_data[set_num][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for i in range(0, len(residuals)):\n",
    "                residuals[i] = residuals[i] - test_data[set_num][i, -1]\n",
    "            curr_score = np.max(residuals)\n",
    "        \n",
    "    \n",
    "        print(f'Wynik biezacego SVRa: {curr_score}')\n",
    "    \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "\n",
    "    sys.stdout = bck_stdout  \n",
    "    return best_model\n",
    "\n",
    "def create_dataset(X, y, time_steps=1):\n",
    "    #based on tutorial:\n",
    "    #https://towardsdatascience.com/\n",
    "    #time-series-forecasting-with-lstms-using-tensorflow-2-and-keras-in-python-6ceee9c6c651\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        v = X.iloc[i:(i + time_steps)].values\n",
    "        Xs.append(v)\n",
    "        ys.append(y.iloc[i + time_steps])\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "\n",
    "    \n",
    "def prepare_timeseries(train_set, test_set_list, prepare):\n",
    "    pd_X = pd.DataFrame(data=train_set[:, :-1])\n",
    "    pd_Y = pd.DataFrame(data=train_set[:, -1])\n",
    "\n",
    "    pd_TX = []\n",
    "    pd_TY = []\n",
    "\n",
    "    for test in test_set_list:\n",
    "        pd_TX.append(pd.DataFrame(test[:, :-1]))\n",
    "        pd_TY.append(pd.DataFrame(test[:, -1]))\n",
    "\n",
    "    print(f'Sekwencjonuje zbior uczacy')\n",
    "    Xs, Ys = create_dataset(pd_X, pd_Y, prepare)\n",
    "\n",
    "    TXs = []\n",
    "    TYs = []\n",
    "    \n",
    "    for i in range(0, len(pd_TX)):\n",
    "        print(f'Sekswencjonuje zbior testowy nr: {i}')\n",
    "        txs, tys = create_dataset(pd_TX[i], pd_TY[i], prepare)\n",
    "        TXs.append(txs)\n",
    "        TYs.append(tys)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs\n",
    "\n",
    "\n",
    "def prepare_surge(test_set_list, prepare):\n",
    "    pd_TX = []\n",
    "    pd_TY = []\n",
    "\n",
    "    for test in test_set_list:\n",
    "        pd_TX.append(pd.DataFrame(test[:, :-1]))\n",
    "        pd_TY.append(pd.DataFrame(test[:, -1]))\n",
    "\n",
    "    TXs = []\n",
    "    TYs = []\n",
    "    \n",
    "    for i in range(0, len(pd_TX)):\n",
    "        print(f'Sekswencjonuje zbior testowy nr: {i}')\n",
    "        txs, tys = create_dataset(pd_TX[i], pd_TY[i], prepare)\n",
    "        TXs.append(txs)\n",
    "        TYs.append(tys)\n",
    "        \n",
    "    return TXs, TYs\n",
    "\n",
    "\n",
    "def train_LSTM_ntimes(structure, patience\n",
    "                     ,epochs, batch_size\n",
    "                     ,dropouts, rec_dropouts, N, debug_log\n",
    "                     ,train_X, train_Y, set_num\n",
    "                     ,test_X, test_Y, criterion):\n",
    "    best_score = 1\n",
    "    nets_trained = 0\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        import sys\n",
    "        bck_stdout = sys.stdout\n",
    "        sys.stdout = open('log.txt', 'w')\n",
    "    \n",
    "    while nets_trained < N:\n",
    "        model = train_LSTM(structure=structure, patience=3\n",
    "                          ,epochs=epochs, batch_size=batch_size\n",
    "                          ,dropouts=dropouts, rec_dropouts=rec_dropouts\n",
    "                          ,train_X=Xs, train_Y=Ys)\n",
    "        \n",
    "        y = model.predict(test_X[set_num])\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = eval_prediction(y, test_Y[set_num][:, -1], 0)\n",
    "            \n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(y[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_data[set_num][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "            \n",
    "        print(f\"Wynik sieci nr {nets_trained+1}: {score}\")\n",
    "        \n",
    "        if score < best_score:\n",
    "            best_score = score\n",
    "            best_model = model\n",
    "        \n",
    "        del model\n",
    "        nets_trained = nets_trained + 1\n",
    "    \n",
    "    import winsound\n",
    "    filename = 'sound.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        sys.stdout = bck_stdout\n",
    "    return best_model\n",
    "\n",
    "\n",
    "def train_LSTM(structure, patience\n",
    "              ,epochs, batch_size\n",
    "              ,dropouts, rec_dropouts\n",
    "              ,train_X, train_Y):\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Activation, LSTM, Dropout\n",
    "    from keras.layers import Activation\n",
    "    from keras.layers import LSTM\n",
    "    from keras.layers import Masking\n",
    "    from keras.callbacks import EarlyStopping    \n",
    "    \n",
    "    model = Sequential()\n",
    "    for i in range(0, len(structure)):\n",
    "        if i == 0:\n",
    "            model.add(Masking(mask_value=-10\n",
    "                             ,input_shape=(train_X.shape[1], train_X.shape[2])))\n",
    "            \n",
    "            model.add(LSTM(units=structure[i]\n",
    "                          ,activation='tanh'\n",
    "                          ,recurrent_activation='sigmoid'\n",
    "                          ,dropout=dropouts[i]\n",
    "                          ,recurrent_dropout=rec_dropouts[i]\n",
    "                          ,return_sequences=False))\n",
    "        else:\n",
    "            model.add(Dense(units=structure[i]\n",
    "                           ,activation='tanh'))\n",
    "            model.add(Dropout(dropouts[i]))\n",
    "                      \n",
    "    model.add(Dense(units=1, activation='linear'))\n",
    "    model.compile(loss='mean_squared_error'\n",
    "                  ,optimizer='adam')\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min'\n",
    "                      ,verbose=1, patience=patience)\n",
    "    \n",
    "    model.fit(train_X, train_Y, epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_split=0.35,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              callbacks=[es])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def multi_train_LSTM(structure_list, patience\n",
    "                    ,dropouts, rec_dropouts\n",
    "                    ,epochs, batch_size, train_single\n",
    "                    ,train_X, train_Y, debug_log\n",
    "                    ,test_X, test_Y, set_num\n",
    "                    ,criterion):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        import sys\n",
    "        bck_stdout = sys.stdout\n",
    "        sys.stdout = open('log.txt', 'w')\n",
    "    \n",
    "    best_score = 1\n",
    "    \n",
    "    for i in range(0, len(structure_list)):\n",
    "        print(f'Trenuje siec o strukturze: {structure_list[i]}')\n",
    "        curr_model = train_LSTM_ntimes(\n",
    "            structure=structure_list[i], patience=patience\n",
    "            ,epochs=epochs, batch_size=batch_size\n",
    "            ,rec_dropouts=rec_dropouts[i]\n",
    "            ,dropouts=dropouts[i], N=train_single, debug_log='no'\n",
    "            ,train_X=train_X, train_Y=train_Y, set_num=set_num\n",
    "            ,test_X=test_X, test_Y=test_Y, criterion='mse')\n",
    "        \n",
    "        pred = curr_model.predict(test_X[set_num])\n",
    "        curr_score = mse(pred[:, 0], test_Y[set_num])\n",
    "        print(f'Wynik biezacej sieci: {curr_score}')\n",
    "        \n",
    "        if curr_score < best_score:\n",
    "            best_score = curr_score\n",
    "            best_model = curr_model\n",
    "            \n",
    "    import winsound\n",
    "    filename = 'jasny chuj.wav'\n",
    "    winsound.PlaySound(filename, winsound.SND_FILENAME)\n",
    "    \n",
    "    if debug_log == 'yes':\n",
    "        sys.stdout = bck_stdout\n",
    "    \n",
    "    return best_model\n",
    "\n",
    "def generate_SVR_params(C_min, C_max, C_step\n",
    "                       ,eps_min, eps_max, eps_step):\n",
    "    C_max = C_max + C_step\n",
    "    C_vals = np.arange(C_min, C_max, C_step)\n",
    "    \n",
    "    eps_max = eps_max + eps_step\n",
    "    eps_vals = np.arange(eps_min, eps_max, eps_step)\n",
    "    \n",
    "    params_list = []\n",
    "    for C in C_vals:\n",
    "        for eps in eps_vals:\n",
    "            params_list.append([C, eps])\n",
    "    \n",
    "    return params_list\n",
    "\n",
    "def try_add_column(cols_taken, test_col, criterion, train_set\n",
    "                  ,test_set, model_type):\n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    col = len(train_set[0])\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, col):\n",
    "        if i not in cols_taken:\n",
    "            cols_test = [i]\n",
    "            for _col in cols_taken:\n",
    "                cols_test.append(_col)\n",
    "\n",
    "            train_set_trim = train_set[:, cols_test]\n",
    "            test_set_trim = []\n",
    "            for test in test_set:\n",
    "                test_set_trim.append(test[:, cols_test])\n",
    "            \n",
    "            if model_type == 'SVR':\n",
    "                model = train_SVR(C=0.4, epsilon=0.014\n",
    "                                  ,train_data=train_set_trim)\n",
    "            \n",
    "            pred = sim(model, test_set_trim, test_col)\n",
    "            \n",
    "            if criterion == 'mse':\n",
    "                score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "            if criterion == 'max':\n",
    "                residuals = np.copy(pred[:, 0])\n",
    "                for j in range(0, len(residuals)):\n",
    "                    residuals[j] = residuals[j] - \\\n",
    "                    test_set_trim[test_col][j, -1]\n",
    "                score = np.max(np.abs(residuals))\n",
    "\n",
    "            if score < best_score:\n",
    "                best_model = model\n",
    "                best_score = score\n",
    "                best_col = i\n",
    "\n",
    "            print(cols_test)\n",
    "            print(f'Test kolumny {i}: wynik: {score}')\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "    \n",
    "def try_drop_column(start_cols, test_col, criterion\n",
    "                   ,train_set ,test_set, model_type):\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error as mse\n",
    "    best_score = 1\n",
    "    best_col = 0\n",
    "\n",
    "    for i in range(0, len(base_cols)):\n",
    "        cols_test = np.copy(base_cols)\n",
    "        cols_test = np.delete(base_cols, i)\n",
    "\n",
    "        train_set_trim = train_set[:, cols_test]\n",
    "        test_set_trim = []\n",
    "        for test in test_set:\n",
    "            test_set_trim.append(test[:, cols_test])\n",
    "        \n",
    "        if model_type == 'SVR':\n",
    "            model = train_SVR(C=0.4, epsilon=0.014\n",
    "                              ,train_data=train_set_trim)\n",
    "        \n",
    "        pred = sim(model, test_set_trim, test_col)\n",
    "        \n",
    "        if criterion == 'mse':\n",
    "            score = mse(pred[:, 0], test_set_trim[test_col][:, -1])\n",
    "        if criterion == 'max':\n",
    "            residuals = np.copy(pred[:, 0])\n",
    "            for j in range(0, len(residuals)):\n",
    "                residuals[j] = residuals[j] - \\\n",
    "                test_set_trim[test_col][j, -1]\n",
    "            score = np.max(np.abs(residuals))\n",
    "\n",
    "        if score < best_score:\n",
    "            best_model = model\n",
    "            best_score = score\n",
    "            best_col = i\n",
    "\n",
    "        print(cols_test)\n",
    "        print(f'Test kolumny {base_cols[i]}: wynik: {score}')\n",
    "\n",
    "    print(f'Najlepsza kolumna to: {best_col} z wynikiem: {best_score}')\n",
    "\n",
    "def dump_timeseries(Xs, Ys, TXs, TYs, seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Xs,f)\n",
    "\n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(Ys,f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TXs,f)\n",
    "\n",
    "    with open(f'ts_trim_test_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(TYs,f)\n",
    "        \n",
    "        \n",
    "def dump_surges(SXs, SYs, seq_len):\n",
    "    with open(f'ts_surge_in_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(SXs,f)\n",
    "\n",
    "    with open(f'ts_surge_out_{seq_len}.pkl', 'wb') as f:\n",
    "        dill.dump(SYs,f)\n",
    "        \n",
    "        \n",
    "def load_timeseries(seq_len):\n",
    "    with open(f'ts_trim_train_in_{seq_len}.pkl', 'rb') as f:\n",
    "        Xs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_train_out_{seq_len}.pkl', 'rb') as f:\n",
    "        Ys = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_in_{seq_len}.pkl', 'rb') as f:\n",
    "        TXs = dill.load(f)\n",
    "        \n",
    "    with open(f'ts_trim_test_out_{seq_len}.pkl', 'rb') as f:\n",
    "        TYs = dill.load(f)\n",
    "        \n",
    "    return Xs, Ys, TXs, TYs\n",
    "\n",
    "\n",
    "def load_surges(seq_len):\n",
    "    with open(f'ts_surge_in_{seq_len}.pkl', 'rb') as f:\n",
    "        SXs = dill.load(f)\n",
    "\n",
    "    with open(f'ts_surge_out_{seq_len}.pkl', 'rb') as f:\n",
    "        SYs = dill.load(f)\n",
    "    \n",
    "    return SXs, SYs\n",
    "\n",
    "\n",
    "import sys\n",
    "bckStdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "    \n",
    "with open('train_set.pkl', 'rb') as  f:\n",
    "    train_set = dill.load(f)\n",
    "    \n",
    "with open('test_set.pkl', 'rb') as f:\n",
    "    test_set = dill.load(f)\n",
    "    \n",
    "with open('surge_test.pkl', 'rb') as  f:\n",
    "    test_set_surge = dill.load(f)\n",
    "    \n",
    "with open('vdips_test.pkl', 'rb') as f:\n",
    "    test_set_vdips = dill.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fine_all.pkl','rb') as f:\n",
    "    fine_all = dill.load(f)\n",
    "    \n",
    "with open('surge_all.pkl','rb') as f:\n",
    "    surge_all = dill.load(f)\n",
    "    \n",
    "with open('vdips_all.pkl','rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wczytanie danych z normalnej pracy\n",
    "#okreslenie poprawności pracy na podstawie dołączonego arkusza\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tLICZBA PRÓBEK\n",
    "# \t02/03/2017\t\t0-2\t\t\t\t397721\n",
    "# \t06/03/2017\t\t3\t\t\t\t48513\n",
    "# \t16/03/2017\t\t4\t\t\t\t518348\n",
    "# \t22/03/2017\t\t5\t\t\t\t644888\n",
    "# \t23/03/2017\t\t6\t\t\t\t408255\n",
    "# \t29/03/2017\t\t7-8\t\t\t\t517850\n",
    "# \t13/04/2017\t\t9-13\t\t\t120844\n",
    "# \t19/04/2017\t\t14-17\t\t\t636238\n",
    "# \t28/04/2017\t\t18\t\t\t\t227459\n",
    "# \t12/05/2017\t\t19-20\t\t\t332583\n",
    "# \t17/05/2017\t\t21\t\t\t\t1993321\n",
    "# \t07/06/2017\t\t22-23\t\t\t183853\n",
    "# \t21/08/2017\t\t24-31\t\t\t738162\n",
    "# \t22/08/2017\t\t32-35\t\t\t1952183\n",
    "# \t23/08/2017\t\t36-37\t\t\t287377\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\fine\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_fine = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_fine.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma w danych z etykietą Voltage Dips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_fine)):\n",
    "    data_fine[i] = data_fine[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja dodająca do zbiorów kolumny z przesuniętymi w czasie prękościami obrotowymi\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def add_history(dataset):\n",
    "    for x in range(0, len(dataset)):\n",
    "        temp = np.concatenate(([dataset[x].values[0, 19]], dataset[x].values[0:-1, 19]))\n",
    "        temp2 = np.concatenate(([dataset[x].values[0, 20]], dataset[x].values[0:-1, 20]))\n",
    "        dataset[x].insert(42, 42, temp)\n",
    "        dataset[x].insert(43, 43, temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dodanie dwóch kolumn z przesuniętymi w czasie prędkościami obrotowymi\n",
    "add_history(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_fine[1].values[:, 19])\n",
    "plt.plot(data_fine[1].values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja usuwająca zakresy wierszy ze zbioru danych\n",
    "import pandas as pd\n",
    "def drop_rows(dataset, beg, end):\n",
    "    dataset.drop(dataset.index[beg:end], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnych obserwacji- długie stany ustalone\n",
    "#wybór na podstawie analizy wykresów przebiegów\n",
    "#zbiory po przetworzeniu:\n",
    "# \tDATA\t\t\tNUMER ZBIORU\tPRÓBEK\t\n",
    "# \t02/03/2017\t\t0-1\t\t\t\t194697\t\t\t\n",
    "# \t06/03/2017\t\t2\t\t\t\t48513\n",
    "# \t16/03/2017\t\t3\t\t\t\t128348\n",
    "# \t22/03/2017\t\t4\t\t\t\t299888\n",
    "# \t23/03/2017\t\t5\t\t\t\t193255\n",
    "# \t29/03/2017\t\t6\t\t\t\t157116\n",
    "# \t13/04/2017\t\t7-11\t\t\t120844\n",
    "# \t19/04/2017\t\t12-13\t\t\t89385\n",
    "# \t28/04/2017\t\t14\t\t\t\t127459\n",
    "# \t12/05/2017\t\t15-16\t\t\t107583\n",
    "# \t17/05/2017\t\t17\t\t\t\t693321\n",
    "# \t07/06/2017\t\t18-19\t\t\t123853\n",
    "# \t21/08/2017\t\t20-22\t\t\t296545\n",
    "# \t22/08/2017\t\t23-25\t\t\t293511\n",
    "# \t23/08/2017\t\t26-27\t\t\t41377\n",
    "drop_rows(data_fine[2], 100000, 300000)\n",
    "drop_rows(data_fine[4], 370000, 500000)\n",
    "drop_rows(data_fine[4], 220000, 340000)\n",
    "drop_rows(data_fine[4], 60000, 200000)\n",
    "drop_rows(data_fine[5], 520000, 620000)\n",
    "drop_rows(data_fine[5], 460000, 500000)\n",
    "drop_rows(data_fine[5], 360000, 390000)\n",
    "drop_rows(data_fine[5], 140000, 265000)\n",
    "drop_rows(data_fine[5], 50000, 100000)\n",
    "drop_rows(data_fine[6], 350000, 360000)\n",
    "drop_rows(data_fine[6], 300000, 320000)\n",
    "drop_rows(data_fine[6], 210000, 270000)\n",
    "drop_rows(data_fine[6], 140000, 190000)\n",
    "drop_rows(data_fine[6], 50000, 125000)\n",
    "drop_rows(data_fine[8], 410000, 450000)\n",
    "drop_rows(data_fine[8], 140000, 375000)\n",
    "drop_rows(data_fine[8], 0, 35000)\n",
    "drop_rows(data_fine[15], 65000, -1)\n",
    "drop_rows(data_fine[17], 20000, 100000)\n",
    "drop_rows(data_fine[18], 100000, 200000)\n",
    "drop_rows(data_fine[19], 210000, 240000)\n",
    "drop_rows(data_fine[19], 0, 150000)\n",
    "drop_rows(data_fine[20], 10000, 55000)\n",
    "drop_rows(data_fine[21], 1300000, 1750000)\n",
    "drop_rows(data_fine[21], 250000, 1100000)\n",
    "drop_rows(data_fine[23], 90000, 110000)\n",
    "drop_rows(data_fine[23], 40000, 80000)\n",
    "drop_rows(data_fine[29], 165000, 180000)\n",
    "drop_rows(data_fine[29], 100000, 140000)\n",
    "drop_rows(data_fine[29], 32000, 75000)\n",
    "drop_rows(data_fine[30], 150000, 180000)\n",
    "drop_rows(data_fine[30], 125000, 140000)\n",
    "drop_rows(data_fine[30], 66000, 92000)\n",
    "drop_rows(data_fine[30], 25000, 50000)\n",
    "drop_rows(data_fine[31], 85000, 160000)\n",
    "drop_rows(data_fine[31], 20000, 65000)\n",
    "drop_rows(data_fine[33], 400000, 500000)\n",
    "drop_rows(data_fine[33], 230000, 375000)\n",
    "drop_rows(data_fine[33], 30000, 200000)\n",
    "drop_rows(data_fine[34], 330000, 440000)\n",
    "drop_rows(data_fine[34], 200000, 310000)\n",
    "drop_rows(data_fine[34], 165000, 180000)\n",
    "drop_rows(data_fine[34], 30000, 150000)\n",
    "drop_rows(data_fine[35], 270000, 365000)\n",
    "drop_rows(data_fine[35], 160000, 256000)\n",
    "drop_rows(data_fine[35], 30000, 140000)\n",
    "drop_rows(data_fine[36], 100000, 195000)\n",
    "drop_rows(data_fine[36], 7000, 85000)\n",
    "drop_rows(data_fine[37], 7000, 80000)\n",
    "indices = 0, 7, 14, 16, 24, 25, 26, 27, 28, 32\n",
    "for i in sorted(indices, reverse=True):\n",
    "    del data_fine[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbicie wszystki pomiarów do jednego dataframe\n",
    "# \tDATA\t\t\tPOCZATEK\t\tKONIEC\n",
    "# \t02/03/2017\t\t0\t\t\t\t194696\t\t\t\n",
    "# \t06/03/2017\t\t194697\t\t\t243209\n",
    "# \t16/03/2017\t\t243210\t\t\t371557\n",
    "# \t22/03/2017\t\t371558\t\t\t671445\n",
    "# \t23/03/2017\t\t671446\t\t\t864700\n",
    "# \t29/03/2017\t\t864701\t\t\t1021816\n",
    "# \t13/04/2017\t\t1021817\t\t\t1142660\n",
    "# \t19/04/2017\t\t1142661 \t\t1243538\n",
    "# \t28/04/2017\t\t1243539\t\t\t1370997\n",
    "# \t17/05/2017\t\t1370998\t\t\t693321\n",
    "# \t12/05/2017\t\t1370998 \t\t1478580\t\t\n",
    "# \t07/06/2017\t\t1478579\t\t\t2197611\n",
    "# \t21/08/2017\t\t2197611\t\t\t2497184\n",
    "# \t22/08/2017\t\t2497185\t\t\t2812172\n",
    "# \t23/08/2017\t\t2812173\t\t\t2927187\n",
    "import pandas as pd\n",
    "fine_all = pd.concat(data_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie niepotrzebnej już listy dataframe'ow\n",
    "del data_fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie outlierów\n",
    "#stwierdzone na podstawie analizy wykresów przebiegów\n",
    "indices = [42931, 42932, 42933, 42934, 42935, 42936, 42937, 42938, 42939]\n",
    "for i in sorted(indices, reverse=True):\n",
    "    fine_all = fine_all.drop(i)\n",
    "drop_rows(fine_all, 42900, 43000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zapicklowanie przetworzonego zbioru danych pochodzących z normalnej pracy\n",
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:  \n",
    "    dill.dump(fine_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Voltage Dips\n",
    "#Dane pochodzą z jednego dnia 14/03/2016\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\vdips\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_vdips = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_vdips.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumny NaN-ów\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_vdips)):\n",
    "    data_vdips[i] = data_vdips[i].drop(42, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóżnionymi o jedną próbkę wartościami prędkości obrotowych\n",
    "add_history(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(data_vdips[1].values[:, 19])\n",
    "plt.plot(data_vdips[1].values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie danych do jednego dataframe\n",
    "import pandas as pd\n",
    "vdips_all = pd.concat(data_vdips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie zbioru\n",
    "import dill\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzenej już listy dataframe'ów\n",
    "del data_vdips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wczytanie danych z etykietą Surge do listy\n",
    "#Stan danych po usunięci dwóch wadliwych dataframe:\n",
    "# \tDATA\t\t\tNR ZBIORU\t\tPRÓBEK\n",
    "# \t17/01/2017\t\t\t0-5\t\t\t411448\n",
    "# \t18/01/2017\t\t\t6-23\t\t1370255\n",
    "# \t19/01/2017\t\t\t24-30\t\t271524\n",
    "# \t20/01/2017\t\t\t31-35\t\t916380\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"D:\\\\9sem\\\\INO\\\\Dane\\\\malf\\\\surge\"\n",
    "all_files = glob.glob(os.path.join(path, \"*.csv\")) #make list of paths\n",
    "\n",
    "data_surge = []\n",
    "for file in all_files:\n",
    "    # Reading the file content to create a DataFrame\n",
    "    data_surge.append(pd.read_csv(file, header=None, skipinitialspace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie uszkodzonych dataFrame\n",
    "#wykonać x2\n",
    "del data_surge[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn, których nie ma vdips\n",
    "import pandas as pd\n",
    "for i in range(0, len(data_surge)):\n",
    "    data_surge[i] = data_surge[i].drop([42, 43, 44, 45, 46, 47, 48, 49, 50, 50], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodanie kolumn z opóźnionymi o jedną próbkę predkościami obrotowymi\n",
    "add_history(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zbicie wszystkich danych w jeden dataframe\n",
    "#Rozkład danych po zbiciu:\n",
    "# \tDATA\t\t\tPOCZĄTEK\t\tKONIEC\n",
    "# \t17/01/2017\t\t0\t\t\t\t411447\n",
    "# \t18/01/2017\t\t411448\t\t\t1781702\n",
    "# \t19/01/2017\t\t1781703\t\t\t2053226\n",
    "# \t20/01/2017\t\t2053227\t\t\t2969606\n",
    "import pandas as pd\n",
    "surge_all = pd.concat(data_surge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zapisanie serii danych\n",
    "import dill\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Usunięcie niepotrzebnej już listy zbiorów\n",
    "del data_surge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max_df(fine, col_num):\n",
    "    max_val = fine[col_num].max()\n",
    "    return max_val\n",
    "def column_min_df(fine, col_num):\n",
    "    min_val = fine[col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#funkcja realizująca normalizacją min-max danych\n",
    "def normalise(dataset, columns_ranges):\n",
    "        for col in range(0, 44):\n",
    "            min_val = columns_ranges[col, 1]\n",
    "            max_val = columns_ranges[col, 0]\n",
    "            dataset[col] = dataset[col].subtract(min_val)\n",
    "            dataset[col] = dataset[col].divide(max_val - min_val)\n",
    "                #dataset.values[row, col] = (value - min_val) / (max_val - min_val)\n",
    "            print(f'Znormalizowano kolumnę {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max_df(fine_all, col)\n",
    "    col_ranges[col, 1] = column_min_df(fine_all, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wstepna normalizacja ułatwiająca porównywanie przebiegów\n",
    "#na wspólnym wykresie.\n",
    "fine_all_norm = fine_all.copy()\n",
    "normalise(fine_all_norm, col_ranges)\n",
    "\n",
    "vdips_all_norm = vdips_all.copy()\n",
    "normalise(vdips_all_norm, col_ranges)\n",
    "\n",
    "surge_all_norm = surge_all.copy()\n",
    "normalise(surge_all_norm, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#DOBÓR ZMIENNYCH WEJŚCIOWYCH\n",
    "#obliczenie współczynników korelacji liniowej między atrybutami\n",
    "#przekierowanie korelacji do arkusza w celu łatwiejszego przeglądania\n",
    "Pearsons = fine_all_norm.corr()\n",
    "path = \"E:\\\\9sem\\\\INO\\\\fine_corrs.xls\"\n",
    "Pearsons.to_excel(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KONTYNUACJA DOBORU ATRYBUTÓW - OBSERWACJA PRZEBIEGÓW\n",
    "#wykreślenie przebiegu wyjścia i potencjalnych wejść\n",
    "#wstepny dobór wejśc na podstawie analizy korelacji Pearsona (impl:pandas.df.corr) i przebiegów\n",
    "import matplotlib.pyplot as plotter\n",
    "%matplotlib qt\n",
    "figManager = plotter.get_current_fig_manager()\n",
    "figManager.window.showMaximized()\n",
    "plotter.plot(fine_all_norm.values[:, 19], label='target')\n",
    "plotter.plot(fine_all_norm.values[:, 6], label='input6')\n",
    "plotter.plot(fine_all_norm.values[:, 40], label='input40')\n",
    "plotter.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie wstępnych znormalizowanych danych\n",
    "del fine_all_norm\n",
    "del vdips_all_norm\n",
    "del surge_all_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)\n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)\n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, 19])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[1][:, 19])\n",
    "plt.plot(test_set[1][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definicje funkcji znajdujących najmniejszą i największa wartość\n",
    "#danego atrybutu spośród wszystkich zbiorów\n",
    "#znalezienie tych wartości jest potrzebne do normalizacji min-max danych\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "def column_max(np_array, col_num):\n",
    "    max_val = np_array[:, col_num].max()\n",
    "    return max_val\n",
    "def column_min(np_array, col_num):\n",
    "    min_val = np_array[:, col_num].min()\n",
    "    return min_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie macierzy wartości największych i najmniejszych wartości\n",
    "#na wierszach: kolejne kolummny zbiorów dancych\n",
    "#kol1 - największa wartość, kol2 - najmniejsza wartość\n",
    "col_ranges = np.empty([44, 2])\n",
    "for col in range(0, 44):\n",
    "    col_ranges[col, 0] = column_max(train_set, col)\n",
    "    col_ranges[col, 1] = column_min(train_set, col)\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_ranges[30, 0] = 100\n",
    "print(col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise(fine_all, col_ranges)\n",
    "normalise(vdips_all, col_ranges)\n",
    "normalise(surge_all, col_ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fine_all.values[:, 19])\n",
    "plt.plot(fine_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(surge_all.values[:, 19])\n",
    "plt.plot(surge_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vdips_all.values[:, 19])\n",
    "plt.plot(vdips_all.values[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open('fine_all.pkl', 'wb') as f:\n",
    "    dill.dump(fine_all, f)\n",
    "\n",
    "with open('vdips_all.pkl', 'wb') as f:\n",
    "    dill.dump(vdips_all, f)\n",
    "\n",
    "with open('surge_all.pkl', 'wb') as f:\n",
    "    dill.dump(surge_all, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, -1])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie analogicznego zbioru testowego\n",
    "test_set=[]\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "\n",
    "col = len(test_set[0][0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "for x in test_set:\n",
    "    x[:, [19, col-1]] = x[:, [col-1, 19]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[0][:, -1])\n",
    "plt.plot(test_set[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nalezy wykonywac gdyz pythonowe referencje przycinaja nam zbior\n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, -2, -1]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, -2, -1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set_trim[:, -2])\n",
    "plt.plot(train_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set_trim[:, -2])\n",
    "plt.plot(train_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_trim[:, -2])\n",
    "plt.plot(test_set_trim[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structs_list = [[6, 4]]\n",
    "model = multi_train_NLP(structs_list=structs_list, \n",
    "                        patience=3,\n",
    "                        batch_size=800,\n",
    "                        epochs=10,\n",
    "                        train_data=train_set_trim,\n",
    "                        test_data=test_set_trim,\n",
    "                        set_num=0\n",
    "                       ,criterion='mse'\n",
    "                       ,single_times=1\n",
    "                       ,dropout_2list=[[0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sim(model, test_set_trim, 1)\n",
    "eval_prediction(y, test_set_trim[1][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [[0.5, 0.1]]\n",
    "model = multi_train_SVR(C_eps_pairs=params\n",
    "                       ,train_data=train_set_trim\n",
    "                       ,test_data=test_set_trim\n",
    "                       ,set_num=0,\n",
    "                       criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sim(model, test_set_trim, 0)\n",
    "y = y.reshape(-1, 1)\n",
    "eval_prediction(y, test_set_trim[0][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_trim = train_set[:, [6, 40, 19]]\n",
    "test_set_trim = []\n",
    "for test_case in test_set:\n",
    "    test_set_trim.append(test_case[:, [6, 40, 19]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_train_LSTM(structure_list=[[4]]\n",
    "                        ,patience=3, epochs=25\n",
    "                        ,batch_size=1000, train_X=Xs\n",
    "                        ,train_Y=Ys, test_X=TXs\n",
    "                        ,test_Y=TYs, set_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(TXs[0])\n",
    "eval_prediction(pred, TYs[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stworzenie zbioru uczacego początkowo zawierający wszystkie możliwe wejścia\n",
    "#uzasadnienie podziału na zbiór uczący i testowy w grafice zbior_uczacy.png\n",
    "    #edit: dodano probki od 2180000 do 2270000 aby było więcej obserwacji stanu 0\n",
    "    #zazegnało to problem dużego błędu już na początku symulacji\n",
    "#dane pochodzą z dni (fragmenty, nie całe dni):\n",
    "    #06/03/2017\n",
    "    #16/03/2017\n",
    "    #22/03/2017\n",
    "    #23/03/2017\n",
    "    #17/05/2017\n",
    "    #21/08/2017\n",
    "    #22/08/2017\n",
    "    #23/08/2017\n",
    "import numpy as np\n",
    "train_set = fine_all.values[305000:315000, :]\n",
    "train_set = np.concatenate((train_set, fine_all.values[210000:260000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2180000:2270000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[275000:285000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[830000:863413, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[636000:644000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[579334:620000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[1985000:2015000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2296300:2338950, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2916630:2920340, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2897410:2898980, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2890000:2891850, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2878300:2881860, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2867840:2875070, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2856640:2859880, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2850400:2853840, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2844370:2846000, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2821670:2830570, :]))\n",
    "train_set = np.concatenate((train_set, fine_all.values[2812220:2815440, :]))\n",
    "train_set = train_set[~np.isnan(train_set).any(axis=1)]\n",
    "col = len(train_set[0])\n",
    "#zamiana kolumn, aby kolumna targetów \n",
    "#była ostatnia w macierzy danych treningowych\n",
    "train_set[:, [19, col-1]] = train_set[:, [col-1, 19]]\n",
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)\n",
    "    \n",
    "np.shape(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for i in sorted(indices, reverse=True):\n",
    "    train_set = np.delete(train_set, i, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_set[:, -1])\n",
    "plt.plot(train_set[:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_set.pkl', 'wb') as f:\n",
    "    dill.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbioru testowego początkowo zawierającego wszystkie atrybuty\n",
    "#zbiór testowy pochodzi z prawidłowej pracy\n",
    "#uzasadnienie doboru w grafice zbior_uczacy.png\n",
    "import numpy as np\n",
    "test_set=[]\n",
    "#02/03/2017\n",
    "test_set.append(fine_all.values[5000:40000, :])\n",
    "test_set.append(fine_all.values[65000:160000, :])\n",
    "#22/03/2017\n",
    "test_set.append(fine_all.values[395000:459000, :])\n",
    "test_set.append(fine_all.values[470770:546400, :])\n",
    "#29/03/2017\n",
    "test_set.append(fine_all.values[867400:958000, :])\n",
    "test_set.append(fine_all.values[979500:998000, :])\n",
    "#13/04/2017\n",
    "test_set.append(fine_all.values[1070000:1115190, :])\n",
    "#19/04/2017\n",
    "test_set.append(fine_all.values[1212500:1230000, :])\n",
    "#28/04/2017\n",
    "test_set.append(fine_all.values[1270000:1310000, :])\n",
    "test_set.append(fine_all.values[1346000:1367500, :])\n",
    "#12/05/2017\n",
    "test_set.append(fine_all.values[1403420:1423060, :])\n",
    "test_set.append(fine_all.values[1436000:1444870, :])\n",
    "test_set.append(fine_all.values[1457230:1462000, :])\n",
    "#17/05/2017\n",
    "test_set.append(fine_all.values[1650000:1665000, :])\n",
    "test_set.append(fine_all.values[1759000:1840000, :])\n",
    "test_set.append(fine_all.values[2162000:2169620, :])\n",
    "for i in range(0, len(test_set)):\n",
    "    test_set[i] = test_set[i][~np.isnan(test_set[i]).any(axis=1)]\n",
    "    col = len(test_set[i][0])\n",
    "    test_set[i][:, [19, col-1]] = test_set[i][:, [col-1, 19]]\n",
    "    \n",
    "with open('fine_all.pkl', 'rb') as f:\n",
    "    fine_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set[0][:, -1])\n",
    "plt.plot(test_set[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set[j] = np.delete(test_set[j], i, axis=1)\n",
    "with open('test_set.pkl', 'wb') as f:\n",
    "    dill.dump(test_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column(vdips_all, 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zbior testowy z danych z etykietą Voltage Dips\n",
    "#Dane z 14/03/2017\n",
    "import numpy as np\n",
    "test_set_vdips=[]\n",
    "test_set_vdips.append(vdips_all.values[250000:270000, :])\n",
    "test_set_vdips.append(vdips_all.values[380000:400000, :])\n",
    "test_set_vdips.append(vdips_all.values[500000:520000, :])\n",
    "test_set_vdips.append(vdips_all.values[820000:840000, :])\n",
    "test_set_vdips.append(vdips_all.values[980000:1000000, :])\n",
    "test_set_vdips.append(vdips_all.values[1100000:1120000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_vdips[i] = test_set_vdips[i][~np.isnan(test_set_vdips[i]).any(axis=1)]\n",
    "    col = len(test_set_vdips[i][0])\n",
    "    test_set_vdips[i][:, [19, col-1]] = test_set_vdips[i][:, [col-1, 19]]\n",
    "    \n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_vdips)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_vdips[j] = np.delete(test_set_vdips[j], i, axis=1)\n",
    "with open('vdips_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_vdips, f)\n",
    "    \n",
    "with open('vdips_all.pkl', 'rb') as f:\n",
    "    vdips_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_vdips[0][:, -1])\n",
    "plt.plot(test_set_vdips[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#przygotowanie zbiorów testowych z etykietą Surge\n",
    "import numpy as np\n",
    "test_set_surge=[]\n",
    "#17/01/2017\n",
    "test_set_surge.append(surge_all.values[150000:400000, :])\n",
    "#18/01/2017\n",
    "test_set_surge.append(surge_all.values[570000:800000, :])\n",
    "test_set_surge.append(surge_all.values[900000:1030000, :])\n",
    "test_set_surge.append(surge_all.values[1113000:1150000, :])\n",
    "test_set_surge.append(surge_all.values[1280000:1305000, :])\n",
    "test_set_surge.append(surge_all.values[1350000:1380000, :])\n",
    "test_set_surge.append(surge_all.values[1580000:1605000, :])\n",
    "#20/01/2017\n",
    "test_set_surge.append(surge_all.values[2700000:2900000, :])\n",
    "for i in range(0, len(test_set_vdips)):\n",
    "    test_set_surge[i] = test_set_surge[i][~np.isnan(test_set_surge[i]).any(axis=1)]\n",
    "    col = len(test_set_surge[i][0])\n",
    "    test_set_surge[i][:, [19, col-1]] = test_set_surge[i][:, [col-1, 19]]\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)\n",
    "    \n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usunięcie kolumn określonych apriori za niepotrzebne\n",
    "#0  kolumna indentyfikatora, czas pracy\n",
    "#4  kolumna o bardzo niskiej wariancji\n",
    "#13 kolumna szumów\n",
    "#14 kolumna szumów\n",
    "#16 kolumna szumów\n",
    "#17 kolumna szumów\n",
    "#23 kolumna o bardzo niskiej wariancji\n",
    "#26 kolumna stała\n",
    "#29 kolumna o bardzo niskiej wariancji\n",
    "#30 kolumna stała\n",
    "#33 kolumna stała\n",
    "#35 kolumna stała\n",
    "indices = 0, 4, 13, 14, 16, 17, 23, 26, 29, 30, 33, 35\n",
    "for j in range(0, len(test_set_surge)):\n",
    "    for i in sorted(indices, reverse=True):\n",
    "        test_set_surge[j] = np.delete(test_set_surge[j], i, axis=1)\n",
    "with open('surge_test.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_surge, f)\n",
    "    \n",
    "with open('surge_all.pkl', 'rb') as f:\n",
    "    surge_all = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_set_surge[0][:, -1])\n",
    "plt.plot(test_set_surge[0][:, -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = len(train_set[0])\n",
    "cols_taken = [9, 17, 25, 27, 28, 29, col-2, col-1]\n",
    "try_add_column(cols_taken=cols_taken\n",
    "              ,test_col=15\n",
    "              ,criterion='mse'\n",
    "              ,train_set=train_set\n",
    "              ,test_set=test_set\n",
    "              ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_column2(test_set[0], 23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 25, 27, 28, 29, 30, 31]\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0]) - 1\n",
    "#cols_taken = [17, 29, 5, 27, col-1, col]\n",
    "cols_taken = [9, 25, 27, 28, 29, col-1, col]\n",
    "print(cols_taken)\n",
    "\n",
    "train_set_trim = train_set[:, cols_taken]\n",
    "test_set_trim = []\n",
    "for test in test_set:\n",
    "    test_set_trim.append(test[:, cols_taken])\n",
    "    \n",
    "vdips_trim = []\n",
    "for test in test_set_vdips:\n",
    "    vdips_trim.append(test[:, cols_taken])\n",
    "    \n",
    "surge_trim = []\n",
    "for test in test_set_surge:\n",
    "    surge_trim.append(test[:, cols_taken])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataSVR.pkl', 'wb') as f:\n",
    "    dill.dump(test_set_trim[0], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_set_trim[15])):\n",
    "    plt.plot(test_set_trim[15][:, i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(test_set_trim[0][0])):\n",
    "    plt.plot(test_set_trim[0][:, i], label=f'{i}')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_SVR(C=0.4, epsilon=0.008, train_data=train_set_trim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_eps = generate_SVR_params(0.4, 1.6, 0.2\n",
    "                           ,0.008, 0.016, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = multi_train_SVR(C_eps, train_set_trim, test_set_trim\n",
    "                        ,7 , 'mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.129506211945852e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9.129506211945852e-05"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = 0\n",
    "y = sim(model, test_set_trim, col)\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 2\n",
    "y = sim(model, surge_trim, num)\n",
    "eval_prediction(y, surge_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num = 5\n",
    "y = sim(model, vdips_trim, num)\n",
    "eval_prediction(y, vdips_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = len(train_set[0])\n",
    "base_cols = [9, 25, 27, 28, 29, col-2, col-1]\n",
    "try_drop_column(start_cols=base_cols\n",
    "               ,test_col=11\n",
    "               ,criterion='mse'\n",
    "               ,train_set=train_set\n",
    "               ,test_set=test_set\n",
    "               ,model_type='SVR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model_svr.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct_list = [[6,8,2],[6,8,3],[6,8,4],[6,8,5],[6,8,6]]\n",
    "#struct_list = [[8,12,16],[8,12,24],[8,12,32]]\n",
    "#struct_list = [[4,12]]\n",
    "\n",
    "#dropouts = [[0.2],[0.2],[0.4],[0.3],[0.5]]\n",
    "\n",
    "#dropouts=[[0.0,0.0],[0.0,0.0],[0.0,0.0],[0.0,0.0],[0.0,0.0]]\n",
    "#dropouts=[[0.0,0.25],[0.0,0.375],[0.0,0.2],[0.0,0.3],[0.0,0.4]]\n",
    "#dropouts=[[0.0,0.0],[0.0,0.25],[0.0,0.0],[0.0,0.2],[0.0,0.0]]\n",
    "\n",
    "dropouts=[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.0]]\n",
    "#dropouts=[[0.0,0.25,0.0],[0.0,0.25,0.0],[0.0,0.25,0.0],[0.0,0.25,0.0],[0.0,0.25,0.0]]\n",
    "#dropouts=[[0.2,0.0,0.0],[0.2,0.0,0.0],[0.2,0.0,0.0],[0.2,0.0,0.0],[0.2,0.0,0.0]]\n",
    "#dropouts=[[0.0,0.0,0.1],[0.0,0.0,0.2],[0.0,0.0,0.3],[0.0,0.0,0.4],[0.0,0.0,0.5]]\n",
    "#dropouts=[[0.0,0.1,0.1],[0.0,0.1,0.2],[0.0,0.2,0.2],[0.0,0.1,0.3],[0.0,0.3,0.3]]\n",
    "#dropouts=[[0.0,0.2,0.0],[0.0,0.2,0.0],[0.0,0.2,0.0],[0.0,0.2,0.0],[0.0,0.2,0.0]]\n",
    "#dropouts=[[0.0,0.0,0.0],[0.0,0.0,0.0],[0.0,0.0,0.2],[0.0,0.0,0.2],[0.0,0.0,0.2]]\n",
    "#dropouts=[[0.0,0.0,0.2],[0.0,0.0,0.2],[0.0,0.0,0.2]]\n",
    "#dropouts = [[0.0,0.0]]\n",
    "model = multi_train_NLP(structs_list=struct_list, patience=3\n",
    "                       ,batch_size=1000, epochs=25\n",
    "                       ,train_data=train_set_trim\n",
    "                       ,test_data=test_set_trim\n",
    "                       ,set_num = 9, criterion='mse'\n",
    "                       ,single_times = 25\n",
    "                       ,dropout_2list=dropouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(model.get_config()['layers']) - 1):\n",
    "    if i%2 == 0:\n",
    "        field = 'units'\n",
    "        log = f'Liczba neuronow w {int(i/2) + 1} warstwie ukrytej:'\n",
    "    else:\n",
    "        field = 'rate'\n",
    "        log = f'Szansa opuszczenia neuronu w {int(i/2) + 1} warstwie ukrytej:'\n",
    "    val  = model.get_config()['layers'][i]['config'][field]\n",
    "    print(f'{log} {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 4\n",
    "y = sim(model, test_set_trim, col)\n",
    "eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in range(0, len(test_set_trim)):\n",
    "    plt.figure(col)\n",
    "    y = sim(model, test_set_trim, col)\n",
    "    eval_prediction(y, test_set_trim[col][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "y = sim(model, surge_trim, num)\n",
    "eval_prediction(y, surge_trim[num][:, -1], 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 5\n",
    "y = sim(model, vdips_trim, num)\n",
    "eval_prediction(y, vdips_trim[num][:, -1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('MLPs\\\\model_MLP_6_8_5_2020-5-5.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 25, 27, 28, 29, 31]\n"
     ]
    }
   ],
   "source": [
    "col = len(train_set[0]) - 1\n",
    "#cols_taken = [17, 29, 5, 27, col-1, col]\n",
    "cols_taken = [9, 25, 27, 28, 29, col]\n",
    "print(cols_taken)\n",
    "\n",
    "train_set_trim = train_set[:, cols_taken]\n",
    "test_set_trim = []\n",
    "for test in test_set:\n",
    "    test_set_trim.append(test[:, cols_taken])\n",
    "    \n",
    "vdips_trim = []\n",
    "for test in test_set_vdips:\n",
    "    vdips_trim.append(test[:, cols_taken])\n",
    "    \n",
    "surge_trim = []\n",
    "for test in test_set_surge:\n",
    "    surge_trim.append(test[:, cols_taken])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use whole series\n",
    "train_series = []\n",
    "\n",
    "indices = pd.read_csv('indices.csv')\n",
    "indices = indices.values\n",
    "for i in range(0,len(indices)):\n",
    "    train_series.append(train_set_trim[indices[i, 0]:indices[i, 1], :])\n",
    "\n",
    "max_len = 0\n",
    "for i in range(0,len(train_series)):\n",
    "    if len(train_series[i]) > max_len:\n",
    "        max_len = len(train_series[i])\n",
    "        \n",
    "for i in range(0, len(train_series)):\n",
    "    train_series[i]=np.pad(train_series[i]\n",
    "                 ,((0,max_len - len(train_series[i])),(0,0))\n",
    "                 ,'constant'\n",
    "                 , constant_values=-10)\n",
    "\n",
    "train_series = np.dstack(train_series)\n",
    "train_series = np.moveaxis(train_series, 2, 0)\n",
    "\n",
    "Xs = train_series[:, :, :-1]\n",
    "Ys = train_series[:, :, -1]\n",
    "Ys = np.reshape(Ys, (-1, max_len, 1))\n",
    "\n",
    "print(f'Wymiar danych treningowych: {Xs.shape}')\n",
    "print(f'Wymiar targetów: {Ys.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekwencjonuje zbior uczacy\n",
      "Sekswencjonuje zbior testowy nr: 0\n",
      "Sekswencjonuje zbior testowy nr: 1\n",
      "Sekswencjonuje zbior testowy nr: 2\n",
      "Sekswencjonuje zbior testowy nr: 3\n",
      "Sekswencjonuje zbior testowy nr: 4\n",
      "Sekswencjonuje zbior testowy nr: 5\n",
      "Sekswencjonuje zbior testowy nr: 6\n",
      "Sekswencjonuje zbior testowy nr: 7\n",
      "Sekswencjonuje zbior testowy nr: 8\n",
      "Sekswencjonuje zbior testowy nr: 9\n",
      "Sekswencjonuje zbior testowy nr: 10\n",
      "Sekswencjonuje zbior testowy nr: 11\n",
      "Sekswencjonuje zbior testowy nr: 12\n",
      "Sekswencjonuje zbior testowy nr: 13\n",
      "Sekswencjonuje zbior testowy nr: 14\n",
      "Sekswencjonuje zbior testowy nr: 15\n"
     ]
    }
   ],
   "source": [
    "Xs, Ys, TXs, TYs = prepare_timeseries(train_set_trim\n",
    "                                     ,test_set_trim\n",
    "                                     ,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_timeseries(Xs, Ys, TXs, TYs, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sekswencjonuje zbior testowy nr: 0\n",
      "Sekswencjonuje zbior testowy nr: 1\n",
      "Sekswencjonuje zbior testowy nr: 2\n",
      "Sekswencjonuje zbior testowy nr: 3\n",
      "Sekswencjonuje zbior testowy nr: 4\n",
      "Sekswencjonuje zbior testowy nr: 5\n",
      "Sekswencjonuje zbior testowy nr: 6\n",
      "Sekswencjonuje zbior testowy nr: 7\n"
     ]
    }
   ],
   "source": [
    "SXs, SYs = prepare_surge(surge_trim, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_surges(SXs, SYs, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, Ys, TXs, TYs = load_timeseries(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SXs, SYs = load_surges(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353066, 12, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(249993, 7, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SXs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 4\n",
    "#structure = [[4],[5],[6],[7],[8],[10]]\n",
    "#structure = [[12],[16],[20],[24],[32],[40]]\n",
    "#structure = [[a,16],[a,2],[a,4],[a,6],[a,8],[a,12]]\n",
    "structure = [[a,20],[a,24],[a,32],[a,40],[a,48],[a,64]]\n",
    "\n",
    "#dropouts = [[0],[0],[0],[0],[0],[0]]\n",
    "#dropouts = [[0],[0],[0],[0.2],[0.25],[0.25]]\n",
    "dropouts = [[0,0],[0,0],[0,0],[0,0],[0,0],[0,0]]\n",
    "\n",
    "rc = 0.0\n",
    "rec_dropouts = [[rc],[rc],[rc],[rc],[rc],[rc]]\n",
    "\n",
    "model = multi_train_LSTM(\n",
    "    structure_list=structure, patience=3\n",
    "    ,dropouts=dropouts, rec_dropouts=rec_dropouts\n",
    "    ,epochs=100, batch_size=128, train_single=1\n",
    "    ,train_X=Xs, train_Y=Ys, debug_log='yes'\n",
    "    ,test_X=TXs, test_Y=TYs, set_num=9\n",
    "    ,criterion='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = [8, 2]\n",
    "dropouts = [0, 0]\n",
    "rec_dropouts = [0]\n",
    "\n",
    "model = train_LSTM(\n",
    "    structure=structure\n",
    "    ,patience=3\n",
    "    ,epochs=100\n",
    "    ,batch_size=128\n",
    "    ,dropouts=dropouts\n",
    "    ,rec_dropouts=rec_dropouts\n",
    "    ,train_X=Xs, train_Y=Ys)\n",
    "\n",
    "import winsound\n",
    "filename = 'sound.wav'\n",
    "winsound.PlaySound(filename, winsound.SND_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout = bckStdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking_20 (Masking)         (None, 7, 5)              0         \n",
      "_________________________________________________________________\n",
      "lstm_20 (LSTM)               (None, 4)                 160       \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 24)                120       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 24)                0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 25        \n",
      "=================================================================\n",
      "Total params: 305\n",
      "Trainable params: 305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.129242197951856e-05\n",
      "5.9410342640896086e-05\n",
      "5.750220090139727e-06\n",
      "7.051945299870351e-05\n",
      "0.00012332493355644928\n",
      "6.477760971538886e-05\n",
      "9.67933988216947e-05\n",
      "9.880372757867003e-06\n",
      "4.118025140573987e-05\n",
      "0.0001264897014060414\n",
      "3.020493845839957e-05\n",
      "4.623559453328304e-05\n",
      "1.7797048133034548e-05\n",
      "1.311001221519908e-05\n",
      "0.00015940655617132428\n",
      "0.00041234913440216356\n"
     ]
    }
   ],
   "source": [
    "col = 0\n",
    "for col in range(0, len(TXs)-0):\n",
    "    plt.figure(col)\n",
    "    pred = model.predict(TXs[col])\n",
    "    eval_prediction(pred, TYs[col], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0010585692919480461\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0010585692919480461"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel\\eventloops.py:106: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  app.exec_()\n"
     ]
    }
   ],
   "source": [
    "col = 2\n",
    "pred = model.predict(SXs[col])\n",
    "eval_prediction(pred, SYs[col], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping    \n",
    "es = EarlyStopping(monitor='val_loss', mode='min'\n",
    "                      ,verbose=1, patience=3)\n",
    "\n",
    "model.fit(Xs, Ys, epochs=25,\n",
    "              batch_size=64,\n",
    "              validation_split=0.35,\n",
    "              verbose=1,\n",
    "              shuffle=False,\n",
    "              callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LSTM7\\\\model_LSTM_4_24_2020-5-23.pkl', 'wb') as f:\n",
    "    dill.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
